{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tune Crop14 Image classification with ViT",
      "provenance": [],
      "collapsed_sections": [
        "kYFkur3sEb6f",
        "NhO8MiJ41HLI",
        "5lYnSRfsvhd_",
        "msyHAoNs_gbq",
        "3qN-Czm1S0OD",
        "18piHpEmt2DF",
        "SQ-QYbKst7i9",
        "qa7cWXbvzOHK",
        "qW0GTGnQzZnw",
        "DQpagmjCuH9_",
        "6f7vgLcGog9B",
        "nMaBzIBLWWs6"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5d0beac885c5441288e24a40d888f67f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97c8893d43c64ea7b67f7c0c51febadf",
              "IPY_MODEL_976f0611f2784dafaff403567d040689",
              "IPY_MODEL_84ada6de0c6540d49402fc9becbfa8ff"
            ],
            "layout": "IPY_MODEL_52ab8f354e22475ea246e5974240582b"
          }
        },
        "97c8893d43c64ea7b67f7c0c51febadf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81f19a31ad71420aa7a25a025960f329",
            "placeholder": "​",
            "style": "IPY_MODEL_00241670ea8140f6a91ff0269bc091d5",
            "value": "Downloading: 100%"
          }
        },
        "976f0611f2784dafaff403567d040689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_658b6c48ab8248b1917a00bca7564b9f",
            "max": 2253,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee12c50b0ab44d8d872e6d51dae8c2d1",
            "value": 2253
          }
        },
        "84ada6de0c6540d49402fc9becbfa8ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1acdf2eeac6f4cdd956e7bb8ddc2250f",
            "placeholder": "​",
            "style": "IPY_MODEL_3b5d698c3a8e43b59f8f56e5aa5f7edc",
            "value": " 2.25k/2.25k [00:00&lt;00:00, 87.6kB/s]"
          }
        },
        "52ab8f354e22475ea246e5974240582b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81f19a31ad71420aa7a25a025960f329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00241670ea8140f6a91ff0269bc091d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "658b6c48ab8248b1917a00bca7564b9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee12c50b0ab44d8d872e6d51dae8c2d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1acdf2eeac6f4cdd956e7bb8ddc2250f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b5d698c3a8e43b59f8f56e5aa5f7edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e5945e353564fec8e53f1b756691f9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_098d70cad85d4b3c8058f3794944ddb7",
              "IPY_MODEL_24b4f7d64f114a079f96f2436dc7d61c",
              "IPY_MODEL_f7048b9d0d4a4f8bb33a841a4994e5b1"
            ],
            "layout": "IPY_MODEL_8c0712895e3049f088fb93c4e8914def"
          }
        },
        "098d70cad85d4b3c8058f3794944ddb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8b1921d6c134b4db25c65f8f4ad9a64",
            "placeholder": "​",
            "style": "IPY_MODEL_619032123dc54b0e9075a2dd3538a821",
            "value": "Downloading data files: 100%"
          }
        },
        "24b4f7d64f114a079f96f2436dc7d61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_822fbddc6ca1466c938e5987c7f0cfdf",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_debe71e277df44a4a13bf165c40ee469",
            "value": 2
          }
        },
        "f7048b9d0d4a4f8bb33a841a4994e5b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb32d0d789304de59102e04aa2ff96de",
            "placeholder": "​",
            "style": "IPY_MODEL_49574e906bd543c0932b2fc8f9be943b",
            "value": " 2/2 [01:43&lt;00:00, 60.07s/it]"
          }
        },
        "8c0712895e3049f088fb93c4e8914def": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8b1921d6c134b4db25c65f8f4ad9a64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "619032123dc54b0e9075a2dd3538a821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "822fbddc6ca1466c938e5987c7f0cfdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "debe71e277df44a4a13bf165c40ee469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb32d0d789304de59102e04aa2ff96de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49574e906bd543c0932b2fc8f9be943b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6eef87135bb646aa9a726dfd10aa0610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17a1b6aadf3f4b53a60fa68b236afce6",
              "IPY_MODEL_1f460951fa4f48a8bb7360c7b2188431",
              "IPY_MODEL_1bdd094f5fe44de9937495cb7b76b823"
            ],
            "layout": "IPY_MODEL_851a313c4f0c4c678cb2e0dcb2f05e0d"
          }
        },
        "17a1b6aadf3f4b53a60fa68b236afce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81f7d5f3088c4c409e7c0f1727400832",
            "placeholder": "​",
            "style": "IPY_MODEL_4c6ecbfa651845ea9ff23bdaf60bbdd6",
            "value": "Downloading data: 100%"
          }
        },
        "1f460951fa4f48a8bb7360c7b2188431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e986e5094684577bee744389ccc51e0",
            "max": 304085068,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9da5a9cdfdad4a819053124962686a4a",
            "value": 304085068
          }
        },
        "1bdd094f5fe44de9937495cb7b76b823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_827f286bdeda4dd09671c6b0a4af8dfa",
            "placeholder": "​",
            "style": "IPY_MODEL_f69285f67c5b4e2cbbc152ee1a4c0d9f",
            "value": " 304M/304M [00:04&lt;00:00, 63.3MB/s]"
          }
        },
        "851a313c4f0c4c678cb2e0dcb2f05e0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81f7d5f3088c4c409e7c0f1727400832": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c6ecbfa651845ea9ff23bdaf60bbdd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e986e5094684577bee744389ccc51e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9da5a9cdfdad4a819053124962686a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "827f286bdeda4dd09671c6b0a4af8dfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f69285f67c5b4e2cbbc152ee1a4c0d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcaed37bfacb4911b3d305d08d5255f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef23103ecf8a4390bc6ea08746ee11c4",
              "IPY_MODEL_d36bfcf8708f4474ad1ccc42a434014b",
              "IPY_MODEL_1a1b82e662a94c628a004a83146251fe"
            ],
            "layout": "IPY_MODEL_0b3f991b89ca4da8b4365c24bbd2abcc"
          }
        },
        "ef23103ecf8a4390bc6ea08746ee11c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_064cf0e017b046c2bc87f817b4b7d760",
            "placeholder": "​",
            "style": "IPY_MODEL_c643ec7ddb714f68bb9180d55ce997a3",
            "value": "Downloading data: 100%"
          }
        },
        "d36bfcf8708f4474ad1ccc42a434014b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1b984e9fc1941c1aec6389343d37f67",
            "max": 458040180,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03b088d198da4d70851a7f7df0df03ab",
            "value": 458040180
          }
        },
        "1a1b82e662a94c628a004a83146251fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36295e647e5c41ebbf2dc0ebca345a21",
            "placeholder": "​",
            "style": "IPY_MODEL_aa0aa6c89f8f4d67932a1bac9e5fc902",
            "value": " 458M/458M [00:14&lt;00:00, 21.0MB/s]"
          }
        },
        "0b3f991b89ca4da8b4365c24bbd2abcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "064cf0e017b046c2bc87f817b4b7d760": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c643ec7ddb714f68bb9180d55ce997a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1b984e9fc1941c1aec6389343d37f67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03b088d198da4d70851a7f7df0df03ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36295e647e5c41ebbf2dc0ebca345a21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa0aa6c89f8f4d67932a1bac9e5fc902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84ace2dd82204797a8608ad9aa042d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a51595d0e4de48298c07925e83edf152",
              "IPY_MODEL_a838ad5de5364344b732fee963dc76f3",
              "IPY_MODEL_95dd0995db4941fbac0aef701fd4cba2"
            ],
            "layout": "IPY_MODEL_3210b799d54942429c28b7e9f4153352"
          }
        },
        "a51595d0e4de48298c07925e83edf152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f62bf871bdc4d539d29c39db6ae05f6",
            "placeholder": "​",
            "style": "IPY_MODEL_02b7305606de4fb09ce30b7c1914b3f9",
            "value": "Downloading data: 100%"
          }
        },
        "a838ad5de5364344b732fee963dc76f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfa11993737e401bbd7a04e917acab4d",
            "max": 432528072,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33f7d7ac395a49a783de824aea79d329",
            "value": 432528072
          }
        },
        "95dd0995db4941fbac0aef701fd4cba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce4b558070644522af5e887098644cc6",
            "placeholder": "​",
            "style": "IPY_MODEL_965e38112de74dae8456a480bd9a3c0b",
            "value": " 433M/433M [00:07&lt;00:00, 64.4MB/s]"
          }
        },
        "3210b799d54942429c28b7e9f4153352": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f62bf871bdc4d539d29c39db6ae05f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02b7305606de4fb09ce30b7c1914b3f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfa11993737e401bbd7a04e917acab4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33f7d7ac395a49a783de824aea79d329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce4b558070644522af5e887098644cc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "965e38112de74dae8456a480bd9a3c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b3f37f4ad944a0fb5e43c0cf8495bb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25db28b1476a4a27b2a03514d69ee87c",
              "IPY_MODEL_3d3c1a975cc94dfba0923b9c4d32bc12",
              "IPY_MODEL_994e15733bd94024a2ed7e03c2a6f538"
            ],
            "layout": "IPY_MODEL_382b5daea4b2454e8a656a44fdb27f8e"
          }
        },
        "25db28b1476a4a27b2a03514d69ee87c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76933da29bdb4ad0b30b996303101571",
            "placeholder": "​",
            "style": "IPY_MODEL_62d9970291b14993bc4cd1c2d5101702",
            "value": "Downloading data: 100%"
          }
        },
        "3d3c1a975cc94dfba0923b9c4d32bc12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e5f3eca32194d02b108b8c21a246b16",
            "max": 447476307,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b529b210cdcb4c07b38aa3a6f19fafee",
            "value": 447476307
          }
        },
        "994e15733bd94024a2ed7e03c2a6f538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29b5044460ee42a8acced1835b3f9337",
            "placeholder": "​",
            "style": "IPY_MODEL_09670b1aa2b644f3809bf7d437f72603",
            "value": " 447M/447M [00:07&lt;00:00, 68.2MB/s]"
          }
        },
        "382b5daea4b2454e8a656a44fdb27f8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76933da29bdb4ad0b30b996303101571": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62d9970291b14993bc4cd1c2d5101702": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e5f3eca32194d02b108b8c21a246b16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b529b210cdcb4c07b38aa3a6f19fafee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29b5044460ee42a8acced1835b3f9337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09670b1aa2b644f3809bf7d437f72603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6b76afce6fc4b6794b5421437e33254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15ca137215c742ee8d976c29d703fee4",
              "IPY_MODEL_dbf1db680cbc444bb4c572de13b07ce8",
              "IPY_MODEL_3d863bf0d3654804913120a5d82597ac"
            ],
            "layout": "IPY_MODEL_b8920da35f534308882bb8f225bdc6fe"
          }
        },
        "15ca137215c742ee8d976c29d703fee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b8454491ddd405ea4a4af1aa1f4b0e6",
            "placeholder": "​",
            "style": "IPY_MODEL_a020aac6b0da468898418987e2dc4421",
            "value": "Downloading data: 100%"
          }
        },
        "dbf1db680cbc444bb4c572de13b07ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64a5b629e25d4b8982a7dc054425dede",
            "max": 429953243,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52ee799b298f4ac8a690c0a9d3978dbd",
            "value": 429953243
          }
        },
        "3d863bf0d3654804913120a5d82597ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8bf63a1bd8643b182504e0b37214b72",
            "placeholder": "​",
            "style": "IPY_MODEL_dcd15d96e2bc4c588b13203651a3db29",
            "value": " 430M/430M [00:06&lt;00:00, 62.3MB/s]"
          }
        },
        "b8920da35f534308882bb8f225bdc6fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b8454491ddd405ea4a4af1aa1f4b0e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a020aac6b0da468898418987e2dc4421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64a5b629e25d4b8982a7dc054425dede": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52ee799b298f4ac8a690c0a9d3978dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8bf63a1bd8643b182504e0b37214b72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcd15d96e2bc4c588b13203651a3db29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f77427fc0274c9ab4ba03d4981a62ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0d7cc687cba41a6b3157499b241fbad",
              "IPY_MODEL_aa0abea829284c22b969d7c76af92296",
              "IPY_MODEL_c163a50b11ca4beab5a39f8c7db8c92d"
            ],
            "layout": "IPY_MODEL_1f6c62e85009488bb169d2cfebf7c8c1"
          }
        },
        "e0d7cc687cba41a6b3157499b241fbad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53dfe59086fc46d2924ad486a4c62ea1",
            "placeholder": "​",
            "style": "IPY_MODEL_927d1f423dda4a13973beec2391d3184",
            "value": "Downloading data: 100%"
          }
        },
        "aa0abea829284c22b969d7c76af92296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fa833b80b7d4a2595b0af641c930983",
            "max": 426072012,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01ad71c71b574ae5ab94523c9933e5a9",
            "value": 426072012
          }
        },
        "c163a50b11ca4beab5a39f8c7db8c92d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8063f9c6c8c34116a17091ae5c57b43a",
            "placeholder": "​",
            "style": "IPY_MODEL_fffbca3da867448593ded2bacfb10cf3",
            "value": " 426M/426M [00:48&lt;00:00, 22.4MB/s]"
          }
        },
        "1f6c62e85009488bb169d2cfebf7c8c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53dfe59086fc46d2924ad486a4c62ea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "927d1f423dda4a13973beec2391d3184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fa833b80b7d4a2595b0af641c930983": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01ad71c71b574ae5ab94523c9933e5a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8063f9c6c8c34116a17091ae5c57b43a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fffbca3da867448593ded2bacfb10cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d313c31a33b84e7eba8848def376e43a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_954f9c8f54b242ef83e63af3eb62aac1",
              "IPY_MODEL_3b5dfdd117884cc881bf720dbebd3d50",
              "IPY_MODEL_23148a30d45f4b7393987e9e1f8f8f91"
            ],
            "layout": "IPY_MODEL_a699774dc155421da14bc7f965bcc254"
          }
        },
        "954f9c8f54b242ef83e63af3eb62aac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f5f9dfa336a40ce848a2089305de48b",
            "placeholder": "​",
            "style": "IPY_MODEL_48eb77ed981e4ee1b086d4baff347fc4",
            "value": "Downloading data: 100%"
          }
        },
        "3b5dfdd117884cc881bf720dbebd3d50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c238e215a0ea44c7bb07d94a0323b352",
            "max": 436739796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48045fd87c674076b686890d68cc6d36",
            "value": 436739796
          }
        },
        "23148a30d45f4b7393987e9e1f8f8f91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e331f7ba10594060886c93a722eda369",
            "placeholder": "​",
            "style": "IPY_MODEL_6b5d228d6a1643bb9265c8ed4bb16228",
            "value": " 437M/437M [00:08&lt;00:00, 60.2MB/s]"
          }
        },
        "a699774dc155421da14bc7f965bcc254": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f5f9dfa336a40ce848a2089305de48b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48eb77ed981e4ee1b086d4baff347fc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c238e215a0ea44c7bb07d94a0323b352": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48045fd87c674076b686890d68cc6d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e331f7ba10594060886c93a722eda369": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b5d228d6a1643bb9265c8ed4bb16228": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6f38870cf1a4a988df23df3299fdafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f747ec060b142b5aa1b2fcdc6abc5ff",
              "IPY_MODEL_076463e13fe14f008408256b79279f1f",
              "IPY_MODEL_5983effd2ecc480fa7bf3e0d5597d099"
            ],
            "layout": "IPY_MODEL_2db9534ddddd4476b107cd28341c2c54"
          }
        },
        "0f747ec060b142b5aa1b2fcdc6abc5ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55f2a7c0ba3b40f4a638f92e2d0daa29",
            "placeholder": "​",
            "style": "IPY_MODEL_0e48c7a6c02043b597e6d16d95c1cf11",
            "value": "Extracting data files: 100%"
          }
        },
        "076463e13fe14f008408256b79279f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4a3b84aada14a2b9dc8da4acf01ae35",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e548832e8ce42da8c8263b464bccdf6",
            "value": 2
          }
        },
        "5983effd2ecc480fa7bf3e0d5597d099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6c2cf45b1584518bc525d79a1756ca6",
            "placeholder": "​",
            "style": "IPY_MODEL_03e91cd759b949f7b21c1dcec8856b0a",
            "value": " 2/2 [00:00&lt;00:00, 58.89it/s]"
          }
        },
        "2db9534ddddd4476b107cd28341c2c54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55f2a7c0ba3b40f4a638f92e2d0daa29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e48c7a6c02043b597e6d16d95c1cf11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4a3b84aada14a2b9dc8da4acf01ae35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e548832e8ce42da8c8263b464bccdf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6c2cf45b1584518bc525d79a1756ca6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03e91cd759b949f7b21c1dcec8856b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae6ccec6b9904f498eb11d92a31fde0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3dda0d384ea74cb3bde47c194ebb6bb0",
              "IPY_MODEL_de0f6e28c87c4f73991b56c4ebd8a037",
              "IPY_MODEL_14ef9e0fbf924402883b4b7572437f4f"
            ],
            "layout": "IPY_MODEL_28d2a48900f74d85843b54376db841b6"
          }
        },
        "3dda0d384ea74cb3bde47c194ebb6bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_871e5d2b32bb4cf2889c54dd9d257359",
            "placeholder": "​",
            "style": "IPY_MODEL_c6f33b08707d4b7c85aacffadaa33a9a",
            "value": "100%"
          }
        },
        "de0f6e28c87c4f73991b56c4ebd8a037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20b84954480e4855aadb2160141d0411",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_136820669e874d63bdbfd76277a04f63",
            "value": 2
          }
        },
        "14ef9e0fbf924402883b4b7572437f4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4200d5278d50456bb0c2989c88e2dcff",
            "placeholder": "​",
            "style": "IPY_MODEL_e8f99e8443e74201b08ee1a7dea5d06e",
            "value": " 2/2 [00:00&lt;00:00,  9.45it/s]"
          }
        },
        "28d2a48900f74d85843b54376db841b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "871e5d2b32bb4cf2889c54dd9d257359": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6f33b08707d4b7c85aacffadaa33a9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20b84954480e4855aadb2160141d0411": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "136820669e874d63bdbfd76277a04f63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4200d5278d50456bb0c2989c88e2dcff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8f99e8443e74201b08ee1a7dea5d06e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61048689ae8e47b5a54983fa42a1a7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82f974bf809742dd99f23a292f16da50",
              "IPY_MODEL_779d92344cb7484cb5579a45d28b0fe3",
              "IPY_MODEL_819703063a004a69930dff87196d08d6"
            ],
            "layout": "IPY_MODEL_b397b770c75a4ea095ebca3a403d970d"
          }
        },
        "82f974bf809742dd99f23a292f16da50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d30056927e114099b5f6bfb9dec3fa53",
            "placeholder": "​",
            "style": "IPY_MODEL_b30b64bc0f634e4893b281fc9cd7658b",
            "value": "100%"
          }
        },
        "779d92344cb7484cb5579a45d28b0fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f22f7b3ba35940689f330286cf596d0a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a9675ebcfec425d91a5b500ffc21a0c",
            "value": 2
          }
        },
        "819703063a004a69930dff87196d08d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_326b3a90551a478a837338d45c5677f7",
            "placeholder": "​",
            "style": "IPY_MODEL_db620b8127614afe801ae22988fb667b",
            "value": " 2/2 [00:00&lt;00:00, 65.12it/s]"
          }
        },
        "b397b770c75a4ea095ebca3a403d970d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d30056927e114099b5f6bfb9dec3fa53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b30b64bc0f634e4893b281fc9cd7658b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f22f7b3ba35940689f330286cf596d0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a9675ebcfec425d91a5b500ffc21a0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "326b3a90551a478a837338d45c5677f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db620b8127614afe801ae22988fb667b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gary109/Colab_Notebooks/blob/main/Fine_Tune_Crop14_Image_classification_with_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 確認 GPU 類型\n",
        "---"
      ],
      "metadata": {
        "id": "2ozr9xyv0ZMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "  raise Exception(\"GPU not availalbe. CPU training will be too slow.\")\n",
        "print(\"device name\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJgt_njFcA4R",
        "outputId": "52c223fe-51a8-4e6b-8253-344119df835f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device name Tesla P100-PCIE-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 是否要掛載 Google Drive\n",
        "---"
      ],
      "metadata": {
        "id": "2kKsDcHu0hlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yEXRaZK1lWUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fcc1fa8-b6e6-442c-a145-c33eb13b28ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#確認 ＴＰＵ規格"
      ],
      "metadata": {
        "id": "kYFkur3sEb6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "metadata": {
        "id": "ZCfuY-RMEctt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 安裝 transformers,datastes,... 相依套件\n",
        "---"
      ],
      "metadata": {
        "id": "1O9n-3Ak0rik"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8eh87Hoee5d"
      },
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/huggingface/datasets.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install soundfile\n",
        "!pip install jiwer\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!apt install git-lfs\n",
        "!git config --global user.email \"gary109@gmail.com\"\n",
        "!git config --global user.name \"GARY\"\n",
        "!git config --global credential.helper store\n",
        "!pip install wandb\n",
        "!wandb login 2cf656515a3b158f4f603aeba63181236de2fc1b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 登入 huggingface \n",
        "---"
      ],
      "metadata": {
        "id": "A1JcSRcJ0_uA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! huggingface-cli login\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLI9ee6CkBQg",
        "outputId": "c2f64b2b-6d8f-499f-8180-e27053e19705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens.\n",
            "        (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C.\n",
            "        \n",
            "Token: \n",
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 下載 ai-cup-2022-crop_classification 程式碼\n",
        "--- "
      ],
      "metadata": {
        "id": "NhO8MiJ41HLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://gary109:Gygy844109109@gitlab.com/gary109/ai-cup-2022-crop_classification.git"
      ],
      "metadata": {
        "id": "IfPGSNnqqdLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 載入 crop14-small crop14-balance crop14-pretrain 訓練資料集\n",
        "---"
      ],
      "metadata": {
        "id": "RcO8DSXZz7W5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "# # dataset = load_dataset(\"/content/ai-cup-2022-crop_classification/datasets/crop14.py\", 'crop14-small')\n",
        "# dataset = load_dataset(\"/content/crop14.py\", 'crop14-balance')\n",
        "# dataset = load_dataset(\"crop14.py\", 'crop14-small',use_auth_token=True,cache_dir='./ggg')\n",
        "# dataset = load_dataset(\"gary109/crop14_balance\", use_auth_token=True)\n",
        "dataset = load_dataset(\"gary109/crop14-small\", use_auth_token=True)\n",
        "# dataset = load_dataset(\"/content/drive/MyDrive/datasets/crop14_colab.py\", 'crop14-pretrain', cache_dir='/content/drive/MyDrive/datasets/crop14-pretrain')\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629,
          "referenced_widgets": [
            "5d0beac885c5441288e24a40d888f67f",
            "97c8893d43c64ea7b67f7c0c51febadf",
            "976f0611f2784dafaff403567d040689",
            "84ada6de0c6540d49402fc9becbfa8ff",
            "52ab8f354e22475ea246e5974240582b",
            "81f19a31ad71420aa7a25a025960f329",
            "00241670ea8140f6a91ff0269bc091d5",
            "658b6c48ab8248b1917a00bca7564b9f",
            "ee12c50b0ab44d8d872e6d51dae8c2d1",
            "1acdf2eeac6f4cdd956e7bb8ddc2250f",
            "3b5d698c3a8e43b59f8f56e5aa5f7edc",
            "3e5945e353564fec8e53f1b756691f9e",
            "098d70cad85d4b3c8058f3794944ddb7",
            "24b4f7d64f114a079f96f2436dc7d61c",
            "f7048b9d0d4a4f8bb33a841a4994e5b1",
            "8c0712895e3049f088fb93c4e8914def",
            "f8b1921d6c134b4db25c65f8f4ad9a64",
            "619032123dc54b0e9075a2dd3538a821",
            "822fbddc6ca1466c938e5987c7f0cfdf",
            "debe71e277df44a4a13bf165c40ee469",
            "fb32d0d789304de59102e04aa2ff96de",
            "49574e906bd543c0932b2fc8f9be943b",
            "6eef87135bb646aa9a726dfd10aa0610",
            "17a1b6aadf3f4b53a60fa68b236afce6",
            "1f460951fa4f48a8bb7360c7b2188431",
            "1bdd094f5fe44de9937495cb7b76b823",
            "851a313c4f0c4c678cb2e0dcb2f05e0d",
            "81f7d5f3088c4c409e7c0f1727400832",
            "4c6ecbfa651845ea9ff23bdaf60bbdd6",
            "3e986e5094684577bee744389ccc51e0",
            "9da5a9cdfdad4a819053124962686a4a",
            "827f286bdeda4dd09671c6b0a4af8dfa",
            "f69285f67c5b4e2cbbc152ee1a4c0d9f",
            "bcaed37bfacb4911b3d305d08d5255f2",
            "ef23103ecf8a4390bc6ea08746ee11c4",
            "d36bfcf8708f4474ad1ccc42a434014b",
            "1a1b82e662a94c628a004a83146251fe",
            "0b3f991b89ca4da8b4365c24bbd2abcc",
            "064cf0e017b046c2bc87f817b4b7d760",
            "c643ec7ddb714f68bb9180d55ce997a3",
            "b1b984e9fc1941c1aec6389343d37f67",
            "03b088d198da4d70851a7f7df0df03ab",
            "36295e647e5c41ebbf2dc0ebca345a21",
            "aa0aa6c89f8f4d67932a1bac9e5fc902",
            "84ace2dd82204797a8608ad9aa042d82",
            "a51595d0e4de48298c07925e83edf152",
            "a838ad5de5364344b732fee963dc76f3",
            "95dd0995db4941fbac0aef701fd4cba2",
            "3210b799d54942429c28b7e9f4153352",
            "0f62bf871bdc4d539d29c39db6ae05f6",
            "02b7305606de4fb09ce30b7c1914b3f9",
            "cfa11993737e401bbd7a04e917acab4d",
            "33f7d7ac395a49a783de824aea79d329",
            "ce4b558070644522af5e887098644cc6",
            "965e38112de74dae8456a480bd9a3c0b",
            "0b3f37f4ad944a0fb5e43c0cf8495bb9",
            "25db28b1476a4a27b2a03514d69ee87c",
            "3d3c1a975cc94dfba0923b9c4d32bc12",
            "994e15733bd94024a2ed7e03c2a6f538",
            "382b5daea4b2454e8a656a44fdb27f8e",
            "76933da29bdb4ad0b30b996303101571",
            "62d9970291b14993bc4cd1c2d5101702",
            "6e5f3eca32194d02b108b8c21a246b16",
            "b529b210cdcb4c07b38aa3a6f19fafee",
            "29b5044460ee42a8acced1835b3f9337",
            "09670b1aa2b644f3809bf7d437f72603",
            "f6b76afce6fc4b6794b5421437e33254",
            "15ca137215c742ee8d976c29d703fee4",
            "dbf1db680cbc444bb4c572de13b07ce8",
            "3d863bf0d3654804913120a5d82597ac",
            "b8920da35f534308882bb8f225bdc6fe",
            "1b8454491ddd405ea4a4af1aa1f4b0e6",
            "a020aac6b0da468898418987e2dc4421",
            "64a5b629e25d4b8982a7dc054425dede",
            "52ee799b298f4ac8a690c0a9d3978dbd",
            "e8bf63a1bd8643b182504e0b37214b72",
            "dcd15d96e2bc4c588b13203651a3db29",
            "3f77427fc0274c9ab4ba03d4981a62ef",
            "e0d7cc687cba41a6b3157499b241fbad",
            "aa0abea829284c22b969d7c76af92296",
            "c163a50b11ca4beab5a39f8c7db8c92d",
            "1f6c62e85009488bb169d2cfebf7c8c1",
            "53dfe59086fc46d2924ad486a4c62ea1",
            "927d1f423dda4a13973beec2391d3184",
            "1fa833b80b7d4a2595b0af641c930983",
            "01ad71c71b574ae5ab94523c9933e5a9",
            "8063f9c6c8c34116a17091ae5c57b43a",
            "fffbca3da867448593ded2bacfb10cf3",
            "d313c31a33b84e7eba8848def376e43a",
            "954f9c8f54b242ef83e63af3eb62aac1",
            "3b5dfdd117884cc881bf720dbebd3d50",
            "23148a30d45f4b7393987e9e1f8f8f91",
            "a699774dc155421da14bc7f965bcc254",
            "4f5f9dfa336a40ce848a2089305de48b",
            "48eb77ed981e4ee1b086d4baff347fc4",
            "c238e215a0ea44c7bb07d94a0323b352",
            "48045fd87c674076b686890d68cc6d36",
            "e331f7ba10594060886c93a722eda369",
            "6b5d228d6a1643bb9265c8ed4bb16228",
            "b6f38870cf1a4a988df23df3299fdafc",
            "0f747ec060b142b5aa1b2fcdc6abc5ff",
            "076463e13fe14f008408256b79279f1f",
            "5983effd2ecc480fa7bf3e0d5597d099",
            "2db9534ddddd4476b107cd28341c2c54",
            "55f2a7c0ba3b40f4a638f92e2d0daa29",
            "0e48c7a6c02043b597e6d16d95c1cf11",
            "e4a3b84aada14a2b9dc8da4acf01ae35",
            "2e548832e8ce42da8c8263b464bccdf6",
            "d6c2cf45b1584518bc525d79a1756ca6",
            "03e91cd759b949f7b21c1dcec8856b0a",
            "ae6ccec6b9904f498eb11d92a31fde0e",
            "3dda0d384ea74cb3bde47c194ebb6bb0",
            "de0f6e28c87c4f73991b56c4ebd8a037",
            "14ef9e0fbf924402883b4b7572437f4f",
            "28d2a48900f74d85843b54376db841b6",
            "871e5d2b32bb4cf2889c54dd9d257359",
            "c6f33b08707d4b7c85aacffadaa33a9a",
            "20b84954480e4855aadb2160141d0411",
            "136820669e874d63bdbfd76277a04f63",
            "4200d5278d50456bb0c2989c88e2dcff",
            "e8f99e8443e74201b08ee1a7dea5d06e"
          ]
        },
        "id": "BDNRJpgbkBTg",
        "outputId": "01de970a-a99a-4bc2-cc7c-bb05fbe89d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.25k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d0beac885c5441288e24a40d888f67f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration gary109--crop14-small-d2283ee3b6a9a5fe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset crop14/crop14-small (download: 2.73 GiB, generated: 2.77 GiB, post-processed: Unknown size, total: 5.51 GiB) to /root/.cache/huggingface/datasets/gary109___parquet/gary109--crop14-small-d2283ee3b6a9a5fe/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e5945e353564fec8e53f1b756691f9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/304M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6eef87135bb646aa9a726dfd10aa0610"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/458M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bcaed37bfacb4911b3d305d08d5255f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/433M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84ace2dd82204797a8608ad9aa042d82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/447M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b3f37f4ad944a0fb5e43c0cf8495bb9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/430M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6b76afce6fc4b6794b5421437e33254"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/426M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f77427fc0274c9ab4ba03d4981a62ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/437M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d313c31a33b84e7eba8848def376e43a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6f38870cf1a4a988df23df3299fdafc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/gary109___parquet/gary109--crop14-small-d2283ee3b6a9a5fe/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae6ccec6b9904f498eb11d92a31fde0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    validation: Dataset({\n",
              "        features: ['filename', 'image', 'labels'],\n",
              "        num_rows: 140\n",
              "    })\n",
              "    train: Dataset({\n",
              "        features: ['filename', 'image', 'labels'],\n",
              "        num_rows: 1260\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.push_to_hub(\"gary109/crop14-small\")\n",
        "# dataset.push_to_hub(\"gary109/crop14-pretrain\")"
      ],
      "metadata": {
        "id": "BjAdBmoLkBWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.utils.dummy_vision_objects import ImageGPTFeatureExtractor\n",
        "import random\n",
        "from PIL import ImageDraw, ImageFont, Image\n",
        "\n",
        "# def show_examples(ds, seed: int = 1234, examples_per_class: int = 3, size=(100, 100)):\n",
        "\n",
        "#     w, h = size\n",
        "#     labels = ds['train'].features['labels'].names\n",
        "#     # labels = labels[:9]\n",
        "#     grid = Image.new('RGB', size=(examples_per_class * w, len(labels) * h))\n",
        "#     draw = ImageDraw.Draw(grid)\n",
        "#     font = ImageFont.truetype(\"./fonts/LiberationMono-Bold.ttf\", 24)\n",
        "#     for label_id, label in enumerate(labels):\n",
        "\n",
        "#         # Filter the dataset by a single label, shuffle it, and grab a few samples\n",
        "#         ds_slice = ds['train'].filter(lambda ex: ex['labels'] == label_id).shuffle(seed).select(range(examples_per_class))\n",
        "\n",
        "#         # Plot this label's examples along a row\n",
        "#         for i, example in enumerate(ds_slice):\n",
        "#             image = example['image']\n",
        "#             idx = examples_per_class * label_id + i\n",
        "#             box = (idx % examples_per_class * w, idx // examples_per_class * h)\n",
        "#             grid.paste(image.resize(size), box=box)\n",
        "#             draw.text(box, str(label), (255, 255, 255), font=font)\n",
        "\n",
        "#     return grid\n",
        "\n",
        "# show_examples(dataset, seed=random.randint(0, 1337), examples_per_class=3)\n",
        "dataset['train'][0]['image']"
      ],
      "metadata": {
        "id": "PoHV-3ge2iRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 安裝加速器\n",
        "---"
      ],
      "metadata": {
        "id": "F3AZ8EIyzzir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install accelerate deepspeed"
      ],
      "metadata": {
        "id": "u6_ypCvRkBdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MFGLCKC5DHC",
        "outputId": "a495d388-f9a6-4ba3-8bc5-a85b4e62bdd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0\n",
            "Which type of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU): 0\n",
            "Do you want to run your training on CPU only (even if a GPU is available)? [yes/NO]:\n",
            "Do you want to use DeepSpeed? [yes/NO]: \n",
            "How many processes in total will you use? [1]: \n",
            "Do you wish to use FP16 or BF16 (mixed precision)? [NO/fp16/bf16]: fp16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate test"
      ],
      "metadata": {
        "id": "I0G_8Qs65EZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FOR TPU needs\n",
        "---"
      ],
      "metadata": {
        "id": "5lYnSRfsvhd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip uninstall -y torch\n",
        "!pip install torch==1.8.2+cpu torchvision==0.9.2+cpu -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "94vK4zQPvfGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 開始訓練\n",
        "---"
      ],
      "metadata": {
        "id": "hdZtjk7Uy2e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/transformers/examples/pytorch/image-pretraining\n",
        "!cp /content/drive/MyDrive/datasets/run_mae.py /content\n",
        "!cp /content/drive/MyDrive/datasets/crop14.py /content\n",
        "!cp /content/drive/MyDrive/datasets/crop14_colab.py /content\n",
        "!cp /content/drive/MyDrive/datasets/run_mim.py /content\n",
        "!cp /content/drive/MyDrive/datasets/run_image_classification.py /content\n",
        "!cp /content/drive/MyDrive/datasets/run_image_classification_no_trainer.py /content\n",
        "!cp /content/drive/MyDrive/datasets/run_image_classification_ViT-MAE.py /content"
      ],
      "metadata": {
        "id": "7E5Kms3u0Gx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tune\n",
        "---\n",
        "- google\n",
        "    - google/vit-base-patch16-224-in21k\n",
        "    - google/vit-base-patch32-224-in21k\n",
        "    - google/vit-large-patch16-224-in21k\n",
        "    - google/vit-large-patch32-224-in21k\n",
        "    - google/vit-huge-patch14-224-in21k\n",
        "---\n",
        "- gary109\n",
        "    - gary109/crop14-small_pretrain_vit-base-mim\n",
        "    - gary109/crop14-small_pretrain_vit-mae-large\n",
        "---\n"
      ],
      "metadata": {
        "id": "msyHAoNs_gbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### google/vit-base-patch16-224-in21k\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3qN-Czm1S0OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch run_image_classification.py \\\n",
        "    --model_name_or_path \"google/vit-base-patch16-224-in21k\" \\\n",
        "    --dataset_name \"gary109/crop14-small\" \\\n",
        "    --output_dir ./crop14-small_vit-base-patch16-224-in21k/ \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id crop14-small_vit-base-patch16-224-in21k \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 20 \\\n",
        "    --per_device_train_batch_size 80 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "id": "tNMZWv7q0G02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### google/vit-base-patch32-224-in21k\n",
        "---\n"
      ],
      "metadata": {
        "id": "18piHpEmt2DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch run_image_classification.py \\\n",
        "    --model_name_or_path \"google/vit-base-patch32-224-in21k\" \\\n",
        "    --dataset_name \"gary109/crop14-small\" \\\n",
        "    --output_dir ./crop14-small_vit-base-patch32-224-in21k/ \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id crop14-small_vit-base-patch32-224-in21k \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 30 \\\n",
        "    --per_device_train_batch_size 80 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "id": "XTWxM4p30G3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### google/vit-large-patch16-224-in21k\n",
        "---\n"
      ],
      "metadata": {
        "id": "SQ-QYbKst7i9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch run_image_classification.py \\\n",
        "    --model_name_or_path \"google/vit-large-patch16-224-in21k\" \\\n",
        "    --dataset_name \"gary109/crop14-small\" \\\n",
        "    --output_dir ./crop14-small_vit-large-patch16-224-in21k/ \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id crop14-small_vit-large-patch16-224-in21k \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 30 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 3 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "id": "Xo5w2JgH0G6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### google/vit-large-patch32-224-in21k\n",
        "---"
      ],
      "metadata": {
        "id": "qa7cWXbvzOHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch run_image_classification.py \\\n",
        "    --model_name_or_path \"google/vit-large-patch32-224-in21k\" \\\n",
        "    --dataset_name \"gary109/crop14-small\" \\\n",
        "    --output_dir ./crop14-small_vit-large-patch32-224-in21k/ \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id crop14-small_vit-large-patch32-224-in21k \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 30 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 1 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsyBqto3d98Q",
        "outputId": "1e2e229d-d6bd-485a-91b7-7fee16c03da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.8\n",
            "[2022-04-27 05:23:41,789] [WARNING] [partition_parameters.py:54:<module>] unable to find torch.distributed._all_gather_base. will fall back to torch.distributed.all_gather which will result in suboptimal performance. please consider upgrading your pytorch installation.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/training_args.py:978: FutureWarning: `--push_to_hub_model_id` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case gary109/crop14-small_vit-large-patch32-224-in21k).\n",
            "  FutureWarning,\n",
            "WARNING:run_image_classification:Process rank: -1, device: xla:1, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "INFO:run_image_classification:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=gary109/crop14-small_vit-large-patch32-224-in21k,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./crop14-small_vit-large-patch32-224-in21k/runs/Apr27_05-23-51_534d9a9e5829,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=./crop14-small_vit-large-patch32-224-in21k/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=True,\n",
            "push_to_hub_model_id=crop14-small_vit-large-patch32-224-in21k,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=False,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./crop14-small_vit-large-patch32-224-in21k/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1337,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration gary109--crop14-small-d2283ee3b6a9a5fe\n",
            "WARNING:datasets.builder:Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/gary109--crop14-small-d2283ee3b6a9a5fe/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
            "100% 2/2 [00:00<00:00, 20.09it/s]\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/gary109--crop14-small-d2283ee3b6a9a5fe/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-533055645b70b0e3.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/gary109--crop14-small-d2283ee3b6a9a5fe/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-d31679b57f3ea77f.arrow\n",
            "[INFO|hub.py:583] 2022-04-27 05:24:04,622 >> https://huggingface.co/google/vit-large-patch32-224-in21k/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvdrh5id8\n",
            "Downloading: 100% 504/504 [00:00<00:00, 426kB/s]\n",
            "[INFO|hub.py:587] 2022-04-27 05:24:04,766 >> storing https://huggingface.co/google/vit-large-patch32-224-in21k/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/26d9776471c741e65abbb75d28b2e67bc32bfcb20c3742f6eb78cd29d265a40e.b22282264c2faaa3b0a3a5c86e9e1728ee6fe16a0bd619f596f6c764fa21ad5c\n",
            "[INFO|hub.py:595] 2022-04-27 05:24:04,766 >> creating metadata file for /root/.cache/huggingface/transformers/26d9776471c741e65abbb75d28b2e67bc32bfcb20c3742f6eb78cd29d265a40e.b22282264c2faaa3b0a3a5c86e9e1728ee6fe16a0bd619f596f6c764fa21ad5c\n",
            "[INFO|configuration_utils.py:659] 2022-04-27 05:24:04,766 >> loading configuration file https://huggingface.co/google/vit-large-patch32-224-in21k/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/26d9776471c741e65abbb75d28b2e67bc32bfcb20c3742f6eb78cd29d265a40e.b22282264c2faaa3b0a3a5c86e9e1728ee6fe16a0bd619f596f6c764fa21ad5c\n",
            "[INFO|configuration_utils.py:704] 2022-04-27 05:24:04,767 >> Model config ViTConfig {\n",
            "  \"_name_or_path\": \"google/vit-large-patch32-224-in21k\",\n",
            "  \"architectures\": [\n",
            "    \"ViTModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"finetuning_task\": \"image-classification\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"peanut\",\n",
            "    \"1\": \"corn\",\n",
            "    \"10\": \"carrot\",\n",
            "    \"11\": \"dragonfruit\",\n",
            "    \"12\": \"pumpkin\",\n",
            "    \"13\": \"tomato\",\n",
            "    \"2\": \"bareland\",\n",
            "    \"3\": \"pineapple\",\n",
            "    \"4\": \"rice\",\n",
            "    \"5\": \"garlic\",\n",
            "    \"6\": \"soybean\",\n",
            "    \"7\": \"guava\",\n",
            "    \"8\": \"banana\",\n",
            "    \"9\": \"sugarcane\"\n",
            "  },\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"banana\": \"8\",\n",
            "    \"bareland\": \"2\",\n",
            "    \"carrot\": \"10\",\n",
            "    \"corn\": \"1\",\n",
            "    \"dragonfruit\": \"11\",\n",
            "    \"garlic\": \"5\",\n",
            "    \"guava\": \"7\",\n",
            "    \"peanut\": \"0\",\n",
            "    \"pineapple\": \"3\",\n",
            "    \"pumpkin\": \"12\",\n",
            "    \"rice\": \"4\",\n",
            "    \"soybean\": \"6\",\n",
            "    \"sugarcane\": \"9\",\n",
            "    \"tomato\": \"13\"\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"patch_size\": 32,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.19.0.dev0\"\n",
            "}\n",
            "\n",
            "[INFO|hub.py:583] 2022-04-27 05:24:04,935 >> https://huggingface.co/google/vit-large-patch32-224-in21k/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpcs8wt99w\n",
            "Downloading: 100% 1.14G/1.14G [00:24<00:00, 50.1MB/s]\n",
            "[INFO|hub.py:587] 2022-04-27 05:24:29,501 >> storing https://huggingface.co/google/vit-large-patch32-224-in21k/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/c798624fccd0b60fd8d493fc5a2b107c919791a29ba72aa480be070637fcf27d.ae7f6b3c7153ae7db3381de4b30b21ef45dca8264b32a3a456f21df8fdbe2326\n",
            "[INFO|hub.py:595] 2022-04-27 05:24:29,502 >> creating metadata file for /root/.cache/huggingface/transformers/c798624fccd0b60fd8d493fc5a2b107c919791a29ba72aa480be070637fcf27d.ae7f6b3c7153ae7db3381de4b30b21ef45dca8264b32a3a456f21df8fdbe2326\n",
            "[INFO|modeling_utils.py:1865] 2022-04-27 05:24:29,502 >> loading weights file https://huggingface.co/google/vit-large-patch32-224-in21k/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/c798624fccd0b60fd8d493fc5a2b107c919791a29ba72aa480be070637fcf27d.ae7f6b3c7153ae7db3381de4b30b21ef45dca8264b32a3a456f21df8fdbe2326\n",
            "[WARNING|modeling_utils.py:2167] 2022-04-27 05:24:35,679 >> Some weights of the model checkpoint at google/vit-large-patch32-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2178] 2022-04-27 05:24:35,679 >> Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-large-patch32-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[INFO|hub.py:583] 2022-04-27 05:24:35,892 >> https://huggingface.co/google/vit-large-patch32-224-in21k/resolve/main/preprocessor_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpm__yp63g\n",
            "Downloading: 100% 160/160 [00:00<00:00, 128kB/s]\n",
            "[INFO|hub.py:587] 2022-04-27 05:24:36,042 >> storing https://huggingface.co/google/vit-large-patch32-224-in21k/resolve/main/preprocessor_config.json in cache at /root/.cache/huggingface/transformers/dced343a4e27f94ffc04c579ec5135530cd739d13ed6ad686a11ce73177fc458.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
            "[INFO|hub.py:595] 2022-04-27 05:24:36,042 >> creating metadata file for /root/.cache/huggingface/transformers/dced343a4e27f94ffc04c579ec5135530cd739d13ed6ad686a11ce73177fc458.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
            "[INFO|feature_extraction_utils.py:465] 2022-04-27 05:24:36,043 >> loading feature extractor configuration file https://huggingface.co/google/vit-large-patch32-224-in21k/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/dced343a4e27f94ffc04c579ec5135530cd739d13ed6ad686a11ce73177fc458.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
            "[INFO|configuration_utils.py:659] 2022-04-27 05:24:36,198 >> loading configuration file https://huggingface.co/google/vit-large-patch32-224-in21k/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/26d9776471c741e65abbb75d28b2e67bc32bfcb20c3742f6eb78cd29d265a40e.b22282264c2faaa3b0a3a5c86e9e1728ee6fe16a0bd619f596f6c764fa21ad5c\n",
            "[INFO|configuration_utils.py:704] 2022-04-27 05:24:36,199 >> Model config ViTConfig {\n",
            "  \"_name_or_path\": \"google/vit-large-patch32-224-in21k\",\n",
            "  \"architectures\": [\n",
            "    \"ViTModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"patch_size\": 32,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.19.0.dev0\"\n",
            "}\n",
            "\n",
            "[INFO|feature_extraction_utils.py:501] 2022-04-27 05:24:36,202 >> Feature extractor ViTFeatureExtractor {\n",
            "  \"do_normalize\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
            "  \"image_mean\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"image_std\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"resample\": 2,\n",
            "  \"size\": 224\n",
            "}\n",
            "\n",
            "Cloning https://huggingface.co/gary109/crop14-small_vit-large-patch32-224-in21k into local empty directory.\n",
            "WARNING:huggingface_hub.repository:Cloning https://huggingface.co/gary109/crop14-small_vit-large-patch32-224-in21k into local empty directory.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1317] 2022-04-27 05:24:56,268 >> ***** Running training *****\n",
            "[INFO|trainer.py:1318] 2022-04-27 05:24:56,268 >>   Num examples = 1260\n",
            "[INFO|trainer.py:1319] 2022-04-27 05:24:56,269 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1320] 2022-04-27 05:24:56,269 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1321] 2022-04-27 05:24:56,269 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1322] 2022-04-27 05:24:56,269 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1323] 2022-04-27 05:24:56,269 >>   Total optimization steps = 2370\n",
            "{'loss': 2.6063, 'learning_rate': 1.9915611814345993e-05, 'epoch': 0.13}\n",
            "{'loss': 2.5687, 'learning_rate': 1.9831223628691984e-05, 'epoch': 0.25}\n",
            "{'loss': 2.498, 'learning_rate': 1.974683544303798e-05, 'epoch': 0.38}\n",
            "{'loss': 2.4343, 'learning_rate': 1.9662447257383967e-05, 'epoch': 0.51}\n",
            "{'loss': 2.3378, 'learning_rate': 1.957805907172996e-05, 'epoch': 0.63}\n",
            "{'loss': 2.2907, 'learning_rate': 1.949367088607595e-05, 'epoch': 0.76}\n",
            "{'loss': 2.2398, 'learning_rate': 1.9409282700421944e-05, 'epoch': 0.89}\n",
            "  3% 79/2370 [04:30<1:11:47,  1.88s/it][INFO|trainer.py:2443] 2022-04-27 05:30:04,817 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 05:30:04,818 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 05:30:04,818 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 14.35it/s]\u001b[A\n",
            " 56% 5/9 [00:07<00:06,  1.73s/it]\u001b[A\n",
            " 78% 7/9 [00:07<00:02,  1.06s/it]\u001b[A\n",
            " 89% 8/9 [00:07<00:00,  1.19it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.068852424621582, 'eval_accuracy': 0.7428571428571429, 'eval_runtime': 35.6423, 'eval_samples_per_second': 3.928, 'eval_steps_per_second': 0.253, 'epoch': 1.0}\n",
            "  3% 79/2370 [05:44<1:11:47,  1.88s/it]\n",
            "100% 9/9 [00:15<00:00,  2.62s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 05:30:40,461 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-79\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 05:30:40,463 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-79/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 05:30:50,363 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-79/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 05:30:50,368 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-79/preprocessor_config.json\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 05:31:25,311 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/preprocessor_config.json\n",
            "{'loss': 2.1015, 'learning_rate': 1.9324894514767935e-05, 'epoch': 1.01}\n",
            "{'loss': 2.0029, 'learning_rate': 1.9240506329113926e-05, 'epoch': 1.14}\n",
            "{'loss': 1.8929, 'learning_rate': 1.9156118143459917e-05, 'epoch': 1.27}\n",
            "{'loss': 1.8029, 'learning_rate': 1.907172995780591e-05, 'epoch': 1.39}\n",
            "{'loss': 1.7647, 'learning_rate': 1.89873417721519e-05, 'epoch': 1.52}\n",
            "{'loss': 1.7351, 'learning_rate': 1.890295358649789e-05, 'epoch': 1.65}\n",
            "{'loss': 1.6779, 'learning_rate': 1.8818565400843886e-05, 'epoch': 1.77}\n",
            "{'loss': 1.5726, 'learning_rate': 1.8734177215189874e-05, 'epoch': 1.9}\n",
            "  7% 158/2370 [11:32<1:20:03,  2.17s/it][INFO|trainer.py:2443] 2022-04-27 05:36:28,761 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 05:36:28,761 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 05:36:28,761 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 22% 2/9 [00:00<00:00, 13.54it/s]\u001b[A\n",
            " 44% 4/9 [00:00<00:00,  9.76it/s]\u001b[A\n",
            " 67% 6/9 [00:15<00:10,  3.52s/it]\u001b[A\n",
            " 78% 7/9 [00:15<00:05,  2.68s/it]\u001b[A\n",
            " 89% 8/9 [00:15<00:02,  2.02s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 1.4734152555465698, 'eval_accuracy': 0.8357142857142857, 'eval_runtime': 35.1751, 'eval_samples_per_second': 3.98, 'eval_steps_per_second': 0.256, 'epoch': 2.0}\n",
            "  7% 158/2370 [12:07<1:20:03,  2.17s/it]\n",
            "100% 9/9 [00:18<00:00,  2.12s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 05:37:03,938 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-158\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 05:37:03,941 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-158/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 05:37:15,269 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-158/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 05:37:15,348 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-158/preprocessor_config.json\n",
            "{'loss': 1.4756, 'learning_rate': 1.8649789029535868e-05, 'epoch': 2.03}\n",
            "{'loss': 1.3698, 'learning_rate': 1.856540084388186e-05, 'epoch': 2.15}\n",
            "{'loss': 1.319, 'learning_rate': 1.848101265822785e-05, 'epoch': 2.28}\n",
            "{'loss': 1.2555, 'learning_rate': 1.8396624472573842e-05, 'epoch': 2.41}\n",
            "{'loss': 1.2028, 'learning_rate': 1.8312236286919833e-05, 'epoch': 2.53}\n",
            "{'loss': 1.2694, 'learning_rate': 1.8227848101265824e-05, 'epoch': 2.66}\n",
            "{'loss': 1.1595, 'learning_rate': 1.8143459915611816e-05, 'epoch': 2.78}\n",
            "{'loss': 1.1788, 'learning_rate': 1.8059071729957807e-05, 'epoch': 2.91}\n",
            " 10% 237/2370 [17:26<1:14:00,  2.08s/it][INFO|trainer.py:2443] 2022-04-27 05:42:22,731 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 05:42:22,731 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 05:42:22,731 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 22% 2/9 [00:00<00:00, 16.14it/s]\u001b[A\n",
            " 44% 4/9 [00:00<00:00,  9.34it/s]\u001b[A\n",
            " 67% 6/9 [00:16<00:11,  3.69s/it]\u001b[A\n",
            " 78% 7/9 [00:16<00:05,  2.81s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 1.0977087020874023, 'eval_accuracy': 0.8571428571428571, 'eval_runtime': 34.5378, 'eval_samples_per_second': 4.054, 'eval_steps_per_second': 0.261, 'epoch': 3.0}\n",
            " 10% 237/2370 [18:00<1:14:00,  2.08s/it]\n",
            "100% 9/9 [00:18<00:00,  2.12s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 05:42:57,270 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-237\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 05:42:57,281 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-237/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 05:43:06,494 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-237/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 05:43:06,495 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-237/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 05:43:43,018 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-79] due to args.save_total_limit\n",
            "{'loss': 1.085, 'learning_rate': 1.7974683544303798e-05, 'epoch': 3.04}\n",
            "{'loss': 0.9826, 'learning_rate': 1.789029535864979e-05, 'epoch': 3.16}\n",
            "{'loss': 0.9376, 'learning_rate': 1.780590717299578e-05, 'epoch': 3.29}\n",
            "{'loss': 0.9176, 'learning_rate': 1.7721518987341772e-05, 'epoch': 3.42}\n",
            "{'loss': 0.9673, 'learning_rate': 1.7637130801687767e-05, 'epoch': 3.54}\n",
            "{'loss': 0.9127, 'learning_rate': 1.7552742616033758e-05, 'epoch': 3.67}\n",
            "{'loss': 0.8985, 'learning_rate': 1.746835443037975e-05, 'epoch': 3.8}\n",
            "{'loss': 0.9051, 'learning_rate': 1.738396624472574e-05, 'epoch': 3.92}\n",
            " 13% 316/2370 [23:34<1:13:33,  2.15s/it][INFO|trainer.py:2443] 2022-04-27 05:48:31,038 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 05:48:31,039 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 05:48:31,039 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 22% 2/9 [00:00<00:00, 14.15it/s]\u001b[A\n",
            " 44% 4/9 [00:00<00:00, 10.06it/s]\u001b[A\n",
            " 67% 6/9 [00:15<00:10,  3.50s/it]\u001b[A\n",
            " 78% 7/9 [00:15<00:05,  2.67s/it]\u001b[A\n",
            " 89% 8/9 [00:15<00:02,  2.00s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.8990786075592041, 'eval_accuracy': 0.8928571428571429, 'eval_runtime': 34.7246, 'eval_samples_per_second': 4.032, 'eval_steps_per_second': 0.259, 'epoch': 4.0}\n",
            " 13% 316/2370 [24:09<1:13:33,  2.15s/it]\n",
            "100% 9/9 [00:18<00:00,  2.09s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 05:49:05,765 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-316\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 05:49:05,767 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-316/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 05:49:14,620 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-316/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 05:49:14,621 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-316/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 05:49:41,360 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-158] due to args.save_total_limit\n",
            "{'loss': 0.8045, 'learning_rate': 1.729957805907173e-05, 'epoch': 4.05}\n",
            "{'loss': 0.8322, 'learning_rate': 1.7215189873417723e-05, 'epoch': 4.18}\n",
            "{'loss': 0.7591, 'learning_rate': 1.7130801687763714e-05, 'epoch': 4.3}\n",
            "{'loss': 0.7283, 'learning_rate': 1.7046413502109705e-05, 'epoch': 4.43}\n",
            "{'loss': 0.7153, 'learning_rate': 1.6962025316455696e-05, 'epoch': 4.56}\n",
            "{'loss': 0.7296, 'learning_rate': 1.687763713080169e-05, 'epoch': 4.68}\n",
            "{'loss': 0.748, 'learning_rate': 1.679324894514768e-05, 'epoch': 4.81}\n",
            "{'loss': 0.7051, 'learning_rate': 1.6708860759493674e-05, 'epoch': 4.94}\n",
            " 17% 395/2370 [29:00<1:01:08,  1.86s/it][INFO|trainer.py:2443] 2022-04-27 05:53:56,715 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 05:53:56,715 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 05:53:56,715 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 22% 2/9 [00:00<00:00, 14.41it/s]\u001b[A\n",
            " 44% 4/9 [00:00<00:00, 11.10it/s]\u001b[A\n",
            " 67% 6/9 [00:13<00:08,  2.98s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.7588834762573242, 'eval_accuracy': 0.9, 'eval_runtime': 34.3609, 'eval_samples_per_second': 4.074, 'eval_steps_per_second': 0.262, 'epoch': 5.0}\n",
            " 17% 395/2370 [29:34<1:01:08,  1.86s/it]\n",
            "100% 9/9 [00:15<00:00,  1.84s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 05:54:31,077 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-395\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 05:54:31,079 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-395/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 05:54:40,814 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-395/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 05:54:40,815 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-395/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 05:55:01,707 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-237] due to args.save_total_limit\n",
            "{'loss': 0.6441, 'learning_rate': 1.662447257383966e-05, 'epoch': 5.06}\n",
            "{'loss': 0.6398, 'learning_rate': 1.6540084388185656e-05, 'epoch': 5.19}\n",
            "{'loss': 0.6739, 'learning_rate': 1.6455696202531647e-05, 'epoch': 5.32}\n",
            "{'loss': 0.5959, 'learning_rate': 1.637130801687764e-05, 'epoch': 5.44}\n",
            "{'loss': 0.5706, 'learning_rate': 1.628691983122363e-05, 'epoch': 5.57}\n",
            "{'loss': 0.5772, 'learning_rate': 1.620253164556962e-05, 'epoch': 5.7}\n",
            "{'loss': 0.5495, 'learning_rate': 1.6118143459915612e-05, 'epoch': 5.82}\n",
            "{'loss': 0.5607, 'learning_rate': 1.6033755274261603e-05, 'epoch': 5.95}\n",
            " 20% 474/2370 [34:11<59:40,  1.89s/it]  [INFO|trainer.py:2443] 2022-04-27 05:59:07,632 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 05:59:07,633 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 05:59:07,633 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 15.89it/s]\u001b[A\n",
            " 56% 5/9 [00:12<00:12,  3.15s/it]\u001b[A\n",
            " 67% 6/9 [00:13<00:07,  2.36s/it]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6862496137619019, 'eval_accuracy': 0.8928571428571429, 'eval_runtime': 29.4736, 'eval_samples_per_second': 4.75, 'eval_steps_per_second': 0.305, 'epoch': 6.0}\n",
            " 20% 474/2370 [34:40<59:40,  1.89s/it]\n",
            "100% 9/9 [00:15<00:00,  1.40s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 05:59:37,107 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-474\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 05:59:37,110 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-474/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 05:59:45,520 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-474/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 05:59:45,521 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-474/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 06:00:06,686 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-316] due to args.save_total_limit\n",
            "{'loss': 0.6166, 'learning_rate': 1.5949367088607598e-05, 'epoch': 6.08}\n",
            "{'loss': 0.4443, 'learning_rate': 1.5864978902953586e-05, 'epoch': 6.2}\n",
            "{'loss': 0.5187, 'learning_rate': 1.578059071729958e-05, 'epoch': 6.33}\n",
            "{'loss': 0.5225, 'learning_rate': 1.5696202531645572e-05, 'epoch': 6.46}\n",
            "{'loss': 0.5086, 'learning_rate': 1.5611814345991563e-05, 'epoch': 6.58}\n",
            "{'loss': 0.4505, 'learning_rate': 1.5527426160337554e-05, 'epoch': 6.71}\n",
            "{'loss': 0.4853, 'learning_rate': 1.5443037974683546e-05, 'epoch': 6.84}\n",
            "{'loss': 0.5128, 'learning_rate': 1.5358649789029537e-05, 'epoch': 6.96}\n",
            " 23% 553/2370 [39:12<1:06:27,  2.19s/it][INFO|trainer.py:2443] 2022-04-27 06:04:09,014 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 06:04:09,015 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 06:04:09,016 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 15.82it/s]\u001b[A\n",
            " 56% 5/9 [00:12<00:12,  3.15s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:03,  1.89s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.6162286996841431, 'eval_accuracy': 0.9214285714285714, 'eval_runtime': 32.5289, 'eval_samples_per_second': 4.304, 'eval_steps_per_second': 0.277, 'epoch': 7.0}\n",
            " 23% 553/2370 [39:45<1:06:27,  2.19s/it]\n",
            "100% 9/9 [00:15<00:00,  1.58s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 06:04:41,544 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-553\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 06:04:41,547 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-553/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 06:04:51,566 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-553/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 06:04:51,568 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-553/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 06:05:20,302 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-395] due to args.save_total_limit\n",
            "{'loss': 0.4528, 'learning_rate': 1.5274261603375528e-05, 'epoch': 7.09}\n",
            "{'loss': 0.4064, 'learning_rate': 1.5189873417721521e-05, 'epoch': 7.22}\n",
            "{'loss': 0.4175, 'learning_rate': 1.5105485232067512e-05, 'epoch': 7.34}\n",
            "{'loss': 0.4839, 'learning_rate': 1.5021097046413503e-05, 'epoch': 7.47}\n",
            "{'loss': 0.4568, 'learning_rate': 1.4936708860759495e-05, 'epoch': 7.59}\n",
            "{'loss': 0.4496, 'learning_rate': 1.4852320675105488e-05, 'epoch': 7.72}\n",
            "{'loss': 0.4788, 'learning_rate': 1.4767932489451477e-05, 'epoch': 7.85}\n",
            "{'loss': 0.3851, 'learning_rate': 1.468354430379747e-05, 'epoch': 7.97}\n",
            " 27% 632/2370 [44:24<54:26,  1.88s/it]  [INFO|trainer.py:2443] 2022-04-27 06:09:20,588 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 06:09:20,588 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 06:09:20,588 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.12it/s]\u001b[A\n",
            " 56% 5/9 [00:13<00:13,  3.27s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:03,  1.97s/it]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.5674796104431152, 'eval_accuracy': 0.9142857142857143, 'eval_runtime': 32.3481, 'eval_samples_per_second': 4.328, 'eval_steps_per_second': 0.278, 'epoch': 8.0}\n",
            " 27% 632/2370 [44:56<54:26,  1.88s/it]\n",
            "100% 9/9 [00:17<00:00,  1.92s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 06:09:52,938 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-632\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 06:09:52,940 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-632/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 06:10:01,330 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-632/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 06:10:01,331 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-632/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 06:10:34,702 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-474] due to args.save_total_limit\n",
            "{'loss': 0.3505, 'learning_rate': 1.459915611814346e-05, 'epoch': 8.1}\n",
            "{'loss': 0.3254, 'learning_rate': 1.4514767932489453e-05, 'epoch': 8.23}\n",
            "{'loss': 0.3803, 'learning_rate': 1.4430379746835444e-05, 'epoch': 8.35}\n",
            "{'loss': 0.4504, 'learning_rate': 1.4345991561181437e-05, 'epoch': 8.48}\n",
            "{'loss': 0.436, 'learning_rate': 1.4261603375527426e-05, 'epoch': 8.61}\n",
            "{'loss': 0.4532, 'learning_rate': 1.417721518987342e-05, 'epoch': 8.73}\n",
            "{'loss': 0.3408, 'learning_rate': 1.4092827004219412e-05, 'epoch': 8.86}\n",
            "{'loss': 0.3293, 'learning_rate': 1.4008438818565402e-05, 'epoch': 8.99}\n",
            " 30% 711/2370 [49:43<56:58,  2.06s/it]  [INFO|trainer.py:2443] 2022-04-27 06:14:40,265 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 06:14:40,266 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 06:14:40,266 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 22% 2/9 [00:00<00:00, 18.92it/s]\u001b[A\n",
            " 44% 4/9 [00:00<00:00, 11.32it/s]\u001b[A\n",
            " 44% 4/9 [00:10<00:00, 11.32it/s]\u001b[A\n",
            " 56% 5/9 [00:13<00:15,  3.85s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:04,  2.13s/it]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.5377211570739746, 'eval_accuracy': 0.9142857142857143, 'eval_runtime': 29.8077, 'eval_samples_per_second': 4.697, 'eval_steps_per_second': 0.302, 'epoch': 9.0}\n",
            " 30% 711/2370 [50:13<56:58,  2.06s/it]\n",
            "100% 9/9 [00:15<00:00,  1.71s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 06:15:10,074 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-711\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 06:15:10,079 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-711/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 06:15:20,203 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-711/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 06:15:20,204 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-711/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 06:15:40,495 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-553] due to args.save_total_limit\n",
            "{'loss': 0.35, 'learning_rate': 1.3924050632911395e-05, 'epoch': 9.11}\n",
            "{'loss': 0.3655, 'learning_rate': 1.3839662447257384e-05, 'epoch': 9.24}\n",
            "{'loss': 0.3175, 'learning_rate': 1.3755274261603377e-05, 'epoch': 9.37}\n",
            "{'loss': 0.3392, 'learning_rate': 1.3670886075949368e-05, 'epoch': 9.49}\n",
            "{'loss': 0.3844, 'learning_rate': 1.358649789029536e-05, 'epoch': 9.62}\n",
            "{'loss': 0.3127, 'learning_rate': 1.350210970464135e-05, 'epoch': 9.75}\n",
            "{'loss': 0.3718, 'learning_rate': 1.3417721518987344e-05, 'epoch': 9.87}\n",
            "{'loss': 0.3773, 'learning_rate': 1.3333333333333333e-05, 'epoch': 10.0}\n",
            " 33% 790/2370 [54:48<48:09,  1.83s/it][INFO|trainer.py:2443] 2022-04-27 06:19:44,585 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 06:19:44,585 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 06:19:44,586 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 22% 2/9 [00:00<00:00, 19.14it/s]\u001b[A\n",
            " 44% 4/9 [00:00<00:00, 12.14it/s]\u001b[A\n",
            " 67% 6/9 [00:12<00:08,  2.92s/it]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.5134822130203247, 'eval_accuracy': 0.8857142857142857, 'eval_runtime': 29.093, 'eval_samples_per_second': 4.812, 'eval_steps_per_second': 0.309, 'epoch': 10.0}\n",
            " 33% 790/2370 [55:17<48:09,  1.83s/it]\n",
            "100% 9/9 [00:15<00:00,  1.80s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 06:20:13,680 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-790\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 06:20:13,681 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-790/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 06:20:22,037 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-790/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 06:20:22,038 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-790/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 06:20:46,463 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-632] due to args.save_total_limit\n",
            "{'loss': 0.3066, 'learning_rate': 1.3248945147679326e-05, 'epoch': 10.13}\n",
            "{'loss': 0.2675, 'learning_rate': 1.3164556962025317e-05, 'epoch': 10.25}\n",
            "{'loss': 0.2395, 'learning_rate': 1.3080168776371309e-05, 'epoch': 10.38}\n",
            "{'loss': 0.2607, 'learning_rate': 1.29957805907173e-05, 'epoch': 10.51}\n",
            "{'loss': 0.2959, 'learning_rate': 1.2911392405063293e-05, 'epoch': 10.63}\n",
            "{'loss': 0.2981, 'learning_rate': 1.2827004219409284e-05, 'epoch': 10.76}\n",
            "{'loss': 0.3054, 'learning_rate': 1.2742616033755275e-05, 'epoch': 10.89}\n",
            " 37% 869/2370 [59:52<49:40,  1.99s/it]  [INFO|trainer.py:2443] 2022-04-27 06:24:49,319 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 06:24:49,320 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 06:24:49,320 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.10it/s]\u001b[A\n",
            " 44% 4/9 [00:11<00:00, 16.10it/s]\u001b[A\n",
            " 56% 5/9 [00:13<00:12,  3.18s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:03,  1.91s/it]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.48332464694976807, 'eval_accuracy': 0.8928571428571429, 'eval_runtime': 29.5501, 'eval_samples_per_second': 4.738, 'eval_steps_per_second': 0.305, 'epoch': 11.0}\n",
            " 37% 869/2370 [1:00:22<49:40,  1.99s/it]\n",
            "100% 9/9 [00:15<00:00,  1.60s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 06:25:18,871 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-869\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 06:25:18,877 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-869/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 06:25:28,776 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-869/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 06:25:28,777 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-869/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 06:25:57,161 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-711] due to args.save_total_limit\n",
            "{'loss': 0.28, 'learning_rate': 1.2658227848101268e-05, 'epoch': 11.01}\n",
            "{'loss': 0.2931, 'learning_rate': 1.2573839662447258e-05, 'epoch': 11.14}\n",
            "{'loss': 0.2545, 'learning_rate': 1.248945147679325e-05, 'epoch': 11.27}\n",
            "{'loss': 0.2909, 'learning_rate': 1.240506329113924e-05, 'epoch': 11.39}\n",
            "{'loss': 0.2317, 'learning_rate': 1.2320675105485233e-05, 'epoch': 11.52}\n",
            "{'loss': 0.3449, 'learning_rate': 1.2236286919831224e-05, 'epoch': 11.65}\n",
            "{'loss': 0.2471, 'learning_rate': 1.2151898734177216e-05, 'epoch': 11.77}\n",
            "{'loss': 0.2575, 'learning_rate': 1.2067510548523207e-05, 'epoch': 11.9}\n",
            " 40% 948/2370 [1:04:57<41:41,  1.76s/it][INFO|trainer.py:2443] 2022-04-27 06:29:53,714 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 06:29:53,715 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 06:29:53,715 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 22% 2/9 [00:00<00:00, 15.12it/s]\u001b[A\n",
            " 44% 4/9 [00:00<00:00,  7.89it/s]\u001b[A\n",
            " 56% 5/9 [00:14<00:16,  4.16s/it]\u001b[A\n",
            " 78% 7/9 [00:14<00:04,  2.30s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 0.44067081809043884, 'eval_accuracy': 0.9071428571428571, 'eval_runtime': 31.8371, 'eval_samples_per_second': 4.397, 'eval_steps_per_second': 0.283, 'epoch': 12.0}\n",
            " 40% 948/2370 [1:05:29<41:41,  1.76s/it]\n",
            "100% 9/9 [00:16<00:00,  1.81s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 06:30:25,553 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-948\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 06:30:25,557 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-948/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 06:30:33,978 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-948/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 06:30:33,979 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-948/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 06:30:57,462 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-790] due to args.save_total_limit\n",
            "{'loss': 0.292, 'learning_rate': 1.19831223628692e-05, 'epoch': 12.03}\n",
            "{'loss': 0.2588, 'learning_rate': 1.189873417721519e-05, 'epoch': 12.15}\n",
            "{'loss': 0.1917, 'learning_rate': 1.1814345991561182e-05, 'epoch': 12.28}\n",
            "{'loss': 0.2219, 'learning_rate': 1.1729957805907175e-05, 'epoch': 12.41}\n",
            "{'loss': 0.2278, 'learning_rate': 1.1645569620253165e-05, 'epoch': 12.53}\n",
            "{'loss': 0.2781, 'learning_rate': 1.1561181434599158e-05, 'epoch': 12.66}\n",
            "{'loss': 0.2079, 'learning_rate': 1.1476793248945149e-05, 'epoch': 12.78}\n",
            "{'loss': 0.1904, 'learning_rate': 1.139240506329114e-05, 'epoch': 12.91}\n",
            " 43% 1027/2370 [1:10:07<44:53,  2.01s/it]  [INFO|trainer.py:2443] 2022-04-27 06:35:03,572 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 06:35:03,572 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 06:35:03,572 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.18it/s]\u001b[A\n",
            " 56% 5/9 [00:13<00:12,  3.24s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:03,  1.95s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.46523934602737427, 'eval_accuracy': 0.9142857142857143, 'eval_runtime': 30.0638, 'eval_samples_per_second': 4.657, 'eval_steps_per_second': 0.299, 'epoch': 13.0}\n",
            " 43% 1027/2370 [1:10:37<44:53,  2.01s/it]\n",
            "100% 9/9 [00:15<00:00,  1.63s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 06:35:33,637 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1027\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 06:35:33,639 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1027/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 06:35:43,675 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1027/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 06:35:43,676 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1027/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 06:36:12,287 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-869] due to args.save_total_limit\n",
            "{'loss': 0.2387, 'learning_rate': 1.1308016877637132e-05, 'epoch': 13.04}\n",
            "{'loss': 0.1827, 'learning_rate': 1.1223628691983124e-05, 'epoch': 13.16}\n",
            "{'loss': 0.2647, 'learning_rate': 1.1139240506329114e-05, 'epoch': 13.29}\n",
            "{'loss': 0.2144, 'learning_rate': 1.1054852320675107e-05, 'epoch': 13.42}\n",
            "{'loss': 0.2452, 'learning_rate': 1.0970464135021096e-05, 'epoch': 13.54}\n",
            "{'loss': 0.2388, 'learning_rate': 1.088607594936709e-05, 'epoch': 13.67}\n",
            "{'loss': 0.2071, 'learning_rate': 1.080168776371308e-05, 'epoch': 13.8}\n",
            "{'loss': 0.2247, 'learning_rate': 1.0717299578059072e-05, 'epoch': 13.92}\n",
            " 47% 1106/2370 [1:15:13<37:40,  1.79s/it][INFO|trainer.py:2443] 2022-04-27 06:40:10,302 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 06:40:10,302 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 06:40:10,302 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.83it/s]\u001b[A\n",
            " 44% 4/9 [00:10<00:00, 16.83it/s]\u001b[A\n",
            " 56% 5/9 [00:13<00:13,  3.26s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:03,  1.96s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.522860586643219, 'eval_accuracy': 0.8642857142857143, 'eval_runtime': 30.0028, 'eval_samples_per_second': 4.666, 'eval_steps_per_second': 0.3, 'epoch': 14.0}\n",
            " 47% 1106/2370 [1:15:44<37:40,  1.79s/it]\n",
            "100% 9/9 [00:15<00:00,  1.64s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 06:40:40,306 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1106\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 06:40:40,308 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1106/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 06:40:51,130 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1106/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 06:40:51,131 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1106/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 06:41:07,948 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-1027] due to args.save_total_limit\n",
            "{'loss': 0.1861, 'learning_rate': 1.0632911392405063e-05, 'epoch': 14.05}\n",
            "{'loss': 0.2058, 'learning_rate': 1.0548523206751056e-05, 'epoch': 14.18}\n",
            "{'loss': 0.244, 'learning_rate': 1.0464135021097049e-05, 'epoch': 14.3}\n",
            "{'loss': 0.2247, 'learning_rate': 1.0379746835443039e-05, 'epoch': 14.43}\n",
            "{'loss': 0.2785, 'learning_rate': 1.0295358649789031e-05, 'epoch': 14.56}\n",
            "{'loss': 0.1921, 'learning_rate': 1.0210970464135021e-05, 'epoch': 14.68}\n",
            "{'loss': 0.1862, 'learning_rate': 1.0126582278481014e-05, 'epoch': 14.81}\n",
            "{'loss': 0.2074, 'learning_rate': 1.0042194092827005e-05, 'epoch': 14.94}\n",
            " 50% 1185/2370 [1:20:16<40:45,  2.06s/it][INFO|trainer.py:2443] 2022-04-27 06:45:12,889 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 06:45:12,889 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 06:45:12,889 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.02it/s]\u001b[A\n",
            " 56% 5/9 [00:12<00:12,  3.11s/it]\u001b[A\n",
            " 78% 7/9 [00:12<00:03,  1.87s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4123903512954712, 'eval_accuracy': 0.9142857142857143, 'eval_runtime': 29.0029, 'eval_samples_per_second': 4.827, 'eval_steps_per_second': 0.31, 'epoch': 15.0}\n",
            " 50% 1185/2370 [1:20:45<40:45,  2.06s/it]\n",
            "100% 9/9 [00:15<00:00,  1.55s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 06:45:41,893 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1185\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 06:45:41,899 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1185/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 06:45:52,020 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1185/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 06:45:52,021 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1185/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 06:46:11,985 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-948] due to args.save_total_limit\n",
            "{'loss': 0.1816, 'learning_rate': 9.957805907172996e-06, 'epoch': 15.06}\n",
            "{'loss': 0.1982, 'learning_rate': 9.87341772151899e-06, 'epoch': 15.19}\n",
            "{'loss': 0.2083, 'learning_rate': 9.78902953586498e-06, 'epoch': 15.32}\n",
            "{'loss': 0.191, 'learning_rate': 9.704641350210972e-06, 'epoch': 15.44}\n",
            "{'loss': 0.1622, 'learning_rate': 9.620253164556963e-06, 'epoch': 15.57}\n",
            "{'loss': 0.2065, 'learning_rate': 9.535864978902954e-06, 'epoch': 15.7}\n",
            "{'loss': 0.1744, 'learning_rate': 9.451476793248946e-06, 'epoch': 15.82}\n",
            "{'loss': 0.2048, 'learning_rate': 9.367088607594937e-06, 'epoch': 15.95}\n",
            " 53% 1264/2370 [1:25:16<32:44,  1.78s/it][INFO|trainer.py:2443] 2022-04-27 06:50:13,232 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 06:50:13,233 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 06:50:13,233 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.06it/s]\u001b[A\n",
            " 56% 5/9 [00:13<00:12,  3.23s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:03,  1.94s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.46027591824531555, 'eval_accuracy': 0.8857142857142857, 'eval_runtime': 31.6141, 'eval_samples_per_second': 4.428, 'eval_steps_per_second': 0.285, 'epoch': 16.0}\n",
            " 53% 1264/2370 [1:25:48<32:44,  1.78s/it]\n",
            "100% 9/9 [00:15<00:00,  1.62s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 06:50:44,848 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1264\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 06:50:44,849 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1264/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 06:50:53,569 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1264/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 06:50:53,570 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1264/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 06:51:20,869 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-1106] due to args.save_total_limit\n",
            "{'loss': 0.1476, 'learning_rate': 9.28270042194093e-06, 'epoch': 16.08}\n",
            "{'loss': 0.1708, 'learning_rate': 9.198312236286921e-06, 'epoch': 16.2}\n",
            "{'loss': 0.1415, 'learning_rate': 9.113924050632912e-06, 'epoch': 16.33}\n",
            "{'loss': 0.1739, 'learning_rate': 9.029535864978903e-06, 'epoch': 16.46}\n",
            "{'loss': 0.1998, 'learning_rate': 8.945147679324895e-06, 'epoch': 16.58}\n",
            "{'loss': 0.164, 'learning_rate': 8.860759493670886e-06, 'epoch': 16.71}\n",
            "{'loss': 0.2001, 'learning_rate': 8.776371308016879e-06, 'epoch': 16.84}\n",
            "{'loss': 0.1757, 'learning_rate': 8.69198312236287e-06, 'epoch': 16.96}\n",
            " 57% 1343/2370 [1:30:32<33:57,  1.98s/it][INFO|trainer.py:2443] 2022-04-27 06:55:29,205 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 06:55:29,205 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 06:55:29,205 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.25it/s]\u001b[A\n",
            " 44% 4/9 [00:11<00:00, 16.25it/s]\u001b[A\n",
            " 56% 5/9 [00:13<00:12,  3.23s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:03,  1.95s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4193400740623474, 'eval_accuracy': 0.9071428571428571, 'eval_runtime': 30.0723, 'eval_samples_per_second': 4.655, 'eval_steps_per_second': 0.299, 'epoch': 17.0}\n",
            " 57% 1343/2370 [1:31:02<33:57,  1.98s/it]\n",
            "100% 9/9 [00:15<00:00,  1.63s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 06:55:59,278 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1343\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 06:55:59,280 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1343/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 06:56:09,285 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1343/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 06:56:09,286 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1343/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 06:56:29,836 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-1264] due to args.save_total_limit\n",
            "{'loss': 0.1664, 'learning_rate': 8.607594936708861e-06, 'epoch': 17.09}\n",
            "{'loss': 0.1705, 'learning_rate': 8.523206751054853e-06, 'epoch': 17.22}\n",
            "{'loss': 0.1129, 'learning_rate': 8.438818565400846e-06, 'epoch': 17.34}\n",
            "{'loss': 0.2048, 'learning_rate': 8.354430379746837e-06, 'epoch': 17.47}\n",
            "{'loss': 0.1318, 'learning_rate': 8.270042194092828e-06, 'epoch': 17.59}\n",
            "{'loss': 0.1891, 'learning_rate': 8.18565400843882e-06, 'epoch': 17.72}\n",
            "{'loss': 0.1352, 'learning_rate': 8.10126582278481e-06, 'epoch': 17.85}\n",
            "{'loss': 0.1514, 'learning_rate': 8.016877637130802e-06, 'epoch': 17.97}\n",
            " 60% 1422/2370 [1:35:37<30:15,  1.92s/it][INFO|trainer.py:2443] 2022-04-27 07:00:34,064 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 07:00:34,064 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 07:00:34,064 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 15.97it/s]\u001b[A\n",
            " 56% 5/9 [00:13<00:13,  3.31s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:03,  1.99s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.44112586975097656, 'eval_accuracy': 0.8857142857142857, 'eval_runtime': 30.4646, 'eval_samples_per_second': 4.595, 'eval_steps_per_second': 0.295, 'epoch': 18.0}\n",
            " 60% 1422/2370 [1:36:08<30:15,  1.92s/it]\n",
            "100% 9/9 [00:16<00:00,  1.66s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 07:01:04,530 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1422\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 07:01:04,532 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1422/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 07:01:28,553 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1422/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 07:01:28,554 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1422/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 07:01:52,827 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-1343] due to args.save_total_limit\n",
            "{'loss': 0.158, 'learning_rate': 7.932489451476793e-06, 'epoch': 18.1}\n",
            "{'loss': 0.1846, 'learning_rate': 7.848101265822786e-06, 'epoch': 18.23}\n",
            "{'loss': 0.1369, 'learning_rate': 7.763713080168777e-06, 'epoch': 18.35}\n",
            "{'loss': 0.1621, 'learning_rate': 7.679324894514768e-06, 'epoch': 18.48}\n",
            "{'loss': 0.1534, 'learning_rate': 7.5949367088607605e-06, 'epoch': 18.61}\n",
            "{'loss': 0.1256, 'learning_rate': 7.510548523206752e-06, 'epoch': 18.73}\n",
            "{'loss': 0.142, 'learning_rate': 7.426160337552744e-06, 'epoch': 18.86}\n",
            "{'loss': 0.1606, 'learning_rate': 7.341772151898735e-06, 'epoch': 18.99}\n",
            " 63% 1501/2370 [1:41:02<27:42,  1.91s/it][INFO|trainer.py:2443] 2022-04-27 07:05:59,215 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 07:05:59,216 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 07:05:59,216 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.38it/s]\u001b[A\n",
            " 44% 4/9 [00:11<00:00, 16.38it/s]\u001b[A\n",
            " 56% 5/9 [00:13<00:12,  3.19s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:03,  1.92s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.39023274183273315, 'eval_accuracy': 0.9142857142857143, 'eval_runtime': 29.7632, 'eval_samples_per_second': 4.704, 'eval_steps_per_second': 0.302, 'epoch': 19.0}\n",
            " 63% 1501/2370 [1:41:32<27:42,  1.91s/it]\n",
            "100% 9/9 [00:15<00:00,  1.60s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 07:06:28,980 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1501\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 07:06:28,982 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1501/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 07:06:39,727 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1501/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 07:06:39,728 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1501/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 07:07:00,108 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-1185] due to args.save_total_limit\n",
            "{'loss': 0.15, 'learning_rate': 7.257383966244726e-06, 'epoch': 19.11}\n",
            "{'loss': 0.1337, 'learning_rate': 7.172995780590718e-06, 'epoch': 19.24}\n",
            "{'loss': 0.1716, 'learning_rate': 7.08860759493671e-06, 'epoch': 19.37}\n",
            "{'loss': 0.1322, 'learning_rate': 7.004219409282701e-06, 'epoch': 19.49}\n",
            "{'loss': 0.1684, 'learning_rate': 6.919831223628692e-06, 'epoch': 19.62}\n",
            "{'loss': 0.1702, 'learning_rate': 6.835443037974684e-06, 'epoch': 19.75}\n",
            "{'loss': 0.1562, 'learning_rate': 6.751054852320675e-06, 'epoch': 19.87}\n",
            "{'loss': 0.1247, 'learning_rate': 6.666666666666667e-06, 'epoch': 20.0}\n",
            " 67% 1580/2370 [1:46:07<24:48,  1.88s/it][INFO|trainer.py:2443] 2022-04-27 07:11:03,500 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 07:11:03,500 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 07:11:03,500 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.30it/s]\u001b[A\n",
            " 56% 5/9 [00:13<00:12,  3.21s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:03,  1.93s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.4001559019088745, 'eval_accuracy': 0.9, 'eval_runtime': 30.1905, 'eval_samples_per_second': 4.637, 'eval_steps_per_second': 0.298, 'epoch': 20.0}\n",
            " 67% 1580/2370 [1:46:37<24:48,  1.88s/it]\n",
            "100% 9/9 [00:15<00:00,  1.62s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 07:11:33,692 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1580\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 07:11:33,693 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1580/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 07:11:44,235 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1580/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 07:11:44,236 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1580/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 07:12:06,499 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-1422] due to args.save_total_limit\n",
            "{'loss': 0.1124, 'learning_rate': 6.582278481012659e-06, 'epoch': 20.13}\n",
            "{'loss': 0.1687, 'learning_rate': 6.49789029535865e-06, 'epoch': 20.25}\n",
            "{'loss': 0.184, 'learning_rate': 6.413502109704642e-06, 'epoch': 20.38}\n",
            "{'loss': 0.2009, 'learning_rate': 6.329113924050634e-06, 'epoch': 20.51}\n",
            "{'loss': 0.1064, 'learning_rate': 6.244725738396625e-06, 'epoch': 20.63}\n",
            "{'loss': 0.0982, 'learning_rate': 6.160337552742617e-06, 'epoch': 20.76}\n",
            "{'loss': 0.1561, 'learning_rate': 6.075949367088608e-06, 'epoch': 20.89}\n",
            " 70% 1659/2370 [1:51:21<22:02,  1.86s/it][INFO|trainer.py:2443] 2022-04-27 07:16:17,539 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 07:16:17,539 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 07:16:17,539 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 15.72it/s]\u001b[A\n",
            " 44% 4/9 [00:12<00:00, 15.72it/s]\u001b[A\n",
            " 56% 5/9 [00:13<00:13,  3.32s/it]\u001b[A\n",
            " 67% 6/9 [00:13<00:07,  2.49s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.40203309059143066, 'eval_accuracy': 0.9142857142857143, 'eval_runtime': 31.3249, 'eval_samples_per_second': 4.469, 'eval_steps_per_second': 0.287, 'epoch': 21.0}\n",
            " 70% 1659/2370 [1:51:52<22:02,  1.86s/it]\n",
            "100% 9/9 [00:16<00:00,  1.48s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 07:16:48,865 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1659\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 07:16:48,867 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1659/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 07:16:58,965 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1659/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 07:16:58,966 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1659/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 07:17:27,515 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-1580] due to args.save_total_limit\n",
            "{'loss': 0.1146, 'learning_rate': 5.9915611814346e-06, 'epoch': 21.01}\n",
            "{'loss': 0.1795, 'learning_rate': 5.907172995780591e-06, 'epoch': 21.14}\n",
            "{'loss': 0.135, 'learning_rate': 5.8227848101265824e-06, 'epoch': 21.27}\n",
            "{'loss': 0.1506, 'learning_rate': 5.7383966244725745e-06, 'epoch': 21.39}\n",
            "{'loss': 0.1112, 'learning_rate': 5.654008438818566e-06, 'epoch': 21.52}\n",
            "{'loss': 0.1159, 'learning_rate': 5.569620253164557e-06, 'epoch': 21.65}\n",
            "{'loss': 0.1173, 'learning_rate': 5.485232067510548e-06, 'epoch': 21.77}\n",
            "{'loss': 0.1, 'learning_rate': 5.40084388185654e-06, 'epoch': 21.9}\n",
            " 73% 1738/2370 [1:56:36<19:05,  1.81s/it][INFO|trainer.py:2443] 2022-04-27 07:21:33,263 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 07:21:33,263 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 07:21:33,263 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.40it/s]\u001b[A\n",
            " 56% 5/9 [00:12<00:12,  3.14s/it]\u001b[A\n",
            " 67% 6/9 [00:13<00:07,  2.36s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.40922313928604126, 'eval_accuracy': 0.8857142857142857, 'eval_runtime': 29.4543, 'eval_samples_per_second': 4.753, 'eval_steps_per_second': 0.306, 'epoch': 22.0}\n",
            " 73% 1738/2370 [1:57:06<19:05,  1.81s/it]\n",
            "100% 9/9 [00:15<00:00,  1.40s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 07:22:02,718 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1738\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 07:22:02,720 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1738/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 07:22:11,473 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1738/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 07:22:11,474 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1738/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 07:22:30,139 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-1659] due to args.save_total_limit\n",
            "{'loss': 0.1633, 'learning_rate': 5.3164556962025316e-06, 'epoch': 22.03}\n",
            "{'loss': 0.1508, 'learning_rate': 5.2320675105485245e-06, 'epoch': 22.15}\n",
            "{'loss': 0.1447, 'learning_rate': 5.147679324894516e-06, 'epoch': 22.28}\n",
            "{'loss': 0.132, 'learning_rate': 5.063291139240507e-06, 'epoch': 22.41}\n",
            "{'loss': 0.156, 'learning_rate': 4.978902953586498e-06, 'epoch': 22.53}\n",
            "{'loss': 0.1109, 'learning_rate': 4.89451476793249e-06, 'epoch': 22.66}\n",
            "{'loss': 0.1243, 'learning_rate': 4.8101265822784815e-06, 'epoch': 22.78}\n",
            "{'loss': 0.1308, 'learning_rate': 4.725738396624473e-06, 'epoch': 22.91}\n",
            " 77% 1817/2370 [2:01:44<16:41,  1.81s/it][INFO|trainer.py:2443] 2022-04-27 07:26:40,586 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 07:26:40,586 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 07:26:40,586 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.23it/s]\u001b[A\n",
            " 56% 5/9 [00:13<00:12,  3.19s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:03,  1.92s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.3903563320636749, 'eval_accuracy': 0.9142857142857143, 'eval_runtime': 31.1608, 'eval_samples_per_second': 4.493, 'eval_steps_per_second': 0.289, 'epoch': 23.0}\n",
            " 77% 1817/2370 [2:02:15<16:41,  1.81s/it]\n",
            "100% 9/9 [00:15<00:00,  1.60s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 07:27:11,748 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1817\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 07:27:11,750 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1817/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 07:27:21,884 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1817/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 07:27:21,885 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1817/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 07:27:40,813 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-1738] due to args.save_total_limit\n",
            "{'loss': 0.1229, 'learning_rate': 4.641350210970465e-06, 'epoch': 23.04}\n",
            "{'loss': 0.1611, 'learning_rate': 4.556962025316456e-06, 'epoch': 23.16}\n",
            "{'loss': 0.123, 'learning_rate': 4.472573839662447e-06, 'epoch': 23.29}\n",
            "{'loss': 0.1157, 'learning_rate': 4.3881856540084394e-06, 'epoch': 23.42}\n",
            "{'loss': 0.0987, 'learning_rate': 4.303797468354431e-06, 'epoch': 23.54}\n",
            "{'loss': 0.1371, 'learning_rate': 4.219409282700423e-06, 'epoch': 23.67}\n",
            "{'loss': 0.1326, 'learning_rate': 4.135021097046414e-06, 'epoch': 23.8}\n",
            "{'loss': 0.1226, 'learning_rate': 4.050632911392405e-06, 'epoch': 23.92}\n",
            " 80% 1896/2370 [2:06:47<14:40,  1.86s/it][INFO|trainer.py:2443] 2022-04-27 07:31:43,566 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 07:31:43,567 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 07:31:43,567 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.34it/s]\u001b[A\n",
            " 56% 5/9 [00:12<00:12,  3.10s/it]\u001b[A\n",
            " 67% 6/9 [00:12<00:06,  2.32s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.3960396349430084, 'eval_accuracy': 0.8928571428571429, 'eval_runtime': 29.1351, 'eval_samples_per_second': 4.805, 'eval_steps_per_second': 0.309, 'epoch': 24.0}\n",
            " 80% 1896/2370 [2:07:16<14:40,  1.86s/it]\n",
            "100% 9/9 [00:15<00:00,  1.38s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 07:32:12,703 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1896\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 07:32:12,704 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1896/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 07:32:37,612 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1896/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 07:32:37,613 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1896/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 07:33:17,528 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-1817] due to args.save_total_limit\n",
            "{'loss': 0.1235, 'learning_rate': 3.9662447257383965e-06, 'epoch': 24.05}\n",
            "{'loss': 0.1368, 'learning_rate': 3.8818565400843886e-06, 'epoch': 24.18}\n",
            "{'loss': 0.1121, 'learning_rate': 3.7974683544303802e-06, 'epoch': 24.3}\n",
            "{'loss': 0.0873, 'learning_rate': 3.713080168776372e-06, 'epoch': 24.43}\n",
            "{'loss': 0.1372, 'learning_rate': 3.628691983122363e-06, 'epoch': 24.56}\n",
            "{'loss': 0.0823, 'learning_rate': 3.544303797468355e-06, 'epoch': 24.68}\n",
            "{'loss': 0.1166, 'learning_rate': 3.459915611814346e-06, 'epoch': 24.81}\n",
            "{'loss': 0.1147, 'learning_rate': 3.3755274261603377e-06, 'epoch': 24.94}\n",
            " 83% 1975/2370 [2:12:27<12:08,  1.84s/it][INFO|trainer.py:2443] 2022-04-27 07:37:24,156 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 07:37:24,156 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 07:37:24,157 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.46it/s]\u001b[A\n",
            " 56% 5/9 [00:13<00:12,  3.25s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:03,  1.95s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.3854289948940277, 'eval_accuracy': 0.9071428571428571, 'eval_runtime': 30.7343, 'eval_samples_per_second': 4.555, 'eval_steps_per_second': 0.293, 'epoch': 25.0}\n",
            " 83% 1975/2370 [2:12:58<12:08,  1.84s/it]\n",
            "100% 9/9 [00:15<00:00,  1.63s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 07:37:54,892 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1975\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 07:37:54,894 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1975/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 07:38:04,444 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1975/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 07:38:04,445 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-1975/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 07:38:30,078 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-1501] due to args.save_total_limit\n",
            "{'loss': 0.119, 'learning_rate': 3.2911392405063294e-06, 'epoch': 25.06}\n",
            "{'loss': 0.1738, 'learning_rate': 3.206751054852321e-06, 'epoch': 25.19}\n",
            "{'loss': 0.1351, 'learning_rate': 3.1223628691983127e-06, 'epoch': 25.32}\n",
            "{'loss': 0.0987, 'learning_rate': 3.037974683544304e-06, 'epoch': 25.44}\n",
            "{'loss': 0.0928, 'learning_rate': 2.9535864978902956e-06, 'epoch': 25.57}\n",
            "{'loss': 0.1208, 'learning_rate': 2.8691983122362873e-06, 'epoch': 25.7}\n",
            "{'loss': 0.1231, 'learning_rate': 2.7848101265822785e-06, 'epoch': 25.82}\n",
            "{'loss': 0.1252, 'learning_rate': 2.70042194092827e-06, 'epoch': 25.95}\n",
            " 87% 2054/2370 [2:17:38<09:40,  1.84s/it][INFO|trainer.py:2443] 2022-04-27 07:42:34,898 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 07:42:34,898 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 07:42:34,898 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.10it/s]\u001b[A\n",
            " 56% 5/9 [00:14<00:13,  3.45s/it]\u001b[A\n",
            " 67% 6/9 [00:14<00:07,  2.62s/it]\u001b[A\n",
            " 78% 7/9 [00:14<00:03,  1.98s/it]\u001b[A\n",
            " 89% 8/9 [00:14<00:01,  1.49s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.3783908784389496, 'eval_accuracy': 0.9, 'eval_runtime': 32.8823, 'eval_samples_per_second': 4.258, 'eval_steps_per_second': 0.274, 'epoch': 26.0}\n",
            " 87% 2054/2370 [2:18:11<09:40,  1.84s/it]\n",
            "100% 9/9 [00:18<00:00,  2.17s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 07:43:07,787 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2054\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 07:43:07,792 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2054/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 07:43:25,093 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2054/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 07:43:25,094 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2054/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 07:43:42,429 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-1896] due to args.save_total_limit\n",
            "{'loss': 0.2293, 'learning_rate': 2.6160337552742622e-06, 'epoch': 26.08}\n",
            "{'loss': 0.1128, 'learning_rate': 2.5316455696202535e-06, 'epoch': 26.2}\n",
            "{'loss': 0.1045, 'learning_rate': 2.447257383966245e-06, 'epoch': 26.33}\n",
            "{'loss': 0.0812, 'learning_rate': 2.3628691983122364e-06, 'epoch': 26.46}\n",
            "{'loss': 0.1469, 'learning_rate': 2.278481012658228e-06, 'epoch': 26.58}\n",
            "{'loss': 0.0853, 'learning_rate': 2.1940928270042197e-06, 'epoch': 26.71}\n",
            "{'loss': 0.1175, 'learning_rate': 2.1097046413502114e-06, 'epoch': 26.84}\n",
            "{'loss': 0.1027, 'learning_rate': 2.0253164556962026e-06, 'epoch': 26.96}\n",
            " 90% 2133/2370 [2:22:53<07:27,  1.89s/it][INFO|trainer.py:2443] 2022-04-27 07:47:50,251 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 07:47:50,252 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 07:47:50,252 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 15.12it/s]\u001b[A\n",
            " 44% 4/9 [00:11<00:00, 15.12it/s]\u001b[A\n",
            " 56% 5/9 [00:12<00:12,  3.15s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:03,  1.89s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.36586570739746094, 'eval_accuracy': 0.9142857142857143, 'eval_runtime': 29.4404, 'eval_samples_per_second': 4.755, 'eval_steps_per_second': 0.306, 'epoch': 27.0}\n",
            " 90% 2133/2370 [2:23:23<07:27,  1.89s/it]\n",
            "100% 9/9 [00:15<00:00,  1.59s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 07:48:19,693 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2133\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 07:48:19,695 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2133/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 07:48:31,709 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2133/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 07:48:31,711 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2133/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 07:48:52,520 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-1975] due to args.save_total_limit\n",
            "{'loss': 0.1034, 'learning_rate': 1.9409282700421943e-06, 'epoch': 27.09}\n",
            "{'loss': 0.1212, 'learning_rate': 1.856540084388186e-06, 'epoch': 27.22}\n",
            "{'loss': 0.1454, 'learning_rate': 1.7721518987341774e-06, 'epoch': 27.34}\n",
            "{'loss': 0.0994, 'learning_rate': 1.6877637130801689e-06, 'epoch': 27.47}\n",
            "{'loss': 0.1317, 'learning_rate': 1.6033755274261605e-06, 'epoch': 27.59}\n",
            "{'loss': 0.0784, 'learning_rate': 1.518987341772152e-06, 'epoch': 27.72}\n",
            "{'loss': 0.1184, 'learning_rate': 1.4345991561181436e-06, 'epoch': 27.85}\n",
            "{'loss': 0.1087, 'learning_rate': 1.350210970464135e-06, 'epoch': 27.97}\n",
            " 93% 2212/2370 [2:27:57<04:49,  1.83s/it][INFO|trainer.py:2443] 2022-04-27 07:52:53,558 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 07:52:53,558 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 07:52:53,558 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.50it/s]\u001b[A\n",
            " 56% 5/9 [00:12<00:12,  3.14s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:03,  1.89s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.37894341349601746, 'eval_accuracy': 0.9071428571428571, 'eval_runtime': 29.289, 'eval_samples_per_second': 4.78, 'eval_steps_per_second': 0.307, 'epoch': 28.0}\n",
            " 93% 2212/2370 [2:28:26<04:49,  1.83s/it]\n",
            "100% 9/9 [00:15<00:00,  1.58s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 07:53:22,848 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2212\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 07:53:22,850 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2212/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 07:53:31,166 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2212/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 07:53:31,167 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2212/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 07:54:05,891 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-2054] due to args.save_total_limit\n",
            "{'loss': 0.0916, 'learning_rate': 1.2658227848101267e-06, 'epoch': 28.1}\n",
            "{'loss': 0.101, 'learning_rate': 1.1814345991561182e-06, 'epoch': 28.23}\n",
            "{'loss': 0.0963, 'learning_rate': 1.0970464135021099e-06, 'epoch': 28.35}\n",
            "{'loss': 0.0945, 'learning_rate': 1.0126582278481013e-06, 'epoch': 28.48}\n",
            "{'loss': 0.0812, 'learning_rate': 9.28270042194093e-07, 'epoch': 28.61}\n",
            "{'loss': 0.1117, 'learning_rate': 8.438818565400844e-07, 'epoch': 28.73}\n",
            "{'loss': 0.0972, 'learning_rate': 7.59493670886076e-07, 'epoch': 28.86}\n",
            "{'loss': 0.1257, 'learning_rate': 6.751054852320675e-07, 'epoch': 28.99}\n",
            " 97% 2291/2370 [2:33:10<02:23,  1.82s/it][INFO|trainer.py:2443] 2022-04-27 07:58:06,967 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 07:58:06,968 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 07:58:06,968 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 33% 3/9 [00:00<00:00, 16.63it/s]\u001b[A\n",
            " 56% 5/9 [00:12<00:12,  3.12s/it]\u001b[A\n",
            " 78% 7/9 [00:13<00:03,  1.88s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.37545546889305115, 'eval_accuracy': 0.9142857142857143, 'eval_runtime': 29.3155, 'eval_samples_per_second': 4.776, 'eval_steps_per_second': 0.307, 'epoch': 29.0}\n",
            " 97% 2291/2370 [2:33:39<02:23,  1.82s/it]\n",
            "100% 9/9 [00:15<00:00,  1.57s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 07:58:36,284 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2291\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 07:58:36,286 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2291/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 07:58:49,253 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2291/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 07:58:49,254 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2291/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 07:59:13,589 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-2212] due to args.save_total_limit\n",
            "{'loss': 0.1054, 'learning_rate': 5.907172995780591e-07, 'epoch': 29.11}\n",
            "{'loss': 0.1524, 'learning_rate': 5.063291139240507e-07, 'epoch': 29.24}\n",
            "{'loss': 0.122, 'learning_rate': 4.219409282700422e-07, 'epoch': 29.37}\n",
            "{'loss': 0.0816, 'learning_rate': 3.3755274261603377e-07, 'epoch': 29.49}\n",
            "{'loss': 0.1152, 'learning_rate': 2.5316455696202533e-07, 'epoch': 29.62}\n",
            "{'loss': 0.1025, 'learning_rate': 1.6877637130801689e-07, 'epoch': 29.75}\n",
            "{'loss': 0.1174, 'learning_rate': 8.438818565400844e-08, 'epoch': 29.87}\n",
            "{'loss': 0.121, 'learning_rate': 0.0, 'epoch': 30.0}\n",
            "100% 2370/2370 [2:38:16<00:00,  1.79s/it][INFO|trainer.py:2443] 2022-04-27 08:03:13,028 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 08:03:13,028 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 08:03:13,028 >>   Batch size = 16\n",
            "\n",
            "  0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            " 22% 2/9 [00:00<00:00, 16.65it/s]\u001b[A\n",
            " 44% 4/9 [00:00<00:00, 11.32it/s]\u001b[A\n",
            " 67% 6/9 [00:13<00:08,  2.97s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.37093833088874817, 'eval_accuracy': 0.9214285714285714, 'eval_runtime': 29.706, 'eval_samples_per_second': 4.713, 'eval_steps_per_second': 0.303, 'epoch': 30.0}\n",
            "100% 2370/2370 [2:38:46<00:00,  1.79s/it]\n",
            "100% 9/9 [00:15<00:00,  1.83s/it]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:2163] 2022-04-27 08:03:42,735 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2370\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 08:03:42,737 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2370/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 08:03:51,270 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2370/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 08:03:51,271 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2370/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-27 08:04:11,588 >> Deleting older checkpoint [crop14-small_vit-large-patch32-224-in21k/checkpoint-2291] due to args.save_total_limit\n",
            "[INFO|trainer.py:1557] 2022-04-27 08:04:11,655 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1566] 2022-04-27 08:04:11,656 >> Loading best model from ./crop14-small_vit-large-patch32-224-in21k/checkpoint-2133 (score: 0.36586570739746094).\n",
            "{'train_runtime': 9575.5481, 'train_samples_per_second': 3.948, 'train_steps_per_second': 0.248, 'train_loss': 0.4199458094588815, 'epoch': 30.0}\n",
            "100% 2370/2370 [2:39:35<00:00,  4.04s/it]\n",
            "[INFO|trainer.py:2163] 2022-04-27 08:04:31,875 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 08:04:31,881 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 08:04:42,272 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 08:04:42,387 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/preprocessor_config.json\n",
            "[INFO|trainer.py:2163] 2022-04-27 08:04:42,387 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 08:04:42,390 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 08:04:50,879 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 08:04:50,881 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/preprocessor_config.json\n",
            "Several commits (2) will be pushed upstream.\n",
            "WARNING:huggingface_hub.repository:Several commits (2) will be pushed upstream.\n",
            "The progress bars may be unreliable.\n",
            "WARNING:huggingface_hub.repository:The progress bars may be unreliable.\n",
            "Upload file pytorch_model.bin:   0% 3.33k/1.14G [00:00<?, ?B/s]\n",
            "Upload file pytorch_model.bin: 100% 1.14G/1.14G [18:29<00:00, 1.07MB/s]To https://huggingface.co/gary109/crop14-small_vit-large-patch32-224-in21k\n",
            "   5042a58..056f3a4  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:To https://huggingface.co/gary109/crop14-small_vit-large-patch32-224-in21k\n",
            "   5042a58..056f3a4  main -> main\n",
            "\n",
            "Upload file pytorch_model.bin: 100% 1.14G/1.14G [18:30<00:00, 1.10MB/s]\n",
            "\n",
            "Upload file runs/Apr27_05-23-51_534d9a9e5829/events.out.tfevents.1651037096.534d9a9e5829.7682.0: 100% 50.1k/50.1k [18:30<00:00, 43.1B/s]\u001b[A\n",
            "Upload file runs/Apr27_05-23-51_534d9a9e5829/events.out.tfevents.1651037096.534d9a9e5829.7682.0: 100% 50.1k/50.1k [18:30<00:00, 43.1B/s]\n",
            "To https://huggingface.co/gary109/crop14-small_vit-large-patch32-224-in21k\n",
            "   056f3a4..eb92fd4  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:To https://huggingface.co/gary109/crop14-small_vit-large-patch32-224-in21k\n",
            "   056f3a4..eb92fd4  main -> main\n",
            "\n",
            "***** train metrics *****\n",
            "  epoch                    =       30.0\n",
            "  train_loss               =     0.4199\n",
            "  train_runtime            = 2:39:35.54\n",
            "  train_samples_per_second =      3.948\n",
            "  train_steps_per_second   =      0.248\n",
            "[INFO|trainer.py:2443] 2022-04-27 08:23:47,820 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-27 08:23:47,821 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-27 08:23:47,821 >>   Batch size = 16\n",
            "100% 9/9 [00:15<00:00,  1.78s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       30.0\n",
            "  eval_accuracy           =     0.9143\n",
            "  eval_loss               =     0.3659\n",
            "  eval_runtime            = 0:00:31.67\n",
            "  eval_samples_per_second =      4.419\n",
            "  eval_steps_per_second   =      0.284\n",
            "[INFO|trainer.py:2163] 2022-04-27 08:24:19,552 >> Saving model checkpoint to ./crop14-small_vit-large-patch32-224-in21k/\n",
            "[INFO|configuration_utils.py:446] 2022-04-27 08:24:19,554 >> Configuration saved in ./crop14-small_vit-large-patch32-224-in21k/config.json\n",
            "[INFO|modeling_utils.py:1454] 2022-04-27 08:24:39,275 >> Model weights saved in ./crop14-small_vit-large-patch32-224-in21k/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-27 08:24:39,276 >> Feature extractor saved in ./crop14-small_vit-large-patch32-224-in21k/preprocessor_config.json\n",
            "Upload file pytorch_model.bin:   0% 3.33k/1.14G [00:00<?, ?B/s]\n",
            "Upload file pytorch_model.bin: 100% 1.14G/1.14G [18:02<00:00, 1.19MB/s]To https://huggingface.co/gary109/crop14-small_vit-large-patch32-224-in21k\n",
            "   eb92fd4..b6d2616  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:To https://huggingface.co/gary109/crop14-small_vit-large-patch32-224-in21k\n",
            "   eb92fd4..b6d2616  main -> main\n",
            "\n",
            "Upload file pytorch_model.bin: 100% 1.14G/1.14G [18:04<00:00, 1.13MB/s]\n",
            "\n",
            "Upload file runs/Apr27_05-23-51_534d9a9e5829/events.out.tfevents.1651047859.534d9a9e5829.7682.2: 100% 363/363 [18:04<?, ?B/s]\u001b[A\n",
            "Upload file runs/Apr27_05-23-51_534d9a9e5829/events.out.tfevents.1651047859.534d9a9e5829.7682.2: 100% 363/363 [18:04<?, ?B/s]\n",
            "To https://huggingface.co/gary109/crop14-small_vit-large-patch32-224-in21k\n",
            "   b6d2616..052d4f1  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:To https://huggingface.co/gary109/crop14-small_vit-large-patch32-224-in21k\n",
            "   b6d2616..052d4f1  main -> main\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### google/vit-huge-patch14-224-in21k\n",
        "---"
      ],
      "metadata": {
        "id": "qW0GTGnQzZnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch run_image_classification.py \\\n",
        "    --model_name_or_path \"google/vit-huge-patch14-224-in21k\" \\\n",
        "    --dataset_name \"gary109/crop14-small\" \\\n",
        "    --output_dir ./crop14-small_vit-huge-patch14-224-in21k/ \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id crop14-small_vit-huge-patch14-224-in21k \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 30 \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 1 \\\n",
        "    --use_auth_token \\\n",
        "    --seed 1337 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --gradient_checkpointing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBJgzxj05VzQ",
        "outputId": "f4212cb6-c488-42c8-97df-633704c9dcea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/training_args.py:978: FutureWarning: `--push_to_hub_model_id` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case gary109/crop14-small_vit-huge-patch14-224-in21k).\n",
            "  FutureWarning,\n",
            "04/28/2022 02:00:36 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/28/2022 02:00:36 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=8,\n",
            "gradient_checkpointing=True,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=gary109/crop14-small_vit-huge-patch14-224-in21k,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./crop14-small_vit-huge-patch14-224-in21k/runs/Apr28_02-00-35_4c65f37d1894,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=./crop14-small_vit-huge-patch14-224-in21k/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=True,\n",
            "push_to_hub_model_id=crop14-small_vit-huge-patch14-224-in21k,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=False,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./crop14-small_vit-huge-patch14-224-in21k/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1337,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "Downloading: 100% 2.25k/2.25k [00:00<00:00, 2.04MB/s]\n",
            "04/28/2022 02:00:40 - WARNING - datasets.builder - Using custom data configuration gary109--crop14-small-d2283ee3b6a9a5fe\n",
            "Downloading and preparing dataset crop14/crop14-small (download: 2.73 GiB, generated: 2.77 GiB, post-processed: Unknown size, total: 5.51 GiB) to /root/.cache/huggingface/datasets/parquet/gary109--crop14-small-d2283ee3b6a9a5fe/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n",
            "Downloading data files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/458M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   0% 18.4k/458M [00:00<1:34:04, 81.1kB/s]\u001b[A\n",
            "Downloading data:   0% 52.2k/458M [00:00<1:03:31, 120kB/s] \u001b[A\n",
            "Downloading data:   0% 139k/458M [00:00<31:59, 239kB/s]   \u001b[A\n",
            "Downloading data:   0% 313k/458M [00:00<17:10, 444kB/s]\u001b[A\n",
            "Downloading data:   0% 644k/458M [00:01<09:28, 804kB/s]\u001b[A\n",
            "Downloading data:   0% 1.34M/458M [00:01<04:52, 1.56MB/s]\u001b[A\n",
            "Downloading data:   0% 2.12M/458M [00:01<03:30, 2.17MB/s]\u001b[A\n",
            "Downloading data:   1% 4.25M/458M [00:01<01:42, 4.43MB/s]\u001b[A\n",
            "Downloading data:   1% 6.63M/458M [00:02<01:11, 6.29MB/s]\u001b[A\n",
            "Downloading data:   2% 9.27M/458M [00:02<00:56, 7.90MB/s]\u001b[A\n",
            "Downloading data:   3% 11.9M/458M [00:02<00:49, 9.02MB/s]\u001b[A\n",
            "Downloading data:   3% 14.6M/458M [00:02<00:45, 9.82MB/s]\u001b[A\n",
            "Downloading data:   4% 17.3M/458M [00:02<00:42, 10.4MB/s]\u001b[A\n",
            "Downloading data:   4% 20.0M/458M [00:03<00:40, 10.8MB/s]\u001b[A\n",
            "Downloading data:   5% 22.7M/458M [00:03<00:39, 11.1MB/s]\u001b[A\n",
            "Downloading data:   6% 25.5M/458M [00:03<00:37, 11.4MB/s]\u001b[A\n",
            "Downloading data:   6% 28.2M/458M [00:03<00:37, 11.6MB/s]\u001b[A\n",
            "Downloading data:   7% 31.0M/458M [00:04<00:36, 11.8MB/s]\u001b[A\n",
            "Downloading data:   7% 33.9M/458M [00:04<00:35, 11.9MB/s]\u001b[A\n",
            "Downloading data:   8% 36.7M/458M [00:04<00:34, 12.1MB/s]\u001b[A\n",
            "Downloading data:   9% 39.6M/458M [00:04<00:34, 12.2MB/s]\u001b[A\n",
            "Downloading data:   9% 42.5M/458M [00:05<00:33, 12.4MB/s]\u001b[A\n",
            "Downloading data:  10% 44.7M/458M [00:05<00:35, 11.6MB/s]\u001b[A\n",
            "Downloading data:  10% 47.6M/458M [00:05<00:34, 11.9MB/s]\u001b[A\n",
            "Downloading data:  11% 50.5M/458M [00:05<00:33, 12.1MB/s]\u001b[A\n",
            "Downloading data:  12% 53.5M/458M [00:05<00:32, 12.3MB/s]\u001b[A\n",
            "Downloading data:  12% 56.4M/458M [00:06<00:32, 12.5MB/s]\u001b[A\n",
            "Downloading data:  13% 59.4M/458M [00:06<00:31, 12.7MB/s]\u001b[A\n",
            "Downloading data:  14% 62.5M/458M [00:06<00:30, 12.8MB/s]\u001b[A\n",
            "Downloading data:  14% 65.5M/458M [00:06<00:30, 12.9MB/s]\u001b[A\n",
            "Downloading data:  15% 68.5M/458M [00:07<00:29, 13.0MB/s]\u001b[A\n",
            "Downloading data:  16% 71.6M/458M [00:07<00:29, 13.1MB/s]\u001b[A\n",
            "Downloading data:  16% 74.6M/458M [00:07<00:29, 13.1MB/s]\u001b[A\n",
            "Downloading data:  17% 77.7M/458M [00:07<00:28, 13.2MB/s]\u001b[A\n",
            "Downloading data:  18% 80.7M/458M [00:08<00:28, 13.2MB/s]\u001b[A\n",
            "Downloading data:  18% 83.8M/458M [00:08<00:28, 13.3MB/s]\u001b[A\n",
            "Downloading data:  19% 86.9M/458M [00:08<00:27, 13.3MB/s]\u001b[A\n",
            "Downloading data:  20% 90.0M/458M [00:08<00:27, 13.4MB/s]\u001b[A\n",
            "Downloading data:  20% 93.2M/458M [00:08<00:27, 13.5MB/s]\u001b[A\n",
            "Downloading data:  21% 96.3M/458M [00:09<00:26, 13.7MB/s]\u001b[A\n",
            "Downloading data:  22% 99.5M/458M [00:09<00:26, 13.6MB/s]\u001b[A\n",
            "Downloading data:  22% 103M/458M [00:09<00:25, 13.7MB/s] \u001b[A\n",
            "Downloading data:  23% 106M/458M [00:09<00:25, 13.8MB/s]\u001b[A\n",
            "Downloading data:  24% 109M/458M [00:10<00:25, 13.9MB/s]\u001b[A\n",
            "Downloading data:  25% 112M/458M [00:10<00:24, 14.0MB/s]\u001b[A\n",
            "Downloading data:  25% 116M/458M [00:10<00:24, 14.1MB/s]\u001b[A\n",
            "Downloading data:  26% 119M/458M [00:10<00:23, 14.2MB/s]\u001b[A\n",
            "Downloading data:  27% 122M/458M [00:11<00:23, 14.3MB/s]\u001b[A\n",
            "Downloading data:  27% 126M/458M [00:11<00:23, 14.4MB/s]\u001b[A\n",
            "Downloading data:  28% 129M/458M [00:11<00:22, 14.4MB/s]\u001b[A\n",
            "Downloading data:  29% 131M/458M [00:11<00:26, 12.3MB/s]\u001b[A\n",
            "Downloading data:  29% 134M/458M [00:11<00:24, 13.1MB/s]\u001b[A\n",
            "Downloading data:  30% 138M/458M [00:12<00:23, 13.7MB/s]\u001b[A\n",
            "Downloading data:  31% 141M/458M [00:12<00:22, 14.0MB/s]\u001b[A\n",
            "Downloading data:  31% 144M/458M [00:12<00:22, 13.8MB/s]\u001b[A\n",
            "Downloading data:  32% 148M/458M [00:12<00:21, 14.3MB/s]\u001b[A\n",
            "Downloading data:  33% 151M/458M [00:13<00:21, 14.6MB/s]\u001b[A\n",
            "Downloading data:  34% 155M/458M [00:13<00:20, 14.8MB/s]\u001b[A\n",
            "Downloading data:  35% 158M/458M [00:13<00:20, 15.0MB/s]\u001b[A\n",
            "Downloading data:  35% 162M/458M [00:13<00:19, 15.2MB/s]\u001b[A\n",
            "Downloading data:  36% 165M/458M [00:14<00:19, 15.3MB/s]\u001b[A\n",
            "Downloading data:  37% 169M/458M [00:14<00:18, 15.5MB/s]\u001b[A\n",
            "Downloading data:  38% 173M/458M [00:14<00:18, 15.7MB/s]\u001b[A\n",
            "Downloading data:  39% 176M/458M [00:14<00:17, 15.8MB/s]\u001b[A\n",
            "Downloading data:  39% 180M/458M [00:14<00:17, 15.9MB/s]\u001b[A\n",
            "Downloading data:  40% 184M/458M [00:15<00:17, 16.1MB/s]\u001b[A\n",
            "Downloading data:  41% 188M/458M [00:15<00:16, 16.2MB/s]\u001b[A\n",
            "Downloading data:  42% 192M/458M [00:15<00:16, 16.3MB/s]\u001b[A\n",
            "Downloading data:  43% 195M/458M [00:15<00:15, 16.5MB/s]\u001b[A\n",
            "Downloading data:  43% 199M/458M [00:16<00:15, 16.6MB/s]\u001b[A\n",
            "Downloading data:  44% 203M/458M [00:16<00:15, 16.7MB/s]\u001b[A\n",
            "Downloading data:  45% 207M/458M [00:16<00:15, 16.6MB/s]\u001b[A\n",
            "Downloading data:  46% 211M/458M [00:16<00:14, 16.7MB/s]\u001b[A\n",
            "Downloading data:  47% 214M/458M [00:16<00:14, 16.4MB/s]\u001b[A\n",
            "Downloading data:  48% 218M/458M [00:17<00:14, 16.6MB/s]\u001b[A\n",
            "Downloading data:  48% 222M/458M [00:17<00:14, 16.7MB/s]\u001b[A\n",
            "Downloading data:  49% 226M/458M [00:17<00:14, 16.5MB/s]\u001b[A\n",
            "Downloading data:  50% 230M/458M [00:17<00:13, 16.7MB/s]\u001b[A\n",
            "Downloading data:  51% 234M/458M [00:18<00:13, 16.9MB/s]\u001b[A\n",
            "Downloading data:  52% 238M/458M [00:18<00:12, 17.1MB/s]\u001b[A\n",
            "Downloading data:  53% 242M/458M [00:18<00:12, 17.2MB/s]\u001b[A\n",
            "Downloading data:  54% 246M/458M [00:18<00:12, 17.3MB/s]\u001b[A\n",
            "Downloading data:  54% 249M/458M [00:19<00:12, 16.9MB/s]\u001b[A\n",
            "Downloading data:  55% 254M/458M [00:19<00:11, 17.3MB/s]\u001b[A\n",
            "Downloading data:  56% 258M/458M [00:19<00:11, 17.5MB/s]\u001b[A\n",
            "Downloading data:  57% 262M/458M [00:19<00:11, 17.8MB/s]\u001b[A\n",
            "Downloading data:  58% 266M/458M [00:19<00:10, 18.0MB/s]\u001b[A\n",
            "Downloading data:  59% 271M/458M [00:20<00:10, 18.3MB/s]\u001b[A\n",
            "Downloading data:  60% 275M/458M [00:20<00:09, 18.5MB/s]\u001b[A\n",
            "Downloading data:  61% 279M/458M [00:20<00:08, 22.0MB/s]\u001b[A\n",
            "Downloading data:  61% 282M/458M [00:20<00:08, 19.6MB/s]\u001b[A\n",
            "Downloading data:  62% 284M/458M [00:20<00:09, 17.5MB/s]\u001b[A\n",
            "Downloading data:  63% 288M/458M [00:21<00:09, 17.9MB/s]\u001b[A\n",
            "Downloading data:  64% 293M/458M [00:21<00:08, 18.5MB/s]\u001b[A\n",
            "Downloading data:  65% 297M/458M [00:21<00:08, 18.8MB/s]\u001b[A\n",
            "Downloading data:  66% 301M/458M [00:21<00:08, 19.0MB/s]\u001b[A\n",
            "Downloading data:  67% 306M/458M [00:22<00:07, 19.3MB/s]\u001b[A\n",
            "Downloading data:  68% 311M/458M [00:22<00:07, 19.4MB/s]\u001b[A\n",
            "Downloading data:  69% 315M/458M [00:22<00:07, 19.6MB/s]\u001b[A\n",
            "Downloading data:  70% 320M/458M [00:22<00:07, 19.7MB/s]\u001b[A\n",
            "Downloading data:  71% 324M/458M [00:22<00:06, 19.9MB/s]\u001b[A\n",
            "Downloading data:  72% 329M/458M [00:23<00:06, 20.0MB/s]\u001b[A\n",
            "Downloading data:  73% 334M/458M [00:23<00:06, 20.1MB/s]\u001b[A\n",
            "Downloading data:  74% 338M/458M [00:23<00:05, 20.2MB/s]\u001b[A\n",
            "Downloading data:  75% 343M/458M [00:23<00:05, 20.4MB/s]\u001b[A\n",
            "Downloading data:  75% 346M/458M [00:24<00:06, 17.0MB/s]\u001b[A\n",
            "Downloading data:  76% 350M/458M [00:24<00:05, 18.1MB/s]\u001b[A\n",
            "Downloading data:  77% 352M/458M [00:24<00:06, 15.5MB/s]\u001b[A\n",
            "Downloading data:  78% 355M/458M [00:24<00:07, 14.4MB/s]\u001b[A\n",
            "Downloading data:  78% 358M/458M [00:25<00:07, 13.3MB/s]\u001b[A\n",
            "Downloading data:  79% 360M/458M [00:25<00:07, 12.8MB/s]\u001b[A\n",
            "Downloading data:  79% 363M/458M [00:25<00:07, 12.5MB/s]\u001b[A\n",
            "Downloading data:  80% 366M/458M [00:25<00:07, 12.3MB/s]\u001b[A\n",
            "Downloading data:  80% 369M/458M [00:25<00:07, 12.3MB/s]\u001b[A\n",
            "Downloading data:  81% 371M/458M [00:26<00:07, 11.1MB/s]\u001b[A\n",
            "Downloading data:  81% 373M/458M [00:26<00:07, 11.4MB/s]\u001b[A\n",
            "Downloading data:  82% 376M/458M [00:26<00:07, 11.6MB/s]\u001b[A\n",
            "Downloading data:  83% 379M/458M [00:26<00:06, 11.8MB/s]\u001b[A\n",
            "Downloading data:  83% 382M/458M [00:27<00:06, 12.0MB/s]\u001b[A\n",
            "Downloading data:  84% 385M/458M [00:27<00:06, 12.2MB/s]\u001b[A\n",
            "Downloading data:  85% 387M/458M [00:27<00:05, 12.2MB/s]\u001b[A\n",
            "Downloading data:  85% 390M/458M [00:27<00:05, 12.3MB/s]\u001b[A\n",
            "Downloading data:  86% 393M/458M [00:28<00:05, 12.4MB/s]\u001b[A\n",
            "Downloading data:  86% 396M/458M [00:28<00:04, 12.4MB/s]\u001b[A\n",
            "Downloading data:  87% 399M/458M [00:28<00:04, 12.5MB/s]\u001b[A\n",
            "Downloading data:  88% 402M/458M [00:28<00:04, 12.5MB/s]\u001b[A\n",
            "Downloading data:  88% 405M/458M [00:28<00:04, 12.5MB/s]\u001b[A\n",
            "Downloading data:  89% 408M/458M [00:29<00:03, 12.6MB/s]\u001b[A\n",
            "Downloading data:  90% 411M/458M [00:29<00:03, 12.7MB/s]\u001b[A\n",
            "Downloading data:  90% 414M/458M [00:29<00:03, 12.8MB/s]\u001b[A\n",
            "Downloading data:  91% 417M/458M [00:29<00:03, 12.9MB/s]\u001b[A\n",
            "Downloading data:  92% 420M/458M [00:30<00:02, 13.0MB/s]\u001b[A\n",
            "Downloading data:  92% 423M/458M [00:30<00:02, 13.1MB/s]\u001b[A\n",
            "Downloading data:  93% 426M/458M [00:30<00:02, 13.2MB/s]\u001b[A\n",
            "Downloading data:  94% 429M/458M [00:30<00:02, 13.3MB/s]\u001b[A\n",
            "Downloading data:  94% 432M/458M [00:31<00:01, 13.4MB/s]\u001b[A\n",
            "Downloading data:  95% 435M/458M [00:31<00:01, 13.5MB/s]\u001b[A\n",
            "Downloading data:  96% 438M/458M [00:31<00:01, 13.6MB/s]\u001b[A\n",
            "Downloading data:  96% 442M/458M [00:31<00:01, 13.7MB/s]\u001b[A\n",
            "Downloading data:  97% 445M/458M [00:31<00:00, 13.8MB/s]\u001b[A\n",
            "Downloading data:  98% 448M/458M [00:32<00:00, 13.9MB/s]\u001b[A\n",
            "Downloading data:  99% 451M/458M [00:32<00:00, 13.9MB/s]\u001b[A\n",
            "Downloading data:  99% 454M/458M [00:32<00:00, 14.0MB/s]\u001b[A\n",
            "Downloading data: 100% 458M/458M [00:32<00:00, 13.9MB/s]\n",
            "\n",
            "Downloading data:   0% 0.00/433M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   0% 8.19k/433M [00:00<3:14:57, 37.0kB/s]\u001b[A\n",
            "Downloading data:   0% 43.0k/433M [00:00<1:06:59, 108kB/s] \u001b[A\n",
            "Downloading data:   0% 113k/433M [00:00<35:48, 201kB/s]   \u001b[A\n",
            "Downloading data:   0% 269k/433M [00:00<18:03, 399kB/s]\u001b[A\n",
            "Downloading data:   0% 565k/433M [00:01<09:48, 734kB/s]\u001b[A\n",
            "Downloading data:   0% 1.16M/433M [00:01<05:11, 1.39MB/s]\u001b[A\n",
            "Downloading data:   1% 2.35M/433M [00:01<02:40, 2.68MB/s]\u001b[A\n",
            "Downloading data:   1% 4.73M/433M [00:01<01:21, 5.23MB/s]\u001b[A\n",
            "Downloading data:   2% 7.45M/433M [00:02<00:57, 7.40MB/s]\u001b[A\n",
            "Downloading data:   2% 10.2M/433M [00:02<00:47, 8.95MB/s]\u001b[A\n",
            "Downloading data:   3% 13.1M/433M [00:02<00:41, 10.1MB/s]\u001b[A\n",
            "Downloading data:   4% 16.0M/433M [00:02<00:37, 11.1MB/s]\u001b[A\n",
            "Downloading data:   4% 19.1M/433M [00:02<00:36, 11.2MB/s]\u001b[A\n",
            "Downloading data:   5% 22.2M/433M [00:03<00:32, 12.7MB/s]\u001b[A\n",
            "Downloading data:   6% 25.3M/433M [00:03<00:31, 13.1MB/s]\u001b[A\n",
            "Downloading data:   7% 28.5M/433M [00:03<00:30, 13.4MB/s]\u001b[A\n",
            "Downloading data:   7% 31.7M/433M [00:03<00:29, 13.8MB/s]\u001b[A\n",
            "Downloading data:   8% 35.1M/433M [00:04<00:28, 14.1MB/s]\u001b[A\n",
            "Downloading data:   9% 38.4M/433M [00:04<00:27, 14.4MB/s]\u001b[A\n",
            "Downloading data:  10% 41.9M/433M [00:04<00:26, 14.7MB/s]\u001b[A\n",
            "Downloading data:  10% 45.3M/433M [00:04<00:25, 14.9MB/s]\u001b[A\n",
            "Downloading data:  11% 48.9M/433M [00:04<00:25, 15.2MB/s]\u001b[A\n",
            "Downloading data:  12% 52.5M/433M [00:05<00:24, 15.6MB/s]\u001b[A\n",
            "Downloading data:  13% 56.4M/433M [00:05<00:23, 16.1MB/s]\u001b[A\n",
            "Downloading data:  14% 60.4M/433M [00:05<00:22, 16.6MB/s]\u001b[A\n",
            "Downloading data:  15% 64.5M/433M [00:05<00:21, 17.1MB/s]\u001b[A\n",
            "Downloading data:  16% 68.7M/433M [00:06<00:20, 17.7MB/s]\u001b[A\n",
            "Downloading data:  17% 73.1M/433M [00:06<00:19, 18.3MB/s]\u001b[A\n",
            "Downloading data:  18% 77.6M/433M [00:06<00:15, 22.2MB/s]\u001b[A\n",
            "Downloading data:  19% 80.1M/433M [00:06<00:17, 20.3MB/s]\u001b[A\n",
            "Downloading data:  19% 82.3M/433M [00:06<00:19, 18.2MB/s]\u001b[A\n",
            "Downloading data:  19% 84.2M/433M [00:06<00:19, 17.7MB/s]\u001b[A\n",
            "Downloading data:  20% 87.0M/433M [00:06<00:17, 19.7MB/s]\u001b[A\n",
            "Downloading data:  21% 89.1M/433M [00:07<00:18, 18.1MB/s]\u001b[A\n",
            "Downloading data:  21% 92.4M/433M [00:07<00:15, 21.5MB/s]\u001b[A\n",
            "Downloading data:  22% 94.7M/433M [00:07<00:16, 20.2MB/s]\u001b[A\n",
            "Downloading data:  23% 97.6M/433M [00:07<00:15, 22.3MB/s]\u001b[A\n",
            "Downloading data:  23% 99.9M/433M [00:07<00:15, 21.2MB/s]\u001b[A\n",
            "Downloading data:  24% 102M/433M [00:07<00:14, 22.3MB/s] \u001b[A\n",
            "Downloading data:  24% 105M/433M [00:07<00:15, 21.0MB/s]\u001b[A\n",
            "Downloading data:  25% 108M/433M [00:07<00:14, 22.8MB/s]\u001b[A\n",
            "Downloading data:  25% 110M/433M [00:07<00:14, 21.6MB/s]\u001b[A\n",
            "Downloading data:  26% 112M/433M [00:08<00:14, 22.3MB/s]\u001b[A\n",
            "Downloading data:  26% 115M/433M [00:08<00:15, 20.7MB/s]\u001b[A\n",
            "Downloading data:  27% 118M/433M [00:08<00:13, 23.2MB/s]\u001b[A\n",
            "Downloading data:  28% 120M/433M [00:08<00:16, 18.6MB/s]\u001b[A\n",
            "Downloading data:  29% 123M/433M [00:08<00:14, 20.9MB/s]\u001b[A\n",
            "Downloading data:  29% 126M/433M [00:08<00:14, 20.6MB/s]\u001b[A\n",
            "Downloading data:  30% 129M/433M [00:08<00:13, 21.9MB/s]\u001b[A\n",
            "Downloading data:  30% 131M/433M [00:08<00:13, 21.6MB/s]\u001b[A\n",
            "Downloading data:  31% 134M/433M [00:09<00:13, 22.9MB/s]\u001b[A\n",
            "Downloading data:  31% 136M/433M [00:09<00:13, 21.9MB/s]\u001b[A\n",
            "Downloading data:  32% 138M/433M [00:09<00:13, 21.5MB/s]\u001b[A\n",
            "Downloading data:  32% 140M/433M [00:09<00:13, 21.5MB/s]\u001b[A\n",
            "Downloading data:  33% 143M/433M [00:09<00:13, 22.2MB/s]\u001b[A\n",
            "Downloading data:  34% 145M/433M [00:09<00:13, 21.4MB/s]\u001b[A\n",
            "Downloading data:  34% 148M/433M [00:09<00:12, 23.0MB/s]\u001b[A\n",
            "Downloading data:  35% 151M/433M [00:09<00:12, 21.9MB/s]\u001b[A\n",
            "Downloading data:  36% 154M/433M [00:09<00:11, 23.3MB/s]\u001b[A\n",
            "Downloading data:  36% 156M/433M [00:10<00:12, 21.9MB/s]\u001b[A\n",
            "Downloading data:  37% 158M/433M [00:10<00:12, 22.6MB/s]\u001b[A\n",
            "Downloading data:  37% 161M/433M [00:10<00:12, 21.1MB/s]\u001b[A\n",
            "Downloading data:  38% 163M/433M [00:10<00:12, 22.4MB/s]\u001b[A\n",
            "Downloading data:  38% 166M/433M [00:10<00:12, 20.7MB/s]\u001b[A\n",
            "Downloading data:  39% 169M/433M [00:10<00:11, 23.8MB/s]\u001b[A\n",
            "Downloading data:  40% 171M/433M [00:10<00:12, 21.6MB/s]\u001b[A\n",
            "Downloading data:  40% 174M/433M [00:10<00:11, 22.4MB/s]\u001b[A\n",
            "Downloading data:  41% 176M/433M [00:11<00:11, 22.1MB/s]\u001b[A\n",
            "Downloading data:  41% 179M/433M [00:11<00:12, 21.1MB/s]\u001b[A\n",
            "Downloading data:  42% 182M/433M [00:11<00:10, 23.6MB/s]\u001b[A\n",
            "Downloading data:  43% 184M/433M [00:11<00:11, 22.2MB/s]\u001b[A\n",
            "Downloading data:  43% 186M/433M [00:11<00:11, 22.3MB/s]\u001b[A\n",
            "Downloading data:  44% 189M/433M [00:11<00:11, 21.6MB/s]\u001b[A\n",
            "Downloading data:  44% 191M/433M [00:11<00:11, 21.5MB/s]\u001b[A\n",
            "Downloading data:  45% 194M/433M [00:11<00:10, 22.5MB/s]\u001b[A\n",
            "Downloading data:  45% 196M/433M [00:11<00:10, 21.7MB/s]\u001b[A\n",
            "Downloading data:  46% 199M/433M [00:12<00:10, 23.2MB/s]\u001b[A\n",
            "Downloading data:  47% 201M/433M [00:12<00:10, 22.1MB/s]\u001b[A\n",
            "Downloading data:  47% 204M/433M [00:12<00:09, 23.5MB/s]\u001b[A\n",
            "Downloading data:  48% 207M/433M [00:12<00:10, 21.5MB/s]\u001b[A\n",
            "Downloading data:  48% 210M/433M [00:12<00:11, 20.0MB/s]\u001b[A\n",
            "Downloading data:  49% 214M/433M [00:12<00:09, 23.0MB/s]\u001b[A\n",
            "Downloading data:  50% 216M/433M [00:12<00:09, 21.7MB/s]\u001b[A\n",
            "Downloading data:  51% 219M/433M [00:12<00:09, 22.9MB/s]\u001b[A\n",
            "Downloading data:  51% 221M/433M [00:13<00:11, 19.2MB/s]\u001b[A\n",
            "Downloading data:  52% 225M/433M [00:13<00:10, 20.4MB/s]\u001b[A\n",
            "Downloading data:  53% 228M/433M [00:13<00:08, 23.6MB/s]\u001b[A\n",
            "Downloading data:  53% 231M/433M [00:13<00:09, 21.9MB/s]\u001b[A\n",
            "Downloading data:  54% 233M/433M [00:13<00:08, 23.3MB/s]\u001b[A\n",
            "Downloading data:  55% 236M/433M [00:13<00:08, 21.9MB/s]\u001b[A\n",
            "Downloading data:  55% 238M/433M [00:13<00:08, 22.8MB/s]\u001b[A\n",
            "Downloading data:  56% 241M/433M [00:13<00:08, 21.3MB/s]\u001b[A\n",
            "Downloading data:  56% 243M/433M [00:14<00:08, 22.7MB/s]\u001b[A\n",
            "Downloading data:  57% 246M/433M [00:14<00:09, 20.5MB/s]\u001b[A\n",
            "Downloading data:  57% 249M/433M [00:14<00:09, 18.5MB/s]\u001b[A\n",
            "Downloading data:  58% 253M/433M [00:14<00:07, 23.2MB/s]\u001b[A\n",
            "Downloading data:  59% 255M/433M [00:14<00:08, 20.0MB/s]\u001b[A\n",
            "Downloading data:  60% 259M/433M [00:14<00:08, 20.9MB/s]\u001b[A\n",
            "Downloading data:  61% 262M/433M [00:14<00:07, 23.1MB/s]\u001b[A\n",
            "Downloading data:  61% 265M/433M [00:15<00:07, 21.2MB/s]\u001b[A\n",
            "Downloading data:  62% 269M/433M [00:15<00:08, 20.4MB/s]\u001b[A\n",
            "Downloading data:  63% 272M/433M [00:15<00:07, 22.7MB/s]\u001b[A\n",
            "Downloading data:  63% 274M/433M [00:15<00:07, 21.6MB/s]\u001b[A\n",
            "Downloading data:  64% 278M/433M [00:15<00:06, 25.4MB/s]\u001b[A\n",
            "Downloading data:  65% 281M/433M [00:15<00:07, 21.7MB/s]\u001b[A\n",
            "Downloading data:  66% 283M/433M [00:15<00:07, 19.9MB/s]\u001b[A\n",
            "Downloading data:  66% 287M/433M [00:16<00:06, 22.6MB/s]\u001b[A\n",
            "Downloading data:  67% 289M/433M [00:16<00:06, 21.4MB/s]\u001b[A\n",
            "Downloading data:  68% 292M/433M [00:16<00:05, 24.1MB/s]\u001b[A\n",
            "Downloading data:  68% 295M/433M [00:16<00:07, 18.0MB/s]\u001b[A\n",
            "Downloading data:  69% 299M/433M [00:16<00:05, 23.0MB/s]\u001b[A\n",
            "Downloading data:  70% 302M/433M [00:16<00:05, 22.4MB/s]\u001b[A\n",
            "Downloading data:  70% 304M/433M [00:16<00:06, 19.2MB/s]\u001b[A\n",
            "Downloading data:  72% 309M/433M [00:17<00:05, 24.3MB/s]\u001b[A\n",
            "Downloading data:  72% 312M/433M [00:17<00:05, 22.9MB/s]\u001b[A\n",
            "Downloading data:  73% 315M/433M [00:17<00:05, 22.1MB/s]\u001b[A\n",
            "Downloading data:  73% 317M/433M [00:17<00:05, 20.5MB/s]\u001b[A\n",
            "Downloading data:  74% 319M/433M [00:17<00:05, 19.6MB/s]\u001b[A\n",
            "Downloading data:  74% 322M/433M [00:17<00:05, 21.0MB/s]\u001b[A\n",
            "Downloading data:  75% 324M/433M [00:17<00:05, 19.8MB/s]\u001b[A\n",
            "Downloading data:  76% 327M/433M [00:17<00:04, 21.8MB/s]\u001b[A\n",
            "Downloading data:  76% 329M/433M [00:18<00:04, 21.8MB/s]\u001b[A\n",
            "Downloading data:  77% 332M/433M [00:18<00:04, 22.7MB/s]\u001b[A\n",
            "Downloading data:  77% 335M/433M [00:18<00:04, 21.9MB/s]\u001b[A\n",
            "Downloading data:  78% 337M/433M [00:18<00:04, 22.0MB/s]\u001b[A\n",
            "Downloading data:  78% 339M/433M [00:18<00:04, 21.2MB/s]\u001b[A\n",
            "Downloading data:  79% 342M/433M [00:18<00:04, 22.3MB/s]\u001b[A\n",
            "Downloading data:  80% 344M/433M [00:18<00:04, 20.9MB/s]\u001b[A\n",
            "Downloading data:  80% 347M/433M [00:18<00:03, 23.6MB/s]\u001b[A\n",
            "Downloading data:  81% 349M/433M [00:18<00:03, 22.5MB/s]\u001b[A\n",
            "Downloading data:  81% 352M/433M [00:19<00:03, 23.5MB/s]\u001b[A\n",
            "Downloading data:  82% 354M/433M [00:19<00:03, 22.3MB/s]\u001b[A\n",
            "Downloading data:  83% 357M/433M [00:19<00:03, 23.1MB/s]\u001b[A\n",
            "Downloading data:  83% 359M/433M [00:19<00:03, 20.7MB/s]\u001b[A\n",
            "Downloading data:  84% 363M/433M [00:19<00:02, 23.5MB/s]\u001b[A\n",
            "Downloading data:  84% 365M/433M [00:19<00:03, 20.0MB/s]\u001b[A\n",
            "Downloading data:  85% 369M/433M [00:19<00:02, 24.8MB/s]\u001b[A\n",
            "Downloading data:  86% 372M/433M [00:19<00:02, 22.6MB/s]\u001b[A\n",
            "Downloading data:  87% 375M/433M [00:20<00:02, 23.0MB/s]\u001b[A\n",
            "Downloading data:  87% 377M/433M [00:20<00:02, 22.3MB/s]\u001b[A\n",
            "Downloading data:  88% 380M/433M [00:20<00:02, 19.1MB/s]\u001b[A\n",
            "Downloading data:  89% 384M/433M [00:20<00:01, 24.9MB/s]\u001b[A\n",
            "Downloading data:  89% 387M/433M [00:20<00:02, 22.2MB/s]\u001b[A\n",
            "Downloading data:  90% 390M/433M [00:20<00:02, 19.9MB/s]\u001b[A\n",
            "Downloading data:  91% 395M/433M [00:20<00:01, 25.8MB/s]\u001b[A\n",
            "Downloading data:  92% 398M/433M [00:21<00:01, 22.6MB/s]\u001b[A\n",
            "Downloading data:  92% 400M/433M [00:21<00:01, 19.4MB/s]\u001b[A\n",
            "Downloading data:  94% 405M/433M [00:21<00:01, 20.6MB/s]\u001b[A\n",
            "Downloading data:  95% 410M/433M [00:21<00:01, 21.3MB/s]\u001b[A\n",
            "Downloading data:  96% 416M/433M [00:21<00:00, 21.7MB/s]\u001b[A\n",
            "Downloading data:  97% 421M/433M [00:22<00:00, 22.1MB/s]\u001b[A\n",
            "Downloading data:  98% 426M/433M [00:22<00:00, 22.4MB/s]\u001b[A\n",
            "Downloading data: 100% 433M/433M [00:22<00:00, 19.1MB/s]\n",
            "\n",
            "Downloading data:   0% 0.00/447M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   0% 16.4k/447M [00:00<1:40:51, 73.9kB/s]\u001b[A\n",
            "Downloading data:   0% 51.2k/447M [00:00<1:01:43, 121kB/s] \u001b[A\n",
            "Downloading data:   0% 138k/447M [00:00<31:03, 240kB/s]   \u001b[A\n",
            "Downloading data:   0% 312k/447M [00:00<16:36, 449kB/s]\u001b[A\n",
            "Downloading data:   0% 660k/447M [00:01<08:52, 839kB/s]\u001b[A\n",
            "Downloading data:   0% 1.34M/447M [00:01<04:44, 1.57MB/s]\u001b[A\n",
            "Downloading data:   1% 2.57M/447M [00:01<02:37, 2.82MB/s]\u001b[A\n",
            "Downloading data:   1% 5.09M/447M [00:01<01:21, 5.44MB/s]\u001b[A\n",
            "Downloading data:   2% 7.74M/447M [00:02<00:59, 7.40MB/s]\u001b[A\n",
            "Downloading data:   2% 10.4M/447M [00:02<00:50, 8.69MB/s]\u001b[A\n",
            "Downloading data:   3% 13.1M/447M [00:02<00:45, 9.58MB/s]\u001b[A\n",
            "Downloading data:   4% 15.7M/447M [00:02<00:42, 10.2MB/s]\u001b[A\n",
            "Downloading data:   4% 18.4M/447M [00:02<00:39, 10.8MB/s]\u001b[A\n",
            "Downloading data:   5% 21.2M/447M [00:03<00:38, 11.1MB/s]\u001b[A\n",
            "Downloading data:   5% 23.9M/447M [00:03<00:37, 11.4MB/s]\u001b[A\n",
            "Downloading data:   6% 26.6M/447M [00:03<00:36, 11.6MB/s]\u001b[A\n",
            "Downloading data:   7% 29.4M/447M [00:03<00:35, 11.7MB/s]\u001b[A\n",
            "Downloading data:   7% 32.1M/447M [00:04<00:35, 11.8MB/s]\u001b[A\n",
            "Downloading data:   8% 34.9M/447M [00:04<00:34, 12.0MB/s]\u001b[A\n",
            "Downloading data:   8% 37.8M/447M [00:04<00:33, 12.1MB/s]\u001b[A\n",
            "Downloading data:   9% 40.7M/447M [00:04<00:32, 12.4MB/s]\u001b[A\n",
            "Downloading data:  10% 43.7M/447M [00:05<00:32, 12.5MB/s]\u001b[A\n",
            "Downloading data:  10% 46.6M/447M [00:05<00:31, 12.7MB/s]\u001b[A\n",
            "Downloading data:  11% 49.6M/447M [00:05<00:31, 12.7MB/s]\u001b[A\n",
            "Downloading data:  12% 52.5M/447M [00:05<00:30, 12.8MB/s]\u001b[A\n",
            "Downloading data:  12% 55.5M/447M [00:05<00:30, 12.9MB/s]\u001b[A\n",
            "Downloading data:  13% 58.5M/447M [00:06<00:29, 13.0MB/s]\u001b[A\n",
            "Downloading data:  14% 61.5M/447M [00:06<00:29, 13.0MB/s]\u001b[A\n",
            "Downloading data:  14% 64.4M/447M [00:06<00:29, 13.0MB/s]\u001b[A\n",
            "Downloading data:  15% 67.1M/447M [00:06<00:25, 15.1MB/s]\u001b[A\n",
            "Downloading data:  15% 68.8M/447M [00:06<00:26, 14.2MB/s]\u001b[A\n",
            "Downloading data:  16% 70.5M/447M [00:07<00:30, 12.2MB/s]\u001b[A\n",
            "Downloading data:  16% 73.5M/447M [00:07<00:29, 12.5MB/s]\u001b[A\n",
            "Downloading data:  17% 76.5M/447M [00:07<00:28, 12.8MB/s]\u001b[A\n",
            "Downloading data:  18% 79.6M/447M [00:07<00:28, 13.0MB/s]\u001b[A\n",
            "Downloading data:  18% 82.8M/447M [00:07<00:27, 13.4MB/s]\u001b[A\n",
            "Downloading data:  19% 85.9M/447M [00:08<00:26, 13.5MB/s]\u001b[A\n",
            "Downloading data:  20% 89.1M/447M [00:08<00:26, 13.5MB/s]\u001b[A\n",
            "Downloading data:  21% 92.3M/447M [00:08<00:25, 13.8MB/s]\u001b[A\n",
            "Downloading data:  21% 95.5M/447M [00:08<00:25, 13.9MB/s]\u001b[A\n",
            "Downloading data:  22% 98.7M/447M [00:09<00:25, 13.9MB/s]\u001b[A\n",
            "Downloading data:  23% 102M/447M [00:09<00:24, 14.1MB/s] \u001b[A\n",
            "Downloading data:  23% 105M/447M [00:09<00:20, 16.9MB/s]\u001b[A\n",
            "Downloading data:  24% 107M/447M [00:09<00:21, 15.7MB/s]\u001b[A\n",
            "Downloading data:  24% 109M/447M [00:09<00:25, 13.1MB/s]\u001b[A\n",
            "Downloading data:  25% 112M/447M [00:10<00:25, 13.3MB/s]\u001b[A\n",
            "Downloading data:  26% 115M/447M [00:10<00:19, 16.7MB/s]\u001b[A\n",
            "Downloading data:  26% 117M/447M [00:10<00:21, 15.6MB/s]\u001b[A\n",
            "Downloading data:  27% 119M/447M [00:10<00:25, 13.0MB/s]\u001b[A\n",
            "Downloading data:  27% 122M/447M [00:10<00:24, 13.3MB/s]\u001b[A\n",
            "Downloading data:  28% 125M/447M [00:10<00:23, 13.8MB/s]\u001b[A\n",
            "Downloading data:  28% 127M/447M [00:11<00:28, 11.3MB/s]\u001b[A\n",
            "Downloading data:  29% 130M/447M [00:11<00:25, 12.3MB/s]\u001b[A\n",
            "Downloading data:  30% 133M/447M [00:11<00:25, 12.3MB/s]\u001b[A\n",
            "Downloading data:  30% 136M/447M [00:11<00:23, 13.1MB/s]\u001b[A\n",
            "Downloading data:  31% 139M/447M [00:12<00:23, 12.9MB/s]\u001b[A\n",
            "Downloading data:  32% 142M/447M [00:12<00:22, 13.3MB/s]\u001b[A\n",
            "Downloading data:  33% 146M/447M [00:12<00:22, 13.7MB/s]\u001b[A\n",
            "Downloading data:  33% 149M/447M [00:12<00:21, 14.0MB/s]\u001b[A\n",
            "Downloading data:  34% 152M/447M [00:12<00:20, 14.3MB/s]\u001b[A\n",
            "Downloading data:  35% 156M/447M [00:13<00:20, 14.5MB/s]\u001b[A\n",
            "Downloading data:  36% 159M/447M [00:13<00:19, 15.0MB/s]\u001b[A\n",
            "Downloading data:  36% 163M/447M [00:13<00:18, 15.2MB/s]\u001b[A\n",
            "Downloading data:  37% 167M/447M [00:13<00:18, 15.3MB/s]\u001b[A\n",
            "Downloading data:  38% 170M/447M [00:14<00:17, 15.5MB/s]\u001b[A\n",
            "Downloading data:  39% 174M/447M [00:14<00:17, 15.7MB/s]\u001b[A\n",
            "Downloading data:  40% 177M/447M [00:14<00:17, 15.4MB/s]\u001b[A\n",
            "Downloading data:  40% 181M/447M [00:14<00:17, 15.7MB/s]\u001b[A\n",
            "Downloading data:  41% 185M/447M [00:15<00:16, 15.8MB/s]\u001b[A\n",
            "Downloading data:  42% 188M/447M [00:15<00:16, 16.0MB/s]\u001b[A\n",
            "Downloading data:  43% 192M/447M [00:15<00:15, 16.2MB/s]\u001b[A\n",
            "Downloading data:  44% 196M/447M [00:15<00:15, 16.3MB/s]\u001b[A\n",
            "Downloading data:  44% 197M/447M [00:15<00:18, 13.2MB/s]\u001b[A\n",
            "Downloading data:  45% 201M/447M [00:16<00:17, 13.9MB/s]\u001b[A\n",
            "Downloading data:  45% 202M/447M [00:16<00:21, 11.4MB/s]\u001b[A\n",
            "Downloading data:  45% 203M/447M [00:16<00:25, 9.48MB/s]\u001b[A\n",
            "Downloading data:  46% 205M/447M [00:16<00:27, 8.85MB/s]\u001b[A\n",
            "Downloading data:  46% 208M/447M [00:17<00:24, 9.67MB/s]\u001b[A\n",
            "Downloading data:  47% 210M/447M [00:17<00:23, 10.2MB/s]\u001b[A\n",
            "Downloading data:  48% 213M/447M [00:17<00:21, 10.7MB/s]\u001b[A\n",
            "Downloading data:  48% 216M/447M [00:17<00:20, 11.1MB/s]\u001b[A\n",
            "Downloading data:  49% 218M/447M [00:18<00:20, 11.3MB/s]\u001b[A\n",
            "Downloading data:  49% 221M/447M [00:18<00:19, 11.7MB/s]\u001b[A\n",
            "Downloading data:  50% 224M/447M [00:18<00:18, 12.0MB/s]\u001b[A\n",
            "Downloading data:  51% 227M/447M [00:18<00:18, 12.1MB/s]\u001b[A\n",
            "Downloading data:  51% 230M/447M [00:18<00:17, 12.4MB/s]\u001b[A\n",
            "Downloading data:  52% 233M/447M [00:19<00:17, 12.6MB/s]\u001b[A\n",
            "Downloading data:  53% 236M/447M [00:19<00:16, 12.5MB/s]\u001b[A\n",
            "Downloading data:  53% 239M/447M [00:19<00:16, 12.9MB/s]\u001b[A\n",
            "Downloading data:  54% 242M/447M [00:19<00:15, 13.1MB/s]\u001b[A\n",
            "Downloading data:  55% 245M/447M [00:20<00:15, 13.5MB/s]\u001b[A\n",
            "Downloading data:  55% 248M/447M [00:20<00:14, 13.5MB/s]\u001b[A\n",
            "Downloading data:  56% 251M/447M [00:20<00:14, 13.7MB/s]\u001b[A\n",
            "Downloading data:  57% 255M/447M [00:20<00:13, 14.0MB/s]\u001b[A\n",
            "Downloading data:  58% 258M/447M [00:20<00:13, 14.2MB/s]\u001b[A\n",
            "Downloading data:  58% 262M/447M [00:21<00:12, 14.5MB/s]\u001b[A\n",
            "Downloading data:  59% 265M/447M [00:21<00:12, 14.6MB/s]\u001b[A\n",
            "Downloading data:  60% 268M/447M [00:21<00:12, 14.8MB/s]\u001b[A\n",
            "Downloading data:  61% 272M/447M [00:21<00:11, 15.0MB/s]\u001b[A\n",
            "Downloading data:  62% 276M/447M [00:22<00:11, 15.2MB/s]\u001b[A\n",
            "Downloading data:  62% 279M/447M [00:22<00:11, 15.3MB/s]\u001b[A\n",
            "Downloading data:  63% 283M/447M [00:22<00:10, 15.5MB/s]\u001b[A\n",
            "Downloading data:  64% 286M/447M [00:22<00:10, 15.7MB/s]\u001b[A\n",
            "Downloading data:  65% 290M/447M [00:23<00:09, 15.8MB/s]\u001b[A\n",
            "Downloading data:  66% 294M/447M [00:23<00:09, 16.0MB/s]\u001b[A\n",
            "Downloading data:  66% 298M/447M [00:23<00:09, 16.1MB/s]\u001b[A\n",
            "Downloading data:  67% 301M/447M [00:23<00:09, 16.2MB/s]\u001b[A\n",
            "Downloading data:  68% 305M/447M [00:23<00:08, 16.3MB/s]\u001b[A\n",
            "Downloading data:  69% 307M/447M [00:24<00:11, 12.7MB/s]\u001b[A\n",
            "Downloading data:  69% 309M/447M [00:24<00:10, 13.1MB/s]\u001b[A\n",
            "Downloading data:  69% 311M/447M [00:24<00:12, 10.9MB/s]\u001b[A\n",
            "Downloading data:  70% 313M/447M [00:24<00:12, 10.7MB/s]\u001b[A\n",
            "Downloading data:  71% 316M/447M [00:25<00:12, 10.9MB/s]\u001b[A\n",
            "Downloading data:  71% 318M/447M [00:25<00:11, 11.1MB/s]\u001b[A\n",
            "Downloading data:  72% 321M/447M [00:25<00:11, 11.3MB/s]\u001b[A\n",
            "Downloading data:  72% 324M/447M [00:25<00:10, 11.5MB/s]\u001b[A\n",
            "Downloading data:  73% 326M/447M [00:25<00:10, 11.7MB/s]\u001b[A\n",
            "Downloading data:  74% 329M/447M [00:26<00:09, 11.9MB/s]\u001b[A\n",
            "Downloading data:  74% 332M/447M [00:26<00:09, 12.0MB/s]\u001b[A\n",
            "Downloading data:  75% 335M/447M [00:26<00:09, 12.0MB/s]\u001b[A\n",
            "Downloading data:  75% 338M/447M [00:26<00:08, 12.3MB/s]\u001b[A\n",
            "Downloading data:  76% 341M/447M [00:27<00:08, 12.4MB/s]\u001b[A\n",
            "Downloading data:  77% 343M/447M [00:27<00:08, 12.4MB/s]\u001b[A\n",
            "Downloading data:  77% 345M/447M [00:27<00:10, 10.2MB/s]\u001b[A\n",
            "Downloading data:  78% 347M/447M [00:27<00:09, 10.5MB/s]\u001b[A\n",
            "Downloading data:  78% 348M/447M [00:27<00:09, 10.2MB/s]\u001b[A\n",
            "Downloading data:  78% 349M/447M [00:28<00:09, 9.96MB/s]\u001b[A\n",
            "Downloading data:  78% 350M/447M [00:28<00:10, 9.58MB/s]\u001b[A\n",
            "Downloading data:  78% 351M/447M [00:28<00:10, 9.35MB/s]\u001b[A\n",
            "Downloading data:  79% 352M/447M [00:28<00:10, 8.97MB/s]\u001b[A\n",
            "Downloading data:  79% 353M/447M [00:28<00:10, 9.38MB/s]\u001b[A\n",
            "Downloading data:  79% 354M/447M [00:28<00:10, 9.00MB/s]\u001b[A\n",
            "Downloading data:  79% 355M/447M [00:28<00:09, 9.59MB/s]\u001b[A\n",
            "Downloading data:  80% 356M/447M [00:28<00:09, 9.17MB/s]\u001b[A\n",
            "Downloading data:  80% 358M/447M [00:28<00:09, 9.75MB/s]\u001b[A\n",
            "Downloading data:  80% 359M/447M [00:29<00:09, 9.30MB/s]\u001b[A\n",
            "Downloading data:  80% 360M/447M [00:29<00:08, 9.85MB/s]\u001b[A\n",
            "Downloading data:  81% 361M/447M [00:29<00:09, 9.41MB/s]\u001b[A\n",
            "Downloading data:  81% 362M/447M [00:29<00:08, 9.97MB/s]\u001b[A\n",
            "Downloading data:  81% 363M/447M [00:29<00:08, 9.50MB/s]\u001b[A\n",
            "Downloading data:  81% 364M/447M [00:29<00:08, 10.0MB/s]\u001b[A\n",
            "Downloading data:  82% 365M/447M [00:29<00:08, 9.65MB/s]\u001b[A\n",
            "Downloading data:  82% 367M/447M [00:29<00:07, 10.2MB/s]\u001b[A\n",
            "Downloading data:  82% 368M/447M [00:29<00:08, 9.73MB/s]\u001b[A\n",
            "Downloading data:  82% 369M/447M [00:30<00:07, 10.2MB/s]\u001b[A\n",
            "Downloading data:  83% 370M/447M [00:30<00:08, 9.61MB/s]\u001b[A\n",
            "Downloading data:  83% 371M/447M [00:30<00:07, 10.4MB/s]\u001b[A\n",
            "Downloading data:  83% 372M/447M [00:30<00:07, 9.94MB/s]\u001b[A\n",
            "Downloading data:  83% 373M/447M [00:30<00:07, 10.5MB/s]\u001b[A\n",
            "Downloading data:  84% 375M/447M [00:30<00:07, 10.0MB/s]\u001b[A\n",
            "Downloading data:  84% 376M/447M [00:30<00:06, 10.5MB/s]\u001b[A\n",
            "Downloading data:  84% 377M/447M [00:30<00:06, 10.1MB/s]\u001b[A\n",
            "Downloading data:  85% 378M/447M [00:31<00:06, 10.6MB/s]\u001b[A\n",
            "Downloading data:  85% 379M/447M [00:31<00:06, 10.0MB/s]\u001b[A\n",
            "Downloading data:  85% 381M/447M [00:31<00:06, 10.7MB/s]\u001b[A\n",
            "Downloading data:  85% 382M/447M [00:31<00:06, 10.2MB/s]\u001b[A\n",
            "Downloading data:  86% 383M/447M [00:31<00:05, 10.8MB/s]\u001b[A\n",
            "Downloading data:  86% 384M/447M [00:31<00:06, 10.2MB/s]\u001b[A\n",
            "Downloading data:  86% 385M/447M [00:31<00:05, 10.9MB/s]\u001b[A\n",
            "Downloading data:  86% 387M/447M [00:31<00:05, 10.3MB/s]\u001b[A\n",
            "Downloading data:  87% 388M/447M [00:31<00:05, 10.9MB/s]\u001b[A\n",
            "Downloading data:  87% 389M/447M [00:32<00:05, 10.3MB/s]\u001b[A\n",
            "Downloading data:  87% 390M/447M [00:32<00:05, 11.0MB/s]\u001b[A\n",
            "Downloading data:  87% 391M/447M [00:32<00:05, 10.3MB/s]\u001b[A\n",
            "Downloading data:  88% 393M/447M [00:32<00:04, 11.1MB/s]\u001b[A\n",
            "Downloading data:  88% 394M/447M [00:32<00:05, 10.4MB/s]\u001b[A\n",
            "Downloading data:  88% 395M/447M [00:32<00:04, 11.1MB/s]\u001b[A\n",
            "Downloading data:  89% 396M/447M [00:32<00:04, 10.5MB/s]\u001b[A\n",
            "Downloading data:  89% 398M/447M [00:32<00:04, 11.1MB/s]\u001b[A\n",
            "Downloading data:  89% 399M/447M [00:32<00:04, 10.6MB/s]\u001b[A\n",
            "Downloading data:  89% 400M/447M [00:33<00:04, 11.1MB/s]\u001b[A\n",
            "Downloading data:  90% 401M/447M [00:33<00:04, 10.6MB/s]\u001b[A\n",
            "Downloading data:  90% 403M/447M [00:33<00:04, 11.2MB/s]\u001b[A\n",
            "Downloading data:  90% 404M/447M [00:33<00:04, 10.6MB/s]\u001b[A\n",
            "Downloading data:  91% 405M/447M [00:33<00:03, 11.2MB/s]\u001b[A\n",
            "Downloading data:  91% 406M/447M [00:33<00:03, 10.7MB/s]\u001b[A\n",
            "Downloading data:  91% 408M/447M [00:33<00:03, 11.0MB/s]\u001b[A\n",
            "Downloading data:  91% 409M/447M [00:33<00:03, 10.6MB/s]\u001b[A\n",
            "Downloading data:  92% 410M/447M [00:33<00:03, 11.0MB/s]\u001b[A\n",
            "Downloading data:  92% 411M/447M [00:34<00:03, 10.7MB/s]\u001b[A\n",
            "Downloading data:  92% 413M/447M [00:34<00:03, 11.4MB/s]\u001b[A\n",
            "Downloading data:  92% 414M/447M [00:34<00:03, 10.8MB/s]\u001b[A\n",
            "Downloading data:  93% 415M/447M [00:34<00:02, 11.1MB/s]\u001b[A\n",
            "Downloading data:  93% 416M/447M [00:34<00:02, 10.7MB/s]\u001b[A\n",
            "Downloading data:  93% 418M/447M [00:34<00:02, 11.4MB/s]\u001b[A\n",
            "Downloading data:  94% 419M/447M [00:34<00:02, 10.7MB/s]\u001b[A\n",
            "Downloading data:  94% 420M/447M [00:34<00:02, 11.4MB/s]\u001b[A\n",
            "Downloading data:  94% 421M/447M [00:34<00:02, 10.8MB/s]\u001b[A\n",
            "Downloading data:  94% 423M/447M [00:35<00:02, 11.3MB/s]\u001b[A\n",
            "Downloading data:  95% 424M/447M [00:35<00:02, 10.7MB/s]\u001b[A\n",
            "Downloading data:  95% 425M/447M [00:35<00:01, 11.3MB/s]\u001b[A\n",
            "Downloading data:  95% 426M/447M [00:35<00:01, 10.7MB/s]\u001b[A\n",
            "Downloading data:  96% 428M/447M [00:35<00:01, 11.4MB/s]\u001b[A\n",
            "Downloading data:  96% 429M/447M [00:35<00:01, 10.8MB/s]\u001b[A\n",
            "Downloading data:  96% 430M/447M [00:35<00:01, 11.3MB/s]\u001b[A\n",
            "Downloading data:  96% 431M/447M [00:35<00:01, 10.4MB/s]\u001b[A\n",
            "Downloading data:  97% 433M/447M [00:36<00:01, 12.2MB/s]\u001b[A\n",
            "Downloading data:  97% 434M/447M [00:36<00:01, 11.6MB/s]\u001b[A\n",
            "Downloading data:  97% 435M/447M [00:36<00:01, 11.5MB/s]\u001b[A\n",
            "Downloading data:  98% 437M/447M [00:36<00:00, 10.9MB/s]\u001b[A\n",
            "Downloading data:  98% 438M/447M [00:36<00:00, 10.9MB/s]\u001b[A\n",
            "Downloading data:  98% 439M/447M [00:36<00:00, 10.3MB/s]\u001b[A\n",
            "Downloading data:  98% 440M/447M [00:36<00:00, 11.2MB/s]\u001b[A\n",
            "Downloading data:  99% 441M/447M [00:36<00:00, 10.5MB/s]\u001b[A\n",
            "Downloading data:  99% 443M/447M [00:36<00:00, 11.2MB/s]\u001b[A\n",
            "Downloading data:  99% 444M/447M [00:37<00:00, 10.8MB/s]\u001b[A\n",
            "Downloading data: 100% 445M/447M [00:37<00:00, 11.3MB/s]\u001b[A\n",
            "Downloading data: 100% 447M/447M [00:37<00:00, 12.0MB/s]\n",
            "\n",
            "Downloading data:   0% 0.00/430M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   0% 8.19k/430M [00:00<3:16:34, 36.5kB/s]\u001b[A\n",
            "Downloading data:   0% 60.4k/430M [00:00<46:46, 153kB/s]   \u001b[A\n",
            "Downloading data:   0% 146k/430M [00:00<27:38, 259kB/s] \u001b[A\n",
            "Downloading data:   0% 304k/430M [00:00<16:28, 435kB/s]\u001b[A\n",
            "Downloading data:   0% 652k/430M [00:01<08:32, 838kB/s]\u001b[A\n",
            "Downloading data:   0% 1.33M/430M [00:01<04:30, 1.58MB/s]\u001b[A\n",
            "Downloading data:   1% 2.72M/430M [00:01<02:18, 3.08MB/s]\u001b[A\n",
            "Downloading data:   1% 5.40M/430M [00:01<01:11, 5.92MB/s]\u001b[A\n",
            "Downloading data:   2% 8.35M/430M [00:02<00:51, 8.18MB/s]\u001b[A\n",
            "Downloading data:   3% 11.5M/430M [00:02<00:42, 9.96MB/s]\u001b[A\n",
            "Downloading data:   3% 14.7M/430M [00:02<00:36, 11.4MB/s]\u001b[A\n",
            "Downloading data:   4% 18.1M/430M [00:02<00:33, 12.5MB/s]\u001b[A\n",
            "Downloading data:   5% 21.5M/430M [00:02<00:30, 13.3MB/s]\u001b[A\n",
            "Downloading data:   6% 25.1M/430M [00:03<00:28, 14.1MB/s]\u001b[A\n",
            "Downloading data:   7% 28.7M/430M [00:03<00:27, 14.8MB/s]\u001b[A\n",
            "Downloading data:   8% 32.7M/430M [00:03<00:25, 15.7MB/s]\u001b[A\n",
            "Downloading data:   9% 36.9M/430M [00:03<00:23, 16.6MB/s]\u001b[A\n",
            "Downloading data:  10% 41.3M/430M [00:04<00:22, 17.4MB/s]\u001b[A\n",
            "Downloading data:  11% 45.8M/430M [00:04<00:20, 18.3MB/s]\u001b[A\n",
            "Downloading data:  12% 50.7M/430M [00:04<00:19, 19.3MB/s]\u001b[A\n",
            "Downloading data:  13% 55.8M/430M [00:04<00:18, 20.4MB/s]\u001b[A\n",
            "Downloading data:  14% 61.1M/430M [00:04<00:17, 21.3MB/s]\u001b[A\n",
            "Downloading data:  15% 66.3M/430M [00:05<00:16, 21.9MB/s]\u001b[A\n",
            "Downloading data:  16% 69.7M/430M [00:05<00:18, 19.8MB/s]\u001b[A\n",
            "Downloading data:  17% 74.8M/430M [00:05<00:17, 20.8MB/s]\u001b[A\n",
            "Downloading data:  18% 78.8M/430M [00:05<00:17, 19.9MB/s]\u001b[A\n",
            "Downloading data:  20% 84.0M/430M [00:06<00:16, 20.9MB/s]\u001b[A\n",
            "Downloading data:  21% 89.1M/430M [00:06<00:16, 21.2MB/s]\u001b[A\n",
            "Downloading data:  22% 94.3M/430M [00:06<00:15, 21.7MB/s]\u001b[A\n",
            "Downloading data:  23% 99.5M/430M [00:06<00:14, 22.1MB/s]\u001b[A\n",
            "Downloading data:  24% 105M/430M [00:06<00:14, 22.3MB/s] \u001b[A\n",
            "Downloading data:  26% 110M/430M [00:07<00:14, 22.4MB/s]\u001b[A\n",
            "Downloading data:  27% 115M/430M [00:07<00:14, 22.5MB/s]\u001b[A\n",
            "Downloading data:  28% 120M/430M [00:07<00:13, 22.7MB/s]\u001b[A\n",
            "Downloading data:  29% 126M/430M [00:07<00:13, 22.9MB/s]\u001b[A\n",
            "Downloading data:  30% 131M/430M [00:08<00:12, 23.1MB/s]\u001b[A\n",
            "Downloading data:  32% 136M/430M [00:08<00:12, 23.1MB/s]\u001b[A\n",
            "Downloading data:  33% 141M/430M [00:08<00:12, 23.1MB/s]\u001b[A\n",
            "Downloading data:  34% 146M/430M [00:08<00:12, 23.2MB/s]\u001b[A\n",
            "Downloading data:  35% 151M/430M [00:08<00:10, 26.2MB/s]\u001b[A\n",
            "Downloading data:  36% 153M/430M [00:09<00:15, 17.8MB/s]\u001b[A\n",
            "Downloading data:  36% 156M/430M [00:09<00:17, 15.7MB/s]\u001b[A\n",
            "Downloading data:  37% 158M/430M [00:09<00:18, 14.7MB/s]\u001b[A\n",
            "Downloading data:  38% 161M/430M [00:09<00:18, 14.3MB/s]\u001b[A\n",
            "Downloading data:  38% 164M/430M [00:10<00:18, 14.3MB/s]\u001b[A\n",
            "Downloading data:  39% 168M/430M [00:10<00:18, 14.3MB/s]\u001b[A\n",
            "Downloading data:  40% 171M/430M [00:10<00:17, 14.5MB/s]\u001b[A\n",
            "Downloading data:  41% 175M/430M [00:10<00:17, 15.0MB/s]\u001b[A\n",
            "Downloading data:  42% 178M/430M [00:11<00:16, 15.6MB/s]\u001b[A\n",
            "Downloading data:  42% 182M/430M [00:11<00:15, 16.1MB/s]\u001b[A\n",
            "Downloading data:  43% 186M/430M [00:11<00:14, 16.8MB/s]\u001b[A\n",
            "Downloading data:  44% 191M/430M [00:11<00:13, 17.6MB/s]\u001b[A\n",
            "Downloading data:  45% 195M/430M [00:11<00:12, 18.5MB/s]\u001b[A\n",
            "Downloading data:  47% 201M/430M [00:12<00:11, 19.7MB/s]\u001b[A\n",
            "Downloading data:  48% 206M/430M [00:12<00:10, 20.8MB/s]\u001b[A\n",
            "Downloading data:  49% 211M/430M [00:12<00:10, 21.6MB/s]\u001b[A\n",
            "Downloading data:  50% 216M/430M [00:12<00:09, 22.1MB/s]\u001b[A\n",
            "Downloading data:  51% 221M/430M [00:13<00:09, 22.4MB/s]\u001b[A\n",
            "Downloading data:  53% 226M/430M [00:13<00:07, 25.9MB/s]\u001b[A\n",
            "Downloading data:  53% 229M/430M [00:13<00:09, 22.2MB/s]\u001b[A\n",
            "Downloading data:  54% 232M/430M [00:13<00:09, 21.9MB/s]\u001b[A\n",
            "Downloading data:  55% 236M/430M [00:13<00:07, 26.1MB/s]\u001b[A\n",
            "Downloading data:  56% 239M/430M [00:13<00:08, 22.1MB/s]\u001b[A\n",
            "Downloading data:  56% 242M/430M [00:13<00:08, 21.7MB/s]\u001b[A\n",
            "Downloading data:  57% 245M/430M [00:14<00:07, 23.8MB/s]\u001b[A\n",
            "Downloading data:  58% 248M/430M [00:14<00:08, 22.3MB/s]\u001b[A\n",
            "Downloading data:  59% 252M/430M [00:14<00:06, 25.9MB/s]\u001b[A\n",
            "Downloading data:  59% 255M/430M [00:14<00:08, 21.3MB/s]\u001b[A\n",
            "Downloading data:  60% 258M/430M [00:14<00:07, 22.2MB/s]\u001b[A\n",
            "Downloading data:  61% 260M/430M [00:14<00:07, 22.4MB/s]\u001b[A\n",
            "Downloading data:  61% 263M/430M [00:14<00:07, 22.7MB/s]\u001b[A\n",
            "Downloading data:  62% 265M/430M [00:14<00:07, 21.7MB/s]\u001b[A\n",
            "Downloading data:  62% 268M/430M [00:15<00:08, 18.8MB/s]\u001b[A\n",
            "Downloading data:  64% 273M/430M [00:15<00:08, 18.5MB/s]\u001b[A\n",
            "Downloading data:  65% 279M/430M [00:15<00:06, 22.2MB/s]\u001b[A\n",
            "Downloading data:  65% 281M/430M [00:15<00:08, 17.2MB/s]\u001b[A\n",
            "Downloading data:  66% 283M/430M [00:16<00:09, 15.0MB/s]\u001b[A\n",
            "Downloading data:  66% 285M/430M [00:16<00:10, 13.6MB/s]\u001b[A\n",
            "Downloading data:  67% 288M/430M [00:16<00:10, 13.4MB/s]\u001b[A\n",
            "Downloading data:  68% 291M/430M [00:16<00:10, 13.4MB/s]\u001b[A\n",
            "Downloading data:  68% 294M/430M [00:16<00:10, 13.5MB/s]\u001b[A\n",
            "Downloading data:  69% 297M/430M [00:17<00:09, 13.6MB/s]\u001b[A\n",
            "Downloading data:  70% 301M/430M [00:17<00:09, 13.9MB/s]\u001b[A\n",
            "Downloading data:  71% 304M/430M [00:17<00:08, 14.2MB/s]\u001b[A\n",
            "Downloading data:  71% 307M/430M [00:17<00:08, 14.5MB/s]\u001b[A\n",
            "Downloading data:  72% 311M/430M [00:18<00:08, 14.9MB/s]\u001b[A\n",
            "Downloading data:  73% 314M/430M [00:18<00:07, 15.2MB/s]\u001b[A\n",
            "Downloading data:  74% 317M/430M [00:18<00:08, 13.0MB/s]\u001b[A\n",
            "Downloading data:  74% 320M/430M [00:18<00:07, 14.0MB/s]\u001b[A\n",
            "Downloading data:  75% 322M/430M [00:19<00:12, 9.01MB/s]\u001b[A\n",
            "Downloading data:  75% 323M/430M [00:19<00:13, 8.15MB/s]\u001b[A\n",
            "Downloading data:  76% 325M/430M [00:19<00:11, 9.11MB/s]\u001b[A\n",
            "Downloading data:  76% 328M/430M [00:19<00:10, 9.90MB/s]\u001b[A\n",
            "Downloading data:  77% 331M/430M [00:20<00:09, 10.7MB/s]\u001b[A\n",
            "Downloading data:  78% 334M/430M [00:20<00:08, 11.4MB/s]\u001b[A\n",
            "Downloading data:  78% 337M/430M [00:20<00:07, 11.9MB/s]\u001b[A\n",
            "Downloading data:  79% 340M/430M [00:20<00:07, 12.5MB/s]\u001b[A\n",
            "Downloading data:  80% 343M/430M [00:21<00:06, 13.0MB/s]\u001b[A\n",
            "Downloading data:  81% 347M/430M [00:21<00:06, 13.7MB/s]\u001b[A\n",
            "Downloading data:  81% 350M/430M [00:21<00:05, 14.4MB/s]\u001b[A\n",
            "Downloading data:  82% 354M/430M [00:21<00:05, 15.0MB/s]\u001b[A\n",
            "Downloading data:  83% 358M/430M [00:21<00:04, 15.7MB/s]\u001b[A\n",
            "Downloading data:  84% 362M/430M [00:22<00:04, 16.6MB/s]\u001b[A\n",
            "Downloading data:  85% 366M/430M [00:22<00:03, 17.3MB/s]\u001b[A\n",
            "Downloading data:  86% 371M/430M [00:22<00:03, 18.0MB/s]\u001b[A\n",
            "Downloading data:  87% 375M/430M [00:22<00:02, 18.7MB/s]\u001b[A\n",
            "Downloading data:  88% 380M/430M [00:23<00:02, 19.5MB/s]\u001b[A\n",
            "Downloading data:  90% 385M/430M [00:23<00:02, 20.2MB/s]\u001b[A\n",
            "Downloading data:  91% 390M/430M [00:23<00:01, 20.9MB/s]\u001b[A\n",
            "Downloading data:  92% 394M/430M [00:23<00:01, 23.6MB/s]\u001b[A\n",
            "Downloading data:  92% 396M/430M [00:23<00:01, 22.3MB/s]\u001b[A\n",
            "Downloading data:  93% 400M/430M [00:23<00:01, 21.4MB/s]\u001b[A\n",
            "Downloading data:  94% 406M/430M [00:24<00:01, 22.1MB/s]\u001b[A\n",
            "Downloading data:  96% 411M/430M [00:24<00:00, 22.4MB/s]\u001b[A\n",
            "Downloading data:  97% 416M/430M [00:24<00:00, 22.2MB/s]\u001b[A\n",
            "Downloading data:  98% 421M/430M [00:24<00:00, 22.6MB/s]\u001b[A\n",
            "Downloading data: 100% 430M/430M [00:25<00:00, 17.1MB/s]\n",
            "\n",
            "Downloading data:   0% 0.00/426M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   0% 16.4k/426M [00:00<1:36:27, 73.6kB/s]\u001b[A\n",
            "Downloading data:   0% 51.2k/426M [00:00<58:38, 121kB/s]   \u001b[A\n",
            "Downloading data:   0% 138k/426M [00:00<29:25, 241kB/s] \u001b[A\n",
            "Downloading data:   0% 295k/426M [00:00<16:55, 419kB/s]\u001b[A\n",
            "Downloading data:   0% 608k/426M [00:01<09:15, 767kB/s]\u001b[A\n",
            "Downloading data:   0% 1.25M/426M [00:01<04:48, 1.47MB/s]\u001b[A\n",
            "Downloading data:   1% 2.52M/426M [00:01<02:29, 2.83MB/s]\u001b[A\n",
            "Downloading data:   1% 5.10M/426M [00:01<01:16, 5.52MB/s]\u001b[A\n",
            "Downloading data:   2% 7.82M/426M [00:01<00:48, 8.69MB/s]\u001b[A\n",
            "Downloading data:   2% 8.85M/426M [00:02<00:47, 8.87MB/s]\u001b[A\n",
            "Downloading data:   2% 10.6M/426M [00:02<00:40, 10.2MB/s]\u001b[A\n",
            "Downloading data:   3% 11.8M/426M [00:02<00:40, 10.4MB/s]\u001b[A\n",
            "Downloading data:   3% 13.5M/426M [00:02<00:37, 11.1MB/s]\u001b[A\n",
            "Downloading data:   4% 15.1M/426M [00:02<00:33, 12.4MB/s]\u001b[A\n",
            "Downloading data:   4% 16.4M/426M [00:02<00:41, 9.88MB/s]\u001b[A\n",
            "Downloading data:   5% 19.3M/426M [00:02<00:30, 13.1MB/s]\u001b[A\n",
            "Downloading data:   5% 20.8M/426M [00:02<00:31, 13.1MB/s]\u001b[A\n",
            "Downloading data:   5% 22.4M/426M [00:03<00:30, 13.3MB/s]\u001b[A\n",
            "Downloading data:   6% 23.8M/426M [00:03<00:30, 13.1MB/s]\u001b[A\n",
            "Downloading data:   6% 25.5M/426M [00:03<00:29, 13.6MB/s]\u001b[A\n",
            "Downloading data:   6% 26.9M/426M [00:03<00:30, 13.1MB/s]\u001b[A\n",
            "Downloading data:   7% 28.7M/426M [00:03<00:29, 13.4MB/s]\u001b[A\n",
            "Downloading data:   7% 30.6M/426M [00:03<00:26, 14.9MB/s]\u001b[A\n",
            "Downloading data:   8% 32.1M/426M [00:03<00:33, 11.7MB/s]\u001b[A\n",
            "Downloading data:   8% 35.1M/426M [00:03<00:24, 15.7MB/s]\u001b[A\n",
            "Downloading data:   9% 36.8M/426M [00:04<00:26, 14.6MB/s]\u001b[A\n",
            "Downloading data:   9% 38.4M/426M [00:04<00:30, 12.6MB/s]\u001b[A\n",
            "Downloading data:  10% 41.6M/426M [00:04<00:24, 16.0MB/s]\u001b[A\n",
            "Downloading data:  10% 43.3M/426M [00:04<00:26, 14.5MB/s]\u001b[A\n",
            "Downloading data:  11% 45.0M/426M [00:04<00:25, 14.9MB/s]\u001b[A\n",
            "Downloading data:  11% 46.6M/426M [00:04<00:26, 14.3MB/s]\u001b[A\n",
            "Downloading data:  11% 48.4M/426M [00:04<00:24, 15.2MB/s]\u001b[A\n",
            "Downloading data:  12% 50.0M/426M [00:05<00:25, 14.5MB/s]\u001b[A\n",
            "Downloading data:  12% 52.0M/426M [00:05<00:23, 15.7MB/s]\u001b[A\n",
            "Downloading data:  13% 53.6M/426M [00:05<00:25, 14.9MB/s]\u001b[A\n",
            "Downloading data:  13% 55.6M/426M [00:05<00:23, 16.1MB/s]\u001b[A\n",
            "Downloading data:  13% 57.2M/426M [00:05<00:24, 15.1MB/s]\u001b[A\n",
            "Downloading data:  14% 59.3M/426M [00:05<00:22, 16.4MB/s]\u001b[A\n",
            "Downloading data:  14% 60.9M/426M [00:05<00:36, 9.99MB/s]\u001b[A\n",
            "Downloading data:  15% 64.6M/426M [00:06<00:29, 12.1MB/s]\u001b[A\n",
            "Downloading data:  16% 67.4M/426M [00:06<00:29, 12.2MB/s]\u001b[A\n",
            "Downloading data:  17% 70.6M/426M [00:06<00:27, 12.9MB/s]\u001b[A\n",
            "Downloading data:  17% 74.4M/426M [00:06<00:24, 14.1MB/s]\u001b[A\n",
            "Downloading data:  18% 78.3M/426M [00:07<00:23, 15.0MB/s]\u001b[A\n",
            "Downloading data:  19% 82.1M/426M [00:07<00:22, 15.6MB/s]\u001b[A\n",
            "Downloading data:  20% 85.4M/426M [00:07<00:22, 15.3MB/s]\u001b[A\n",
            "Downloading data:  21% 89.4M/426M [00:07<00:21, 16.0MB/s]\u001b[A\n",
            "Downloading data:  22% 93.5M/426M [00:07<00:20, 16.6MB/s]\u001b[A\n",
            "Downloading data:  23% 97.6M/426M [00:08<00:19, 17.1MB/s]\u001b[A\n",
            "Downloading data:  24% 102M/426M [00:08<00:18, 17.7MB/s] \u001b[A\n",
            "Downloading data:  25% 107M/426M [00:08<00:17, 18.3MB/s]\u001b[A\n",
            "Downloading data:  26% 111M/426M [00:08<00:16, 19.1MB/s]\u001b[A\n",
            "Downloading data:  27% 116M/426M [00:09<00:15, 19.5MB/s]\u001b[A\n",
            "Downloading data:  28% 120M/426M [00:09<00:12, 23.7MB/s]\u001b[A\n",
            "Downloading data:  29% 123M/426M [00:09<00:13, 21.9MB/s]\u001b[A\n",
            "Downloading data:  30% 126M/426M [00:09<00:15, 19.3MB/s]\u001b[A\n",
            "Downloading data:  31% 131M/426M [00:09<00:14, 20.3MB/s]\u001b[A\n",
            "Downloading data:  32% 136M/426M [00:09<00:13, 20.9MB/s]\u001b[A\n",
            "Downloading data:  33% 141M/426M [00:10<00:13, 21.5MB/s]\u001b[A\n",
            "Downloading data:  34% 146M/426M [00:10<00:12, 21.9MB/s]\u001b[A\n",
            "Downloading data:  36% 151M/426M [00:10<00:12, 22.1MB/s]\u001b[A\n",
            "Downloading data:  37% 156M/426M [00:10<00:10, 25.6MB/s]\u001b[A\n",
            "Downloading data:  37% 159M/426M [00:10<00:11, 24.0MB/s]\u001b[A\n",
            "Downloading data:  38% 161M/426M [00:11<00:13, 19.7MB/s]\u001b[A\n",
            "Downloading data:  39% 165M/426M [00:11<00:13, 18.9MB/s]\u001b[A\n",
            "Downloading data:  40% 170M/426M [00:11<00:10, 24.3MB/s]\u001b[A\n",
            "Downloading data:  41% 173M/426M [00:11<00:11, 22.3MB/s]\u001b[A\n",
            "Downloading data:  41% 175M/426M [00:11<00:12, 19.5MB/s]\u001b[A\n",
            "Downloading data:  42% 180M/426M [00:11<00:09, 25.7MB/s]\u001b[A\n",
            "Downloading data:  43% 183M/426M [00:12<00:10, 23.3MB/s]\u001b[A\n",
            "Downloading data:  44% 186M/426M [00:12<00:12, 19.9MB/s]\u001b[A\n",
            "Downloading data:  45% 190M/426M [00:12<00:11, 19.7MB/s]\u001b[A\n",
            "Downloading data:  46% 195M/426M [00:12<00:12, 18.6MB/s]\u001b[A\n",
            "Downloading data:  47% 200M/426M [00:13<00:11, 20.0MB/s]\u001b[A\n",
            "Downloading data:  48% 202M/426M [00:13<00:13, 16.3MB/s]\u001b[A\n",
            "Downloading data:  48% 204M/426M [00:13<00:15, 13.9MB/s]\u001b[A\n",
            "Downloading data:  49% 207M/426M [00:13<00:16, 13.3MB/s]\u001b[A\n",
            "Downloading data:  49% 210M/426M [00:13<00:16, 13.0MB/s]\u001b[A\n",
            "Downloading data:  50% 213M/426M [00:14<00:16, 12.9MB/s]\u001b[A\n",
            "Downloading data:  51% 216M/426M [00:14<00:15, 13.2MB/s]\u001b[A\n",
            "Downloading data:  51% 219M/426M [00:14<00:15, 13.4MB/s]\u001b[A\n",
            "Downloading data:  52% 222M/426M [00:14<00:14, 13.8MB/s]\u001b[A\n",
            "Downloading data:  53% 225M/426M [00:15<00:14, 14.0MB/s]\u001b[A\n",
            "Downloading data:  54% 229M/426M [00:15<00:13, 14.4MB/s]\u001b[A\n",
            "Downloading data:  55% 232M/426M [00:15<00:13, 14.8MB/s]\u001b[A\n",
            "Downloading data:  55% 235M/426M [00:15<00:13, 14.1MB/s]\u001b[A\n",
            "Downloading data:  56% 239M/426M [00:15<00:12, 14.7MB/s]\u001b[A\n",
            "Downloading data:  57% 243M/426M [00:16<00:12, 15.2MB/s]\u001b[A\n",
            "Downloading data:  58% 246M/426M [00:16<00:11, 15.6MB/s]\u001b[A\n",
            "Downloading data:  59% 250M/426M [00:16<00:09, 18.9MB/s]\u001b[A\n",
            "Downloading data:  59% 252M/426M [00:16<00:09, 17.7MB/s]\u001b[A\n",
            "Downloading data:  60% 254M/426M [00:16<00:11, 15.1MB/s]\u001b[A\n",
            "Downloading data:  61% 258M/426M [00:16<00:08, 18.7MB/s]\u001b[A\n",
            "Downloading data:  61% 260M/426M [00:17<00:09, 17.6MB/s]\u001b[A\n",
            "Downloading data:  61% 262M/426M [00:17<00:11, 14.1MB/s]\u001b[A\n",
            "Downloading data:  62% 266M/426M [00:17<00:10, 15.6MB/s]\u001b[A\n",
            "Downloading data:  63% 267M/426M [00:17<00:12, 12.7MB/s]\u001b[A\n",
            "Downloading data:  63% 270M/426M [00:17<00:10, 14.9MB/s]\u001b[A\n",
            "Downloading data:  64% 271M/426M [00:18<00:11, 13.7MB/s]\u001b[A\n",
            "Downloading data:  64% 273M/426M [00:18<00:13, 11.7MB/s]\u001b[A\n",
            "Downloading data:  65% 275M/426M [00:18<00:11, 13.1MB/s]\u001b[A\n",
            "Downloading data:  65% 277M/426M [00:18<00:11, 13.1MB/s]\u001b[A\n",
            "Downloading data:  65% 278M/426M [00:18<00:11, 12.9MB/s]\u001b[A\n",
            "Downloading data:  66% 280M/426M [00:18<00:11, 13.0MB/s]\u001b[A\n",
            "Downloading data:  66% 281M/426M [00:18<00:12, 11.9MB/s]\u001b[A\n",
            "Downloading data:  67% 284M/426M [00:19<00:09, 14.7MB/s]\u001b[A\n",
            "Downloading data:  67% 286M/426M [00:19<00:10, 13.1MB/s]\u001b[A\n",
            "Downloading data:  67% 287M/426M [00:19<00:10, 12.9MB/s]\u001b[A\n",
            "Downloading data:  68% 289M/426M [00:19<00:10, 13.1MB/s]\u001b[A\n",
            "Downloading data:  68% 290M/426M [00:19<00:10, 13.3MB/s]\u001b[A\n",
            "Downloading data:  68% 292M/426M [00:19<00:10, 13.1MB/s]\u001b[A\n",
            "Downloading data:  69% 293M/426M [00:19<00:10, 13.2MB/s]\u001b[A\n",
            "Downloading data:  69% 295M/426M [00:19<00:09, 13.6MB/s]\u001b[A\n",
            "Downloading data:  70% 297M/426M [00:19<00:09, 13.4MB/s]\u001b[A\n",
            "Downloading data:  70% 298M/426M [00:20<00:09, 13.4MB/s]\u001b[A\n",
            "Downloading data:  70% 300M/426M [00:20<00:09, 13.5MB/s]\u001b[A\n",
            "Downloading data:  71% 301M/426M [00:20<00:09, 13.8MB/s]\u001b[A\n",
            "Downloading data:  71% 303M/426M [00:20<00:09, 13.6MB/s]\u001b[A\n",
            "Downloading data:  71% 304M/426M [00:20<00:08, 14.0MB/s]\u001b[A\n",
            "Downloading data:  72% 306M/426M [00:20<00:08, 14.4MB/s]\u001b[A\n",
            "Downloading data:  72% 307M/426M [00:20<00:08, 13.4MB/s]\u001b[A\n",
            "Downloading data:  73% 309M/426M [00:20<00:08, 14.1MB/s]\u001b[A\n",
            "Downloading data:  73% 311M/426M [00:20<00:08, 13.6MB/s]\u001b[A\n",
            "Downloading data:  73% 312M/426M [00:21<00:08, 13.4MB/s]\u001b[A\n",
            "Downloading data:  74% 315M/426M [00:21<00:07, 15.7MB/s]\u001b[A\n",
            "Downloading data:  74% 316M/426M [00:21<00:08, 12.6MB/s]\u001b[A\n",
            "Downloading data:  75% 319M/426M [00:21<00:07, 14.5MB/s]\u001b[A\n",
            "Downloading data:  75% 320M/426M [00:21<00:07, 14.1MB/s]\u001b[A\n",
            "Downloading data:  76% 322M/426M [00:21<00:07, 14.4MB/s]\u001b[A\n",
            "Downloading data:  76% 323M/426M [00:21<00:07, 14.1MB/s]\u001b[A\n",
            "Downloading data:  76% 325M/426M [00:22<00:07, 14.2MB/s]\u001b[A\n",
            "Downloading data:  77% 327M/426M [00:22<00:07, 14.1MB/s]\u001b[A\n",
            "Downloading data:  77% 328M/426M [00:22<00:07, 13.6MB/s]\u001b[A\n",
            "Downloading data:  78% 331M/426M [00:22<00:05, 16.7MB/s]\u001b[A\n",
            "Downloading data:  78% 333M/426M [00:22<00:07, 13.2MB/s]\u001b[A\n",
            "Downloading data:  79% 335M/426M [00:22<00:06, 14.5MB/s]\u001b[A\n",
            "Downloading data:  79% 337M/426M [00:22<00:06, 14.5MB/s]\u001b[A\n",
            "Downloading data:  79% 338M/426M [00:22<00:05, 14.9MB/s]\u001b[A\n",
            "Downloading data:  80% 340M/426M [00:23<00:06, 14.3MB/s]\u001b[A\n",
            "Downloading data:  80% 342M/426M [00:23<00:05, 14.9MB/s]\u001b[A\n",
            "Downloading data:  81% 343M/426M [00:23<00:05, 14.2MB/s]\u001b[A\n",
            "Downloading data:  81% 345M/426M [00:23<00:05, 14.4MB/s]\u001b[A\n",
            "Downloading data:  81% 346M/426M [00:23<00:05, 14.1MB/s]\u001b[A\n",
            "Downloading data:  82% 348M/426M [00:23<00:05, 14.5MB/s]\u001b[A\n",
            "Downloading data:  82% 350M/426M [00:23<00:05, 14.2MB/s]\u001b[A\n",
            "Downloading data:  82% 351M/426M [00:23<00:05, 14.5MB/s]\u001b[A\n",
            "Downloading data:  83% 353M/426M [00:23<00:05, 14.2MB/s]\u001b[A\n",
            "Downloading data:  83% 355M/426M [00:24<00:04, 15.3MB/s]\u001b[A\n",
            "Downloading data:  84% 356M/426M [00:24<00:04, 14.3MB/s]\u001b[A\n",
            "Downloading data:  84% 358M/426M [00:24<00:04, 13.8MB/s]\u001b[A\n",
            "Downloading data:  85% 361M/426M [00:24<00:03, 16.6MB/s]\u001b[A\n",
            "Downloading data:  85% 362M/426M [00:24<00:04, 13.2MB/s]\u001b[A\n",
            "Downloading data:  86% 365M/426M [00:24<00:03, 15.4MB/s]\u001b[A\n",
            "Downloading data:  86% 367M/426M [00:24<00:04, 14.6MB/s]\u001b[A\n",
            "Downloading data:  86% 368M/426M [00:24<00:03, 14.6MB/s]\u001b[A\n",
            "Downloading data:  87% 370M/426M [00:25<00:03, 14.3MB/s]\u001b[A\n",
            "Downloading data:  87% 372M/426M [00:25<00:03, 14.5MB/s]\u001b[A\n",
            "Downloading data:  88% 373M/426M [00:25<00:03, 14.4MB/s]\u001b[A\n",
            "Downloading data:  88% 375M/426M [00:25<00:03, 15.0MB/s]\u001b[A\n",
            "Downloading data:  88% 376M/426M [00:25<00:03, 14.5MB/s]\u001b[A\n",
            "Downloading data:  89% 378M/426M [00:25<00:03, 15.0MB/s]\u001b[A\n",
            "Downloading data:  89% 380M/426M [00:25<00:03, 14.5MB/s]\u001b[A\n",
            "Downloading data:  90% 382M/426M [00:25<00:02, 15.0MB/s]\u001b[A\n",
            "Downloading data:  90% 383M/426M [00:25<00:02, 14.5MB/s]\u001b[A\n",
            "Downloading data:  90% 385M/426M [00:26<00:02, 14.9MB/s]\u001b[A\n",
            "Downloading data:  91% 386M/426M [00:26<00:02, 14.4MB/s]\u001b[A\n",
            "Downloading data:  91% 388M/426M [00:26<00:02, 15.0MB/s]\u001b[A\n",
            "Downloading data:  91% 390M/426M [00:26<00:02, 14.5MB/s]\u001b[A\n",
            "Downloading data:  92% 392M/426M [00:26<00:02, 15.2MB/s]\u001b[A\n",
            "Downloading data:  92% 393M/426M [00:26<00:02, 14.5MB/s]\u001b[A\n",
            "Downloading data:  93% 395M/426M [00:26<00:02, 15.0MB/s]\u001b[A\n",
            "Downloading data:  93% 396M/426M [00:26<00:02, 14.4MB/s]\u001b[A\n",
            "Downloading data:  93% 398M/426M [00:27<00:01, 14.9MB/s]\u001b[A\n",
            "Downloading data:  94% 400M/426M [00:27<00:01, 14.5MB/s]\u001b[A\n",
            "Downloading data:  94% 402M/426M [00:27<00:01, 14.9MB/s]\u001b[A\n",
            "Downloading data:  95% 403M/426M [00:27<00:01, 14.5MB/s]\u001b[A\n",
            "Downloading data:  95% 405M/426M [00:27<00:01, 15.0MB/s]\u001b[A\n",
            "Downloading data:  95% 406M/426M [00:27<00:01, 14.5MB/s]\u001b[A\n",
            "Downloading data:  96% 408M/426M [00:27<00:01, 14.6MB/s]\u001b[A\n",
            "Downloading data:  96% 410M/426M [00:27<00:01, 14.5MB/s]\u001b[A\n",
            "Downloading data:  97% 412M/426M [00:27<00:00, 15.1MB/s]\u001b[A\n",
            "Downloading data:  97% 413M/426M [00:28<00:00, 14.7MB/s]\u001b[A\n",
            "Downloading data:  97% 415M/426M [00:28<00:00, 14.8MB/s]\u001b[A\n",
            "Downloading data:  98% 417M/426M [00:28<00:00, 14.5MB/s]\u001b[A\n",
            "Downloading data:  98% 418M/426M [00:28<00:00, 15.0MB/s]\u001b[A\n",
            "Downloading data:  99% 420M/426M [00:28<00:00, 14.5MB/s]\u001b[A\n",
            "Downloading data:  99% 422M/426M [00:28<00:00, 15.0MB/s]\u001b[A\n",
            "Downloading data:  99% 423M/426M [00:28<00:00, 14.6MB/s]\u001b[A\n",
            "Downloading data: 100% 426M/426M [00:28<00:00, 14.7MB/s]\n",
            "\n",
            "Downloading data:   0% 0.00/437M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   0% 17.4k/437M [00:00<1:32:27, 78.7kB/s]\u001b[A\n",
            "Downloading data:   0% 52.2k/437M [00:00<58:35, 124kB/s]   \u001b[A\n",
            "Downloading data:   0% 139k/437M [00:00<29:35, 246kB/s] \u001b[A\n",
            "Downloading data:   0% 313k/437M [00:00<15:56, 456kB/s]\u001b[A\n",
            "Downloading data:   0% 644k/437M [00:01<08:48, 826kB/s]\u001b[A\n",
            "Downloading data:   0% 1.34M/437M [00:01<04:31, 1.60MB/s]\u001b[A\n",
            "Downloading data:   1% 2.70M/437M [00:01<02:21, 3.07MB/s]\u001b[A\n",
            "Downloading data:   1% 5.36M/437M [00:01<01:13, 5.88MB/s]\u001b[A\n",
            "Downloading data:   2% 8.16M/437M [00:01<00:48, 8.88MB/s]\u001b[A\n",
            "Downloading data:   2% 9.66M/437M [00:02<00:42, 10.0MB/s]\u001b[A\n",
            "Downloading data:   3% 11.1M/437M [00:02<00:46, 9.12MB/s]\u001b[A\n",
            "Downloading data:   3% 14.3M/437M [00:02<00:38, 10.8MB/s]\u001b[A\n",
            "Downloading data:   4% 17.5M/437M [00:02<00:34, 12.0MB/s]\u001b[A\n",
            "Downloading data:   5% 20.8M/437M [00:02<00:32, 13.0MB/s]\u001b[A\n",
            "Downloading data:   6% 24.4M/437M [00:03<00:29, 13.9MB/s]\u001b[A\n",
            "Downloading data:   6% 28.0M/437M [00:03<00:27, 14.7MB/s]\u001b[A\n",
            "Downloading data:   7% 31.8M/437M [00:03<00:26, 15.4MB/s]\u001b[A\n",
            "Downloading data:   8% 35.8M/437M [00:03<00:24, 16.2MB/s]\u001b[A\n",
            "Downloading data:   9% 40.1M/437M [00:04<00:23, 17.1MB/s]\u001b[A\n",
            "Downloading data:  10% 44.7M/437M [00:04<00:21, 18.0MB/s]\u001b[A\n",
            "Downloading data:  11% 49.4M/437M [00:04<00:20, 19.0MB/s]\u001b[A\n",
            "Downloading data:  12% 54.3M/437M [00:04<00:19, 19.9MB/s]\u001b[A\n",
            "Downloading data:  14% 59.3M/437M [00:04<00:18, 20.7MB/s]\u001b[A\n",
            "Downloading data:  15% 64.6M/437M [00:05<00:17, 21.5MB/s]\u001b[A\n",
            "Downloading data:  16% 69.5M/437M [00:05<00:17, 21.6MB/s]\u001b[A\n",
            "Downloading data:  17% 74.7M/437M [00:05<00:16, 22.1MB/s]\u001b[A\n",
            "Downloading data:  18% 80.0M/437M [00:05<00:13, 25.9MB/s]\u001b[A\n",
            "Downloading data:  19% 82.8M/437M [00:05<00:14, 24.3MB/s]\u001b[A\n",
            "Downloading data:  20% 85.3M/437M [00:06<00:16, 21.2MB/s]\u001b[A\n",
            "Downloading data:  20% 87.5M/437M [00:06<00:21, 16.6MB/s]\u001b[A\n",
            "Downloading data:  21% 90.7M/437M [00:06<00:21, 16.4MB/s]\u001b[A\n",
            "Downloading data:  21% 92.4M/437M [00:06<00:25, 13.6MB/s]\u001b[A\n",
            "Downloading data:  22% 95.0M/437M [00:06<00:26, 13.0MB/s]\u001b[A\n",
            "Downloading data:  22% 97.7M/437M [00:07<00:26, 12.7MB/s]\u001b[A\n",
            "Downloading data:  23% 100M/437M [00:07<00:26, 12.6MB/s] \u001b[A\n",
            "Downloading data:  24% 103M/437M [00:07<00:26, 12.6MB/s]\u001b[A\n",
            "Downloading data:  24% 106M/437M [00:07<00:26, 12.6MB/s]\u001b[A\n",
            "Downloading data:  25% 109M/437M [00:08<00:25, 12.7MB/s]\u001b[A\n",
            "Downloading data:  26% 112M/437M [00:08<00:23, 13.9MB/s]\u001b[A\n",
            "Downloading data:  26% 115M/437M [00:08<00:19, 16.4MB/s]\u001b[A\n",
            "Downloading data:  27% 117M/437M [00:08<00:23, 13.4MB/s]\u001b[A\n",
            "Downloading data:  27% 118M/437M [00:08<00:26, 11.9MB/s]\u001b[A\n",
            "Downloading data:  28% 122M/437M [00:08<00:24, 13.1MB/s]\u001b[A\n",
            "Downloading data:  28% 124M/437M [00:09<00:24, 12.7MB/s]\u001b[A\n",
            "Downloading data:  29% 128M/437M [00:09<00:22, 13.6MB/s]\u001b[A\n",
            "Downloading data:  30% 131M/437M [00:09<00:21, 14.4MB/s]\u001b[A\n",
            "Downloading data:  31% 135M/437M [00:09<00:20, 15.1MB/s]\u001b[A\n",
            "Downloading data:  32% 139M/437M [00:10<00:18, 15.8MB/s]\u001b[A\n",
            "Downloading data:  33% 143M/437M [00:10<00:17, 16.5MB/s]\u001b[A\n",
            "Downloading data:  34% 147M/437M [00:10<00:16, 17.4MB/s]\u001b[A\n",
            "Downloading data:  35% 152M/437M [00:10<00:15, 18.3MB/s]\u001b[A\n",
            "Downloading data:  36% 156M/437M [00:10<00:14, 19.1MB/s]\u001b[A\n",
            "Downloading data:  37% 161M/437M [00:11<00:13, 20.0MB/s]\u001b[A\n",
            "Downloading data:  38% 166M/437M [00:11<00:13, 20.6MB/s]\u001b[A\n",
            "Downloading data:  39% 171M/437M [00:11<00:12, 21.4MB/s]\u001b[A\n",
            "Downloading data:  40% 177M/437M [00:11<00:12, 21.4MB/s]\u001b[A\n",
            "Downloading data:  42% 182M/437M [00:12<00:11, 21.9MB/s]\u001b[A\n",
            "Downloading data:  43% 187M/437M [00:12<00:11, 22.3MB/s]\u001b[A\n",
            "Downloading data:  44% 192M/437M [00:12<00:11, 22.0MB/s]\u001b[A\n",
            "Downloading data:  45% 197M/437M [00:12<00:10, 22.2MB/s]\u001b[A\n",
            "Downloading data:  46% 202M/437M [00:12<00:10, 22.6MB/s]\u001b[A\n",
            "Downloading data:  47% 207M/437M [00:13<00:10, 21.8MB/s]\u001b[A\n",
            "Downloading data:  49% 212M/437M [00:13<00:10, 22.0MB/s]\u001b[A\n",
            "Downloading data:  50% 217M/437M [00:13<00:09, 22.4MB/s]\u001b[A\n",
            "Downloading data:  51% 222M/437M [00:13<00:09, 22.7MB/s]\u001b[A\n",
            "Downloading data:  52% 227M/437M [00:13<00:07, 26.5MB/s]\u001b[A\n",
            "Downloading data:  53% 230M/437M [00:14<00:08, 24.0MB/s]\u001b[A\n",
            "Downloading data:  53% 233M/437M [00:14<00:09, 21.6MB/s]\u001b[A\n",
            "Downloading data:  54% 238M/437M [00:14<00:07, 25.7MB/s]\u001b[A\n",
            "Downloading data:  55% 241M/437M [00:14<00:08, 23.9MB/s]\u001b[A\n",
            "Downloading data:  56% 243M/437M [00:14<00:09, 21.1MB/s]\u001b[A\n",
            "Downloading data:  57% 248M/437M [00:14<00:07, 25.7MB/s]\u001b[A\n",
            "Downloading data:  58% 251M/437M [00:15<00:08, 23.0MB/s]\u001b[A\n",
            "Downloading data:  58% 254M/437M [00:15<00:08, 21.0MB/s]\u001b[A\n",
            "Downloading data:  59% 258M/437M [00:15<00:06, 25.9MB/s]\u001b[A\n",
            "Downloading data:  60% 261M/437M [00:15<00:07, 22.5MB/s]\u001b[A\n",
            "Downloading data:  60% 264M/437M [00:15<00:08, 21.3MB/s]\u001b[A\n",
            "Downloading data:  61% 268M/437M [00:15<00:06, 25.6MB/s]\u001b[A\n",
            "Downloading data:  62% 271M/437M [00:15<00:07, 22.2MB/s]\u001b[A\n",
            "Downloading data:  63% 275M/437M [00:16<00:06, 24.3MB/s]\u001b[A\n",
            "Downloading data:  63% 277M/437M [00:16<00:06, 23.1MB/s]\u001b[A\n",
            "Downloading data:  64% 280M/437M [00:16<00:07, 21.3MB/s]\u001b[A\n",
            "Downloading data:  65% 284M/437M [00:16<00:05, 26.4MB/s]\u001b[A\n",
            "Downloading data:  66% 287M/437M [00:16<00:06, 21.9MB/s]\u001b[A\n",
            "Downloading data:  66% 290M/437M [00:16<00:06, 21.7MB/s]\u001b[A\n",
            "Downloading data:  67% 294M/437M [00:16<00:05, 25.9MB/s]\u001b[A\n",
            "Downloading data:  68% 297M/437M [00:17<00:06, 21.6MB/s]\u001b[A\n",
            "Downloading data:  69% 301M/437M [00:17<00:06, 22.0MB/s]\u001b[A\n",
            "Downloading data:  70% 304M/437M [00:17<00:05, 24.9MB/s]\u001b[A\n",
            "Downloading data:  70% 307M/437M [00:17<00:06, 20.9MB/s]\u001b[A\n",
            "Downloading data:  71% 311M/437M [00:17<00:05, 22.6MB/s]\u001b[A\n",
            "Downloading data:  72% 314M/437M [00:17<00:05, 22.9MB/s]\u001b[A\n",
            "Downloading data:  72% 316M/437M [00:17<00:05, 22.9MB/s]\u001b[A\n",
            "Downloading data:  73% 319M/437M [00:18<00:04, 24.0MB/s]\u001b[A\n",
            "Downloading data:  74% 322M/437M [00:18<00:05, 22.8MB/s]\u001b[A\n",
            "Downloading data:  74% 324M/437M [00:18<00:04, 22.6MB/s]\u001b[A\n",
            "Downloading data:  75% 327M/437M [00:18<00:04, 23.9MB/s]\u001b[A\n",
            "Downloading data:  75% 329M/437M [00:18<00:04, 22.9MB/s]\u001b[A\n",
            "Downloading data:  76% 332M/437M [00:18<00:04, 23.9MB/s]\u001b[A\n",
            "Downloading data:  77% 334M/437M [00:18<00:04, 22.7MB/s]\u001b[A\n",
            "Downloading data:  77% 337M/437M [00:18<00:04, 23.9MB/s]\u001b[A\n",
            "Downloading data:  78% 340M/437M [00:18<00:04, 21.9MB/s]\u001b[A\n",
            "Downloading data:  78% 342M/437M [00:19<00:04, 23.6MB/s]\u001b[A\n",
            "Downloading data:  79% 345M/437M [00:19<00:04, 22.2MB/s]\u001b[A\n",
            "Downloading data:  80% 348M/437M [00:19<00:03, 24.1MB/s]\u001b[A\n",
            "Downloading data:  80% 350M/437M [00:19<00:03, 22.3MB/s]\u001b[A\n",
            "Downloading data:  81% 353M/437M [00:19<00:03, 23.6MB/s]\u001b[A\n",
            "Downloading data:  81% 355M/437M [00:19<00:03, 22.2MB/s]\u001b[A\n",
            "Downloading data:  82% 358M/437M [00:19<00:03, 23.8MB/s]\u001b[A\n",
            "Downloading data:  83% 361M/437M [00:19<00:03, 22.2MB/s]\u001b[A\n",
            "Downloading data:  83% 364M/437M [00:19<00:03, 24.3MB/s]\u001b[A\n",
            "Downloading data:  84% 366M/437M [00:20<00:03, 22.3MB/s]\u001b[A\n",
            "Downloading data:  85% 369M/437M [00:20<00:03, 22.3MB/s]\u001b[A\n",
            "Downloading data:  85% 372M/437M [00:20<00:02, 24.1MB/s]\u001b[A\n",
            "Downloading data:  86% 375M/437M [00:20<00:03, 19.5MB/s]\u001b[A\n",
            "Downloading data:  87% 379M/437M [00:20<00:02, 20.0MB/s]\u001b[A\n",
            "Downloading data:  88% 384M/437M [00:20<00:01, 26.6MB/s]\u001b[A\n",
            "Downloading data:  89% 387M/437M [00:21<00:02, 23.5MB/s]\u001b[A\n",
            "Downloading data:  89% 390M/437M [00:21<00:02, 20.5MB/s]\u001b[A\n",
            "Downloading data:  90% 395M/437M [00:21<00:01, 21.0MB/s]\u001b[A\n",
            "Downloading data:  92% 400M/437M [00:21<00:01, 27.0MB/s]\u001b[A\n",
            "Downloading data:  92% 403M/437M [00:21<00:01, 23.5MB/s]\u001b[A\n",
            "Downloading data:  93% 406M/437M [00:21<00:01, 21.2MB/s]\u001b[A\n",
            "Downloading data:  94% 410M/437M [00:21<00:01, 24.5MB/s]\u001b[A\n",
            "Downloading data:  95% 413M/437M [00:22<00:01, 23.0MB/s]\u001b[A\n",
            "Downloading data:  95% 416M/437M [00:22<00:00, 22.2MB/s]\u001b[A\n",
            "Downloading data:  96% 418M/437M [00:22<00:00, 23.6MB/s]\u001b[A\n",
            "Downloading data:  96% 421M/437M [00:22<00:00, 20.9MB/s]\u001b[A\n",
            "Downloading data:  98% 426M/437M [00:22<00:00, 25.3MB/s]\u001b[A\n",
            "Downloading data:  98% 429M/437M [00:22<00:00, 23.0MB/s]\u001b[A\n",
            "Downloading data:  99% 431M/437M [00:22<00:00, 22.7MB/s]\u001b[A\n",
            "Downloading data:  99% 434M/437M [00:23<00:00, 22.1MB/s]\u001b[A\n",
            "Downloading data: 100% 437M/437M [00:23<00:00, 18.8MB/s]\n",
            "Downloading data files:  50% 1/2 [03:12<03:12, 192.44s/it]\n",
            "Downloading data:   0% 0.00/304M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   0% 2.05k/304M [00:00<9:15:30, 9.12kB/s]\u001b[A\n",
            "Downloading data:   0% 52.2k/304M [00:00<37:43, 134kB/s]   \u001b[A\n",
            "Downloading data:   0% 122k/304M [00:00<23:47, 213kB/s] \u001b[A\n",
            "Downloading data:   0% 296k/304M [00:00<11:43, 432kB/s]\u001b[A\n",
            "Downloading data:   0% 609k/304M [00:01<06:31, 776kB/s]\u001b[A\n",
            "Downloading data:   0% 1.25M/304M [00:01<03:24, 1.48MB/s]\u001b[A\n",
            "Downloading data:   1% 2.53M/304M [00:01<01:46, 2.83MB/s]\u001b[A\n",
            "Downloading data:   2% 5.08M/304M [00:01<00:54, 5.51MB/s]\u001b[A\n",
            "Downloading data:   3% 7.83M/304M [00:02<00:39, 7.58MB/s]\u001b[A\n",
            "Downloading data:   4% 10.8M/304M [00:02<00:31, 9.24MB/s]\u001b[A\n",
            "Downloading data:   5% 13.7M/304M [00:02<00:27, 10.4MB/s]\u001b[A\n",
            "Downloading data:   6% 16.8M/304M [00:02<00:25, 11.4MB/s]\u001b[A\n",
            "Downloading data:   7% 20.0M/304M [00:02<00:23, 12.2MB/s]\u001b[A\n",
            "Downloading data:   8% 23.3M/304M [00:03<00:21, 12.8MB/s]\u001b[A\n",
            "Downloading data:   9% 26.6M/304M [00:03<00:20, 13.4MB/s]\u001b[A\n",
            "Downloading data:  10% 30.0M/304M [00:03<00:19, 13.9MB/s]\u001b[A\n",
            "Downloading data:  11% 33.5M/304M [00:03<00:18, 14.4MB/s]\u001b[A\n",
            "Downloading data:  12% 37.1M/304M [00:04<00:18, 14.8MB/s]\u001b[A\n",
            "Downloading data:  13% 40.8M/304M [00:04<00:17, 15.2MB/s]\u001b[A\n",
            "Downloading data:  15% 44.7M/304M [00:04<00:16, 15.9MB/s]\u001b[A\n",
            "Downloading data:  16% 48.8M/304M [00:04<00:15, 16.4MB/s]\u001b[A\n",
            "Downloading data:  17% 52.9M/304M [00:04<00:14, 17.0MB/s]\u001b[A\n",
            "Downloading data:  19% 57.1M/304M [00:05<00:14, 17.4MB/s]\u001b[A\n",
            "Downloading data:  20% 61.4M/304M [00:05<00:13, 18.0MB/s]\u001b[A\n",
            "Downloading data:  22% 65.6M/304M [00:05<00:10, 21.8MB/s]\u001b[A\n",
            "Downloading data:  22% 68.1M/304M [00:05<00:12, 19.4MB/s]\u001b[A\n",
            "Downloading data:  23% 70.3M/304M [00:05<00:13, 17.4MB/s]\u001b[A\n",
            "Downloading data:  25% 74.7M/304M [00:05<00:10, 22.4MB/s]\u001b[A\n",
            "Downloading data:  25% 77.3M/304M [00:06<00:11, 19.5MB/s]\u001b[A\n",
            "Downloading data:  26% 79.6M/304M [00:06<00:12, 17.8MB/s]\u001b[A\n",
            "Downloading data:  28% 84.2M/304M [00:06<00:09, 23.7MB/s]\u001b[A\n",
            "Downloading data:  29% 87.0M/304M [00:06<00:10, 20.6MB/s]\u001b[A\n",
            "Downloading data:  29% 89.4M/304M [00:06<00:11, 18.4MB/s]\u001b[A\n",
            "Downloading data:  31% 93.9M/304M [00:07<00:10, 19.3MB/s]\u001b[A\n",
            "Downloading data:  32% 98.5M/304M [00:07<00:08, 24.3MB/s]\u001b[A\n",
            "Downloading data:  33% 101M/304M [00:07<00:09, 21.4MB/s] \u001b[A\n",
            "Downloading data:  34% 104M/304M [00:07<00:10, 19.6MB/s]\u001b[A\n",
            "Downloading data:  36% 109M/304M [00:07<00:07, 25.2MB/s]\u001b[A\n",
            "Downloading data:  37% 112M/304M [00:07<00:08, 21.8MB/s]\u001b[A\n",
            "Downloading data:  38% 114M/304M [00:07<00:09, 20.4MB/s]\u001b[A\n",
            "Downloading data:  39% 118M/304M [00:08<00:07, 24.6MB/s]\u001b[A\n",
            "Downloading data:  40% 121M/304M [00:08<00:08, 21.6MB/s]\u001b[A\n",
            "Downloading data:  41% 125M/304M [00:08<00:08, 21.1MB/s]\u001b[A\n",
            "Downloading data:  42% 129M/304M [00:08<00:06, 25.6MB/s]\u001b[A\n",
            "Downloading data:  43% 132M/304M [00:08<00:07, 22.2MB/s]\u001b[A\n",
            "Downloading data:  44% 135M/304M [00:08<00:08, 21.1MB/s]\u001b[A\n",
            "Downloading data:  46% 139M/304M [00:08<00:06, 25.8MB/s]\u001b[A\n",
            "Downloading data:  47% 142M/304M [00:09<00:07, 21.9MB/s]\u001b[A\n",
            "Downloading data:  48% 146M/304M [00:09<00:07, 21.0MB/s]\u001b[A\n",
            "Downloading data:  49% 150M/304M [00:09<00:06, 25.7MB/s]\u001b[A\n",
            "Downloading data:  50% 153M/304M [00:09<00:06, 21.9MB/s]\u001b[A\n",
            "Downloading data:  51% 156M/304M [00:09<00:07, 20.9MB/s]\u001b[A\n",
            "Downloading data:  53% 161M/304M [00:09<00:05, 26.5MB/s]\u001b[A\n",
            "Downloading data:  54% 164M/304M [00:10<00:06, 22.3MB/s]\u001b[A\n",
            "Downloading data:  55% 166M/304M [00:10<00:06, 20.5MB/s]\u001b[A\n",
            "Downloading data:  56% 171M/304M [00:10<00:05, 25.8MB/s]\u001b[A\n",
            "Downloading data:  57% 174M/304M [00:10<00:05, 21.9MB/s]\u001b[A\n",
            "Downloading data:  58% 177M/304M [00:10<00:06, 19.4MB/s]\u001b[A\n",
            "Downloading data:  60% 182M/304M [00:10<00:05, 21.9MB/s]\u001b[A\n",
            "Downloading data:  61% 186M/304M [00:10<00:04, 26.4MB/s]\u001b[A\n",
            "Downloading data:  62% 189M/304M [00:11<00:05, 22.5MB/s]\u001b[A\n",
            "Downloading data:  63% 192M/304M [00:11<00:05, 21.1MB/s]\u001b[A\n",
            "Downloading data:  65% 197M/304M [00:11<00:04, 26.0MB/s]\u001b[A\n",
            "Downloading data:  66% 200M/304M [00:11<00:04, 21.9MB/s]\u001b[A\n",
            "Downloading data:  67% 202M/304M [00:11<00:04, 20.7MB/s]\u001b[A\n",
            "Downloading data:  68% 207M/304M [00:11<00:03, 26.7MB/s]\u001b[A\n",
            "Downloading data:  69% 210M/304M [00:12<00:04, 22.6MB/s]\u001b[A\n",
            "Downloading data:  70% 213M/304M [00:12<00:04, 19.4MB/s]\u001b[A\n",
            "Downloading data:  72% 218M/304M [00:12<00:03, 21.7MB/s]\u001b[A\n",
            "Downloading data:  73% 222M/304M [00:12<00:03, 25.1MB/s]\u001b[A\n",
            "Downloading data:  74% 225M/304M [00:12<00:03, 21.8MB/s]\u001b[A\n",
            "Downloading data:  75% 228M/304M [00:12<00:03, 21.9MB/s]\u001b[A\n",
            "Downloading data:  77% 233M/304M [00:13<00:02, 26.6MB/s]\u001b[A\n",
            "Downloading data:  78% 236M/304M [00:13<00:03, 22.6MB/s]\u001b[A\n",
            "Downloading data:  79% 239M/304M [00:13<00:03, 21.3MB/s]\u001b[A\n",
            "Downloading data:  80% 243M/304M [00:13<00:02, 25.5MB/s]\u001b[A\n",
            "Downloading data:  81% 246M/304M [00:13<00:02, 21.5MB/s]\u001b[A\n",
            "Downloading data:  82% 249M/304M [00:13<00:02, 20.7MB/s]\u001b[A\n",
            "Downloading data:  83% 253M/304M [00:13<00:01, 26.0MB/s]\u001b[A\n",
            "Downloading data:  84% 256M/304M [00:14<00:02, 21.9MB/s]\u001b[A\n",
            "Downloading data:  85% 259M/304M [00:14<00:02, 20.9MB/s]\u001b[A\n",
            "Downloading data:  86% 262M/304M [00:14<00:01, 23.1MB/s]\u001b[A\n",
            "Downloading data:  87% 265M/304M [00:14<00:01, 20.4MB/s]\u001b[A\n",
            "Downloading data:  89% 270M/304M [00:14<00:01, 21.9MB/s]\u001b[A\n",
            "Downloading data:  90% 274M/304M [00:14<00:01, 27.3MB/s]\u001b[A\n",
            "Downloading data:  91% 277M/304M [00:15<00:01, 22.8MB/s]\u001b[A\n",
            "Downloading data:  92% 280M/304M [00:15<00:01, 21.0MB/s]\u001b[A\n",
            "Downloading data:  93% 284M/304M [00:15<00:00, 24.6MB/s]\u001b[A\n",
            "Downloading data:  94% 286M/304M [00:15<00:00, 21.2MB/s]\u001b[A\n",
            "Downloading data:  95% 290M/304M [00:15<00:00, 21.4MB/s]\u001b[A\n",
            "Downloading data:  97% 295M/304M [00:15<00:00, 26.6MB/s]\u001b[A\n",
            "Downloading data:  98% 298M/304M [00:15<00:00, 22.4MB/s]\u001b[A\n",
            "Downloading data: 100% 304M/304M [00:16<00:00, 18.8MB/s]\n",
            "Downloading data files: 100% 2/2 [03:32<00:00, 106.26s/it]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 474.39it/s]\n",
            "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/gary109--crop14-small-d2283ee3b6a9a5fe/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 35.55it/s]\n",
            "Casting the dataset: 100% 1/1 [00:13<00:00, 13.83s/ba]\n",
            "Casting the dataset: 100% 1/1 [00:01<00:00,  1.65s/ba]\n",
            "Downloading builder script: 3.19kB [00:00, 3.33MB/s]       \n",
            "[INFO|hub.py:583] 2022-04-28 02:04:53,346 >> https://huggingface.co/google/vit-huge-patch14-224-in21k/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpknjqvgxg\n",
            "Downloading: 100% 503/503 [00:00<00:00, 457kB/s]\n",
            "[INFO|hub.py:587] 2022-04-28 02:04:54,243 >> storing https://huggingface.co/google/vit-huge-patch14-224-in21k/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/8ade236f30a7feb4a8dba30a453548d0570e96d6544bfee36b460db42557c7c1.4421305ebaffc3e34437ab7e4244ddb07cf9cab530b37fa79b40117d724755cb\n",
            "[INFO|hub.py:595] 2022-04-28 02:04:54,243 >> creating metadata file for /root/.cache/huggingface/transformers/8ade236f30a7feb4a8dba30a453548d0570e96d6544bfee36b460db42557c7c1.4421305ebaffc3e34437ab7e4244ddb07cf9cab530b37fa79b40117d724755cb\n",
            "[INFO|configuration_utils.py:659] 2022-04-28 02:04:54,244 >> loading configuration file https://huggingface.co/google/vit-huge-patch14-224-in21k/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8ade236f30a7feb4a8dba30a453548d0570e96d6544bfee36b460db42557c7c1.4421305ebaffc3e34437ab7e4244ddb07cf9cab530b37fa79b40117d724755cb\n",
            "[INFO|configuration_utils.py:704] 2022-04-28 02:04:54,244 >> Model config ViTConfig {\n",
            "  \"_name_or_path\": \"google/vit-huge-patch14-224-in21k\",\n",
            "  \"architectures\": [\n",
            "    \"ViTModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"finetuning_task\": \"image-classification\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 1280,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"peanut\",\n",
            "    \"1\": \"corn\",\n",
            "    \"10\": \"carrot\",\n",
            "    \"11\": \"dragonfruit\",\n",
            "    \"12\": \"pumpkin\",\n",
            "    \"13\": \"tomato\",\n",
            "    \"2\": \"bareland\",\n",
            "    \"3\": \"pineapple\",\n",
            "    \"4\": \"rice\",\n",
            "    \"5\": \"garlic\",\n",
            "    \"6\": \"soybean\",\n",
            "    \"7\": \"guava\",\n",
            "    \"8\": \"banana\",\n",
            "    \"9\": \"sugarcane\"\n",
            "  },\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 5120,\n",
            "  \"label2id\": {\n",
            "    \"banana\": \"8\",\n",
            "    \"bareland\": \"2\",\n",
            "    \"carrot\": \"10\",\n",
            "    \"corn\": \"1\",\n",
            "    \"dragonfruit\": \"11\",\n",
            "    \"garlic\": \"5\",\n",
            "    \"guava\": \"7\",\n",
            "    \"peanut\": \"0\",\n",
            "    \"pineapple\": \"3\",\n",
            "    \"pumpkin\": \"12\",\n",
            "    \"rice\": \"4\",\n",
            "    \"soybean\": \"6\",\n",
            "    \"sugarcane\": \"9\",\n",
            "    \"tomato\": \"13\"\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"patch_size\": 14,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.19.0.dev0\"\n",
            "}\n",
            "\n",
            "[INFO|hub.py:583] 2022-04-28 02:04:55,166 >> https://huggingface.co/google/vit-huge-patch14-224-in21k/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfes48okw\n",
            "Downloading: 100% 2.36G/2.36G [00:39<00:00, 64.2MB/s]\n",
            "[INFO|hub.py:587] 2022-04-28 02:05:34,778 >> storing https://huggingface.co/google/vit-huge-patch14-224-in21k/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/0e1d87a339ae8bdffce2108071c66b5a0db276a5dc294fe5d123fed1473d7c1e.6e61a750a3a5b1ac7b5a9f5cf2edc3506e1218bc898b57eae7e3c45c6429357b\n",
            "[INFO|hub.py:595] 2022-04-28 02:05:34,778 >> creating metadata file for /root/.cache/huggingface/transformers/0e1d87a339ae8bdffce2108071c66b5a0db276a5dc294fe5d123fed1473d7c1e.6e61a750a3a5b1ac7b5a9f5cf2edc3506e1218bc898b57eae7e3c45c6429357b\n",
            "[INFO|modeling_utils.py:1879] 2022-04-28 02:05:34,779 >> loading weights file https://huggingface.co/google/vit-huge-patch14-224-in21k/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0e1d87a339ae8bdffce2108071c66b5a0db276a5dc294fe5d123fed1473d7c1e.6e61a750a3a5b1ac7b5a9f5cf2edc3506e1218bc898b57eae7e3c45c6429357b\n",
            "[WARNING|modeling_utils.py:2181] 2022-04-28 02:05:41,853 >> Some weights of the model checkpoint at google/vit-huge-patch14-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2192] 2022-04-28 02:05:41,853 >> Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-huge-patch14-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[INFO|hub.py:583] 2022-04-28 02:05:42,758 >> https://huggingface.co/google/vit-huge-patch14-224-in21k/resolve/main/preprocessor_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzcv4ts30\n",
            "Downloading: 100% 160/160 [00:00<00:00, 139kB/s]\n",
            "[INFO|hub.py:587] 2022-04-28 02:05:43,657 >> storing https://huggingface.co/google/vit-huge-patch14-224-in21k/resolve/main/preprocessor_config.json in cache at /root/.cache/huggingface/transformers/ffb3ab81931c92867adb522fc01b459b501bc18f909584a14b11ba76194a2f8d.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
            "[INFO|hub.py:595] 2022-04-28 02:05:43,657 >> creating metadata file for /root/.cache/huggingface/transformers/ffb3ab81931c92867adb522fc01b459b501bc18f909584a14b11ba76194a2f8d.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
            "[INFO|feature_extraction_utils.py:465] 2022-04-28 02:05:43,657 >> loading feature extractor configuration file https://huggingface.co/google/vit-huge-patch14-224-in21k/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/ffb3ab81931c92867adb522fc01b459b501bc18f909584a14b11ba76194a2f8d.c322cbf30b69973d5aae6c0866f5cba198b5fe51a2fe259d2a506827ec6274bc\n",
            "[INFO|configuration_utils.py:659] 2022-04-28 02:05:44,549 >> loading configuration file https://huggingface.co/google/vit-huge-patch14-224-in21k/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8ade236f30a7feb4a8dba30a453548d0570e96d6544bfee36b460db42557c7c1.4421305ebaffc3e34437ab7e4244ddb07cf9cab530b37fa79b40117d724755cb\n",
            "[INFO|configuration_utils.py:704] 2022-04-28 02:05:44,550 >> Model config ViTConfig {\n",
            "  \"_name_or_path\": \"google/vit-huge-patch14-224-in21k\",\n",
            "  \"architectures\": [\n",
            "    \"ViTModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 1280,\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 5120,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"patch_size\": 14,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.19.0.dev0\"\n",
            "}\n",
            "\n",
            "[INFO|feature_extraction_utils.py:501] 2022-04-28 02:05:44,554 >> Feature extractor ViTFeatureExtractor {\n",
            "  \"do_normalize\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
            "  \"image_mean\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"image_std\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"resample\": 2,\n",
            "  \"size\": 224\n",
            "}\n",
            "\n",
            "Cloning https://huggingface.co/gary109/crop14-small_vit-huge-patch14-224-in21k into local empty directory.\n",
            "04/28/2022 02:06:02 - WARNING - huggingface_hub.repository - Cloning https://huggingface.co/gary109/crop14-small_vit-huge-patch14-224-in21k into local empty directory.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1317] 2022-04-28 02:06:08,596 >> ***** Running training *****\n",
            "[INFO|trainer.py:1318] 2022-04-28 02:06:08,596 >>   Num examples = 1260\n",
            "[INFO|trainer.py:1319] 2022-04-28 02:06:08,596 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1320] 2022-04-28 02:06:08,596 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1321] 2022-04-28 02:06:08,596 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1322] 2022-04-28 02:06:08,596 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:1323] 2022-04-28 02:06:08,596 >>   Total optimization steps = 1170\n",
            "{'loss': 2.6255, 'learning_rate': 1.982905982905983e-05, 'epoch': 0.25}\n",
            "{'loss': 2.5785, 'learning_rate': 1.965811965811966e-05, 'epoch': 0.51}\n",
            "{'loss': 2.5287, 'learning_rate': 1.9487179487179488e-05, 'epoch': 0.76}\n",
            "  3% 39/1170 [13:06<6:24:15, 20.38s/it][INFO|trainer.py:2443] 2022-04-28 02:19:23,107 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 02:19:23,107 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 02:19:23,107 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.64it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.18it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.02s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.07s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.11s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.16s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.20s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.20s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.22s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:29,  1.24s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.23s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:26,  1.22s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:25,  1.23s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.23s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.21s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.21s/it]\u001b[A\n",
            " 51% 18/35 [00:20<00:21,  1.25s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.20s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.19s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.19s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.17s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.19s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:13,  1.20s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:11,  1.19s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.18s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.18s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.20s/it]\u001b[A\n",
            " 83% 29/35 [00:34<00:07,  1.24s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.22s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.20s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.21s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.19s/it]\u001b[A\n",
            " 97% 34/35 [00:40<00:01,  1.23s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.24s/it]\u001b[A\n",
            "{'eval_loss': 2.436788558959961, 'eval_accuracy': 0.6142857142857143, 'eval_runtime': 42.8557, 'eval_samples_per_second': 3.267, 'eval_steps_per_second': 0.817, 'epoch': 0.99}\n",
            "\n",
            "  3% 39/1170 [13:57<6:24:15, 20.38s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 02:20:05,964 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-39\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 02:20:05,965 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-39/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 02:20:15,682 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-39/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 02:20:15,683 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-39/preprocessor_config.json\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 02:20:47,035 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/preprocessor_config.json\n",
            "{'loss': 2.5503, 'learning_rate': 1.9316239316239317e-05, 'epoch': 1.03}\n",
            "{'loss': 2.3811, 'learning_rate': 1.914529914529915e-05, 'epoch': 1.28}\n",
            "{'loss': 2.3015, 'learning_rate': 1.8974358974358975e-05, 'epoch': 1.53}\n",
            "{'loss': 2.2329, 'learning_rate': 1.8803418803418804e-05, 'epoch': 1.79}\n",
            "  7% 78/1170 [29:11<6:34:19, 21.67s/it][INFO|trainer.py:2443] 2022-04-28 02:35:27,981 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 02:35:27,981 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 02:35:27,981 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:23,  1.39it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:32,  1.01s/it]\u001b[A\n",
            " 11% 4/35 [00:04<00:37,  1.20s/it]\u001b[A\n",
            " 14% 5/35 [00:05<00:37,  1.26s/it]\u001b[A\n",
            " 17% 6/35 [00:07<00:37,  1.31s/it]\u001b[A\n",
            " 20% 7/35 [00:08<00:38,  1.37s/it]\u001b[A\n",
            " 23% 8/35 [00:10<00:38,  1.42s/it]\u001b[A\n",
            " 26% 9/35 [00:11<00:36,  1.41s/it]\u001b[A\n",
            " 29% 10/35 [00:13<00:36,  1.44s/it]\u001b[A\n",
            " 31% 11/35 [00:14<00:35,  1.47s/it]\u001b[A\n",
            " 34% 12/35 [00:16<00:33,  1.46s/it]\u001b[A\n",
            " 37% 13/35 [00:17<00:31,  1.43s/it]\u001b[A\n",
            " 40% 14/35 [00:18<00:30,  1.45s/it]\u001b[A\n",
            " 43% 15/35 [00:20<00:28,  1.44s/it]\u001b[A\n",
            " 46% 16/35 [00:21<00:26,  1.42s/it]\u001b[A\n",
            " 49% 17/35 [00:23<00:25,  1.44s/it]\u001b[A\n",
            " 51% 18/35 [00:24<00:24,  1.46s/it]\u001b[A\n",
            " 54% 19/35 [00:25<00:22,  1.40s/it]\u001b[A\n",
            " 57% 20/35 [00:27<00:20,  1.38s/it]\u001b[A\n",
            " 60% 21/35 [00:28<00:19,  1.39s/it]\u001b[A\n",
            " 63% 22/35 [00:30<00:17,  1.37s/it]\u001b[A\n",
            " 66% 23/35 [00:31<00:16,  1.39s/it]\u001b[A\n",
            " 69% 24/35 [00:32<00:15,  1.40s/it]\u001b[A\n",
            " 71% 25/35 [00:34<00:13,  1.40s/it]\u001b[A\n",
            " 74% 26/35 [00:35<00:12,  1.37s/it]\u001b[A\n",
            " 77% 27/35 [00:36<00:11,  1.38s/it]\u001b[A\n",
            " 80% 28/35 [00:38<00:09,  1.41s/it]\u001b[A\n",
            " 83% 29/35 [00:40<00:08,  1.46s/it]\u001b[A\n",
            " 86% 30/35 [00:41<00:07,  1.43s/it]\u001b[A\n",
            " 89% 31/35 [00:42<00:05,  1.40s/it]\u001b[A\n",
            " 91% 32/35 [00:44<00:04,  1.40s/it]\u001b[A\n",
            " 94% 33/35 [00:45<00:02,  1.38s/it]\u001b[A\n",
            " 97% 34/35 [00:47<00:01,  1.43s/it]\u001b[A\n",
            "100% 35/35 [00:48<00:00,  1.43s/it]\u001b[A\n",
            "{'eval_loss': 2.099579095840454, 'eval_accuracy': 0.7928571428571428, 'eval_runtime': 50.1917, 'eval_samples_per_second': 2.789, 'eval_steps_per_second': 0.697, 'epoch': 1.99}\n",
            "\n",
            "  7% 78/1170 [30:09<6:34:19, 21.67s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 02:36:18,175 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-78\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 02:36:18,176 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-78/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 02:36:27,814 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-78/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 02:36:27,819 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-78/preprocessor_config.json\n",
            "{'loss': 2.2162, 'learning_rate': 1.8632478632478636e-05, 'epoch': 2.05}\n",
            "{'loss': 2.0336, 'learning_rate': 1.8461538461538465e-05, 'epoch': 2.3}\n",
            "{'loss': 1.9008, 'learning_rate': 1.829059829059829e-05, 'epoch': 2.56}\n",
            "{'loss': 1.8252, 'learning_rate': 1.8119658119658122e-05, 'epoch': 2.81}\n",
            " 10% 117/1170 [44:52<6:24:06, 21.89s/it][INFO|trainer.py:2443] 2022-04-28 02:51:09,469 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 02:51:09,469 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 02:51:09,469 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:23,  1.38it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:32,  1.01s/it]\u001b[A\n",
            " 11% 4/35 [00:04<00:37,  1.20s/it]\u001b[A\n",
            " 14% 5/35 [00:05<00:37,  1.25s/it]\u001b[A\n",
            " 17% 6/35 [00:07<00:37,  1.30s/it]\u001b[A\n",
            " 20% 7/35 [00:08<00:37,  1.35s/it]\u001b[A\n",
            " 23% 8/35 [00:10<00:37,  1.40s/it]\u001b[A\n",
            " 26% 9/35 [00:11<00:36,  1.40s/it]\u001b[A\n",
            " 29% 10/35 [00:12<00:35,  1.43s/it]\u001b[A\n",
            " 31% 11/35 [00:14<00:35,  1.47s/it]\u001b[A\n",
            " 34% 12/35 [00:15<00:33,  1.45s/it]\u001b[A\n",
            " 37% 13/35 [00:17<00:31,  1.44s/it]\u001b[A\n",
            " 40% 14/35 [00:18<00:30,  1.45s/it]\u001b[A\n",
            " 43% 15/35 [00:20<00:29,  1.45s/it]\u001b[A\n",
            " 46% 16/35 [00:21<00:26,  1.38s/it]\u001b[A\n",
            " 49% 17/35 [00:22<00:24,  1.36s/it]\u001b[A\n",
            " 51% 18/35 [00:24<00:22,  1.35s/it]\u001b[A\n",
            " 54% 19/35 [00:25<00:20,  1.27s/it]\u001b[A\n",
            " 57% 20/35 [00:26<00:18,  1.24s/it]\u001b[A\n",
            " 60% 21/35 [00:27<00:17,  1.24s/it]\u001b[A\n",
            " 63% 22/35 [00:28<00:15,  1.21s/it]\u001b[A\n",
            " 66% 23/35 [00:30<00:14,  1.23s/it]\u001b[A\n",
            " 69% 24/35 [00:31<00:13,  1.23s/it]\u001b[A\n",
            " 71% 25/35 [00:32<00:12,  1.23s/it]\u001b[A\n",
            " 74% 26/35 [00:33<00:10,  1.21s/it]\u001b[A\n",
            " 77% 27/35 [00:34<00:09,  1.21s/it]\u001b[A\n",
            " 80% 28/35 [00:36<00:08,  1.24s/it]\u001b[A\n",
            " 83% 29/35 [00:37<00:07,  1.29s/it]\u001b[A\n",
            " 86% 30/35 [00:38<00:06,  1.27s/it]\u001b[A\n",
            " 89% 31/35 [00:40<00:04,  1.24s/it]\u001b[A\n",
            " 91% 32/35 [00:41<00:03,  1.24s/it]\u001b[A\n",
            " 94% 33/35 [00:42<00:02,  1.22s/it]\u001b[A\n",
            " 97% 34/35 [00:43<00:01,  1.25s/it]\u001b[A\n",
            "100% 35/35 [00:45<00:00,  1.27s/it]\u001b[A\n",
            "{'eval_loss': 1.7134546041488647, 'eval_accuracy': 0.8357142857142857, 'eval_runtime': 46.67, 'eval_samples_per_second': 3.0, 'eval_steps_per_second': 0.75, 'epoch': 2.99}\n",
            "\n",
            " 10% 117/1170 [45:47<6:24:06, 21.89s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 02:51:56,141 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-117\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 02:51:56,141 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-117/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 02:52:05,734 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-117/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 02:52:05,734 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-117/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 02:52:25,857 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-78] due to args.save_total_limit\n",
            "{'loss': 1.7938, 'learning_rate': 1.794871794871795e-05, 'epoch': 3.08}\n",
            "{'loss': 1.6386, 'learning_rate': 1.7777777777777777e-05, 'epoch': 3.33}\n",
            "{'loss': 1.5981, 'learning_rate': 1.760683760683761e-05, 'epoch': 3.58}\n",
            "{'loss': 1.5134, 'learning_rate': 1.7435897435897438e-05, 'epoch': 3.84}\n",
            " 13% 156/1170 [59:39<5:46:58, 20.53s/it][INFO|trainer.py:2443] 2022-04-28 03:05:55,736 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 03:05:55,736 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 03:05:55,736 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.63it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.15it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:32,  1.03s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.09s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.13s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.17s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.21s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.21s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.23s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:30,  1.26s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.24s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:27,  1.24s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:26,  1.25s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.25s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:23,  1.22s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:22,  1.23s/it]\u001b[A\n",
            " 51% 18/35 [00:21<00:21,  1.25s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.20s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.19s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.20s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.18s/it]\u001b[A\n",
            " 66% 23/35 [00:27<00:14,  1.20s/it]\u001b[A\n",
            " 69% 24/35 [00:28<00:13,  1.20s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:12,  1.20s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.18s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.19s/it]\u001b[A\n",
            " 80% 28/35 [00:33<00:08,  1.21s/it]\u001b[A\n",
            " 83% 29/35 [00:34<00:07,  1.25s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.23s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.21s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.21s/it]\u001b[A\n",
            " 94% 33/35 [00:39<00:02,  1.19s/it]\u001b[A\n",
            " 97% 34/35 [00:40<00:01,  1.22s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.23s/it]\u001b[A\n",
            "{'eval_loss': 1.4369395971298218, 'eval_accuracy': 0.85, 'eval_runtime': 43.0204, 'eval_samples_per_second': 3.254, 'eval_steps_per_second': 0.814, 'epoch': 3.99}\n",
            "\n",
            " 13% 156/1170 [1:00:30<5:46:58, 20.53s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 03:06:38,769 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-156\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 03:06:38,769 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-156/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 03:06:48,421 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-156/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 03:06:48,422 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-156/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 03:07:08,616 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-39] due to args.save_total_limit\n",
            "{'loss': 1.4288, 'learning_rate': 1.7264957264957267e-05, 'epoch': 4.1}\n",
            "{'loss': 1.3388, 'learning_rate': 1.7094017094017095e-05, 'epoch': 4.36}\n",
            "{'loss': 1.3278, 'learning_rate': 1.6923076923076924e-05, 'epoch': 4.61}\n",
            "{'loss': 1.2749, 'learning_rate': 1.6752136752136753e-05, 'epoch': 4.86}\n",
            " 17% 195/1170 [1:14:21<5:34:37, 20.59s/it][INFO|trainer.py:2443] 2022-04-28 03:20:37,497 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 03:20:37,498 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 03:20:37,498 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.64it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.16it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:32,  1.03s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.09s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.13s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.18s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.21s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.21s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.23s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:30,  1.26s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.25s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:27,  1.24s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:26,  1.25s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.25s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:23,  1.23s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:22,  1.25s/it]\u001b[A\n",
            " 51% 18/35 [00:21<00:21,  1.27s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.22s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.19s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.19s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.18s/it]\u001b[A\n",
            " 66% 23/35 [00:27<00:14,  1.21s/it]\u001b[A\n",
            " 69% 24/35 [00:28<00:13,  1.22s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:12,  1.21s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.19s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.19s/it]\u001b[A\n",
            " 80% 28/35 [00:33<00:08,  1.22s/it]\u001b[A\n",
            " 83% 29/35 [00:34<00:07,  1.26s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.24s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.22s/it]\u001b[A\n",
            " 91% 32/35 [00:38<00:03,  1.22s/it]\u001b[A\n",
            " 94% 33/35 [00:39<00:02,  1.20s/it]\u001b[A\n",
            " 97% 34/35 [00:40<00:01,  1.25s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.25s/it]\u001b[A\n",
            "{'eval_loss': 1.2311246395111084, 'eval_accuracy': 0.8857142857142857, 'eval_runtime': 43.2775, 'eval_samples_per_second': 3.235, 'eval_steps_per_second': 0.809, 'epoch': 4.99}\n",
            "\n",
            " 17% 195/1170 [1:15:12<5:34:37, 20.59s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 03:21:20,777 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-195\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 03:21:20,778 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-195/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 03:21:30,410 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-195/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 03:21:30,411 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-195/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 03:21:50,560 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-117] due to args.save_total_limit\n",
            "{'loss': 1.2339, 'learning_rate': 1.6581196581196585e-05, 'epoch': 5.13}\n",
            "{'loss': 1.1684, 'learning_rate': 1.641025641025641e-05, 'epoch': 5.38}\n",
            "{'loss': 1.103, 'learning_rate': 1.623931623931624e-05, 'epoch': 5.63}\n",
            "{'loss': 1.0568, 'learning_rate': 1.6068376068376072e-05, 'epoch': 5.89}\n",
            " 20% 234/1170 [1:29:03<5:17:52, 20.38s/it][INFO|trainer.py:2443] 2022-04-28 03:35:19,548 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 03:35:19,549 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 03:35:19,549 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.64it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.16it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.03s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.08s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.12s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.17s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.21s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.21s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.24s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:30,  1.26s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.26s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:27,  1.24s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:26,  1.25s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.25s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:23,  1.23s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:22,  1.24s/it]\u001b[A\n",
            " 51% 18/35 [00:21<00:21,  1.26s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.21s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.19s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.19s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.18s/it]\u001b[A\n",
            " 66% 23/35 [00:27<00:14,  1.20s/it]\u001b[A\n",
            " 69% 24/35 [00:28<00:13,  1.20s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:12,  1.20s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.19s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.20s/it]\u001b[A\n",
            " 80% 28/35 [00:33<00:08,  1.23s/it]\u001b[A\n",
            " 83% 29/35 [00:34<00:07,  1.26s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.24s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.21s/it]\u001b[A\n",
            " 91% 32/35 [00:38<00:03,  1.21s/it]\u001b[A\n",
            " 94% 33/35 [00:39<00:02,  1.20s/it]\u001b[A\n",
            " 97% 34/35 [00:40<00:01,  1.23s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.24s/it]\u001b[A\n",
            "{'eval_loss': 1.0943340063095093, 'eval_accuracy': 0.8714285714285714, 'eval_runtime': 43.1847, 'eval_samples_per_second': 3.242, 'eval_steps_per_second': 0.81, 'epoch': 5.99}\n",
            "\n",
            " 20% 234/1170 [1:29:54<5:17:52, 20.38s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 03:36:02,735 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-234\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 03:36:02,735 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-234/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 03:36:12,420 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-234/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 03:36:12,421 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-234/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 03:36:32,439 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-156] due to args.save_total_limit\n",
            "{'loss': 1.027, 'learning_rate': 1.5897435897435897e-05, 'epoch': 6.15}\n",
            "{'loss': 0.9982, 'learning_rate': 1.5726495726495726e-05, 'epoch': 6.41}\n",
            "{'loss': 0.9819, 'learning_rate': 1.555555555555556e-05, 'epoch': 6.66}\n",
            "{'loss': 0.9367, 'learning_rate': 1.5384615384615387e-05, 'epoch': 6.91}\n",
            " 23% 273/1170 [1:43:44<5:06:21, 20.49s/it][INFO|trainer.py:2443] 2022-04-28 03:50:00,683 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 03:50:00,683 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 03:50:00,683 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.64it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.15it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:32,  1.04s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.08s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.12s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.18s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.21s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.21s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.24s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:30,  1.26s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.25s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:27,  1.24s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:26,  1.25s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.25s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:23,  1.23s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:22,  1.24s/it]\u001b[A\n",
            " 51% 18/35 [00:21<00:21,  1.27s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.21s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.19s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.20s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.18s/it]\u001b[A\n",
            " 66% 23/35 [00:27<00:14,  1.20s/it]\u001b[A\n",
            " 69% 24/35 [00:28<00:13,  1.21s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:12,  1.21s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.19s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.20s/it]\u001b[A\n",
            " 80% 28/35 [00:33<00:08,  1.22s/it]\u001b[A\n",
            " 83% 29/35 [00:34<00:07,  1.26s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.24s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.22s/it]\u001b[A\n",
            " 91% 32/35 [00:38<00:03,  1.21s/it]\u001b[A\n",
            " 94% 33/35 [00:39<00:02,  1.20s/it]\u001b[A\n",
            " 97% 34/35 [00:40<00:01,  1.24s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.24s/it]\u001b[A\n",
            "{'eval_loss': 0.9774573445320129, 'eval_accuracy': 0.8857142857142857, 'eval_runtime': 43.2458, 'eval_samples_per_second': 3.237, 'eval_steps_per_second': 0.809, 'epoch': 6.99}\n",
            "\n",
            " 23% 273/1170 [1:44:35<5:06:21, 20.49s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 03:50:43,931 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-273\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 03:50:43,931 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-273/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 03:50:53,489 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-273/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 03:50:53,490 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-273/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 03:51:13,582 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-195] due to args.save_total_limit\n",
            "{'loss': 0.9479, 'learning_rate': 1.5213675213675214e-05, 'epoch': 7.18}\n",
            "{'loss': 0.8823, 'learning_rate': 1.5042735042735043e-05, 'epoch': 7.43}\n",
            "{'loss': 0.8967, 'learning_rate': 1.4871794871794874e-05, 'epoch': 7.69}\n",
            "{'loss': 0.8347, 'learning_rate': 1.4700854700854703e-05, 'epoch': 7.94}\n",
            " 27% 312/1170 [1:58:22<4:53:44, 20.54s/it][INFO|trainer.py:2443] 2022-04-28 04:04:38,846 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 04:04:38,846 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 04:04:38,846 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.65it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.17it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.02s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.08s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.12s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.17s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.21s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.21s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.24s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:30,  1.26s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.24s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:27,  1.23s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:26,  1.24s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.24s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:23,  1.22s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:22,  1.23s/it]\u001b[A\n",
            " 51% 18/35 [00:21<00:21,  1.25s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.20s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.18s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.19s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.17s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.20s/it]\u001b[A\n",
            " 69% 24/35 [00:28<00:13,  1.21s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:12,  1.21s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.18s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.19s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.21s/it]\u001b[A\n",
            " 83% 29/35 [00:34<00:07,  1.24s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.22s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.20s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.20s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.18s/it]\u001b[A\n",
            " 97% 34/35 [00:40<00:01,  1.22s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.24s/it]\u001b[A\n",
            "{'eval_loss': 0.9073304533958435, 'eval_accuracy': 0.9142857142857143, 'eval_runtime': 42.9332, 'eval_samples_per_second': 3.261, 'eval_steps_per_second': 0.815, 'epoch': 7.99}\n",
            "\n",
            " 27% 312/1170 [1:59:13<4:53:44, 20.54s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 04:05:21,781 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-312\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 04:05:21,781 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-312/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 04:05:31,372 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-312/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 04:05:31,373 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-312/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 04:05:51,815 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-234] due to args.save_total_limit\n",
            "{'loss': 0.8221, 'learning_rate': 1.4529914529914531e-05, 'epoch': 8.2}\n",
            "{'loss': 0.7876, 'learning_rate': 1.435897435897436e-05, 'epoch': 8.46}\n",
            "{'loss': 0.8081, 'learning_rate': 1.4188034188034189e-05, 'epoch': 8.71}\n",
            "{'loss': 0.7833, 'learning_rate': 1.4017094017094018e-05, 'epoch': 8.97}\n",
            " 30% 351/1170 [2:13:02<4:40:03, 20.52s/it][INFO|trainer.py:2443] 2022-04-28 04:19:19,156 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 04:19:19,156 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 04:19:19,156 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.64it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.17it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.02s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.08s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.12s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.16s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.20s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.21s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.23s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:30,  1.26s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.25s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:27,  1.24s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:26,  1.25s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.25s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:23,  1.23s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:22,  1.24s/it]\u001b[A\n",
            " 51% 18/35 [00:21<00:21,  1.27s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.21s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.19s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.19s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.18s/it]\u001b[A\n",
            " 66% 23/35 [00:27<00:14,  1.21s/it]\u001b[A\n",
            " 69% 24/35 [00:28<00:13,  1.22s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:12,  1.21s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.19s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.20s/it]\u001b[A\n",
            " 80% 28/35 [00:33<00:08,  1.22s/it]\u001b[A\n",
            " 83% 29/35 [00:34<00:07,  1.26s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.24s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.22s/it]\u001b[A\n",
            " 91% 32/35 [00:38<00:03,  1.21s/it]\u001b[A\n",
            " 94% 33/35 [00:39<00:02,  1.20s/it]\u001b[A\n",
            " 97% 34/35 [00:40<00:01,  1.23s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.25s/it]\u001b[A\n",
            "{'eval_loss': 0.8478495478630066, 'eval_accuracy': 0.9, 'eval_runtime': 43.1763, 'eval_samples_per_second': 3.243, 'eval_steps_per_second': 0.811, 'epoch': 8.99}\n",
            "\n",
            " 30% 351/1170 [2:13:53<4:40:03, 20.52s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 04:20:02,334 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-351\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 04:20:02,335 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-351/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 04:20:11,864 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-351/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 04:20:11,865 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-351/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 04:20:32,061 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-273] due to args.save_total_limit\n",
            "{'loss': 0.7807, 'learning_rate': 1.3846153846153847e-05, 'epoch': 9.23}\n",
            "{'loss': 0.7723, 'learning_rate': 1.3675213675213677e-05, 'epoch': 9.48}\n",
            "{'loss': 0.7093, 'learning_rate': 1.3504273504273506e-05, 'epoch': 9.74}\n",
            "{'loss': 0.6913, 'learning_rate': 1.3333333333333333e-05, 'epoch': 9.99}\n",
            " 33% 390/1170 [2:27:21<4:13:05, 19.47s/it][INFO|trainer.py:2443] 2022-04-28 04:33:38,204 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 04:33:38,204 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 04:33:38,204 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:19,  1.65it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.18it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.00s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:31,  1.05s/it]\u001b[A\n",
            " 17% 6/35 [00:05<00:31,  1.09s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:31,  1.14s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:31,  1.18s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:30,  1.17s/it]\u001b[A\n",
            " 29% 10/35 [00:10<00:29,  1.20s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:29,  1.22s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:27,  1.21s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:26,  1.20s/it]\u001b[A\n",
            " 40% 14/35 [00:15<00:25,  1.21s/it]\u001b[A\n",
            " 43% 15/35 [00:16<00:24,  1.20s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.19s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.20s/it]\u001b[A\n",
            " 51% 18/35 [00:20<00:20,  1.22s/it]\u001b[A\n",
            " 54% 19/35 [00:21<00:18,  1.16s/it]\u001b[A\n",
            " 57% 20/35 [00:22<00:17,  1.14s/it]\u001b[A\n",
            " 60% 21/35 [00:23<00:16,  1.15s/it]\u001b[A\n",
            " 63% 22/35 [00:24<00:14,  1.14s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:13,  1.17s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:12,  1.17s/it]\u001b[A\n",
            " 71% 25/35 [00:28<00:11,  1.16s/it]\u001b[A\n",
            " 74% 26/35 [00:29<00:10,  1.14s/it]\u001b[A\n",
            " 77% 27/35 [00:30<00:09,  1.15s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.17s/it]\u001b[A\n",
            " 83% 29/35 [00:33<00:07,  1.21s/it]\u001b[A\n",
            " 86% 30/35 [00:34<00:05,  1.19s/it]\u001b[A\n",
            " 89% 31/35 [00:35<00:04,  1.17s/it]\u001b[A\n",
            " 91% 32/35 [00:36<00:03,  1.17s/it]\u001b[A\n",
            " 94% 33/35 [00:37<00:02,  1.16s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.19s/it]\u001b[A\n",
            "100% 35/35 [00:40<00:00,  1.20s/it]\u001b[A\n",
            "{'eval_loss': 0.7950195074081421, 'eval_accuracy': 0.9071428571428571, 'eval_runtime': 41.7593, 'eval_samples_per_second': 3.353, 'eval_steps_per_second': 0.838, 'epoch': 9.99}\n",
            "\n",
            " 33% 390/1170 [2:28:11<4:13:05, 19.47s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 04:34:19,964 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-390\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 04:34:19,965 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-390/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 04:34:29,542 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-390/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 04:34:29,543 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-390/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 04:34:49,616 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-312] due to args.save_total_limit\n",
            "{'loss': 0.7017, 'learning_rate': 1.3162393162393164e-05, 'epoch': 10.25}\n",
            "{'loss': 0.6915, 'learning_rate': 1.2991452991452993e-05, 'epoch': 10.51}\n",
            "{'loss': 0.6626, 'learning_rate': 1.2820512820512823e-05, 'epoch': 10.76}\n",
            " 37% 429/1170 [2:41:26<4:04:52, 19.83s/it][INFO|trainer.py:2443] 2022-04-28 04:47:42,370 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 04:47:42,370 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 04:47:42,370 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:19,  1.68it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:26,  1.19it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.00s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:31,  1.06s/it]\u001b[A\n",
            " 17% 6/35 [00:05<00:31,  1.09s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:31,  1.14s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:31,  1.18s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:30,  1.18s/it]\u001b[A\n",
            " 29% 10/35 [00:10<00:30,  1.21s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:29,  1.23s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.22s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:26,  1.21s/it]\u001b[A\n",
            " 40% 14/35 [00:15<00:25,  1.22s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.21s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.19s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.20s/it]\u001b[A\n",
            " 51% 18/35 [00:20<00:20,  1.22s/it]\u001b[A\n",
            " 54% 19/35 [00:21<00:18,  1.17s/it]\u001b[A\n",
            " 57% 20/35 [00:22<00:17,  1.16s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.16s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:14,  1.15s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:13,  1.17s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:12,  1.17s/it]\u001b[A\n",
            " 71% 25/35 [00:28<00:11,  1.17s/it]\u001b[A\n",
            " 74% 26/35 [00:29<00:10,  1.14s/it]\u001b[A\n",
            " 77% 27/35 [00:30<00:09,  1.15s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.18s/it]\u001b[A\n",
            " 83% 29/35 [00:33<00:07,  1.22s/it]\u001b[A\n",
            " 86% 30/35 [00:34<00:05,  1.19s/it]\u001b[A\n",
            " 89% 31/35 [00:35<00:04,  1.17s/it]\u001b[A\n",
            " 91% 32/35 [00:36<00:03,  1.17s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.16s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.19s/it]\u001b[A\n",
            "100% 35/35 [00:40<00:00,  1.20s/it]\u001b[A\n",
            "{'eval_loss': 0.7425774931907654, 'eval_accuracy': 0.9142857142857143, 'eval_runtime': 41.8703, 'eval_samples_per_second': 3.344, 'eval_steps_per_second': 0.836, 'epoch': 10.99}\n",
            "\n",
            " 37% 429/1170 [2:42:15<4:04:52, 19.83s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 04:48:24,242 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-429\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 04:48:24,242 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-429/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 04:48:33,806 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-429/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 04:48:33,807 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-429/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 04:48:53,987 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-351] due to args.save_total_limit\n",
            "{'loss': 0.6793, 'learning_rate': 1.264957264957265e-05, 'epoch': 11.03}\n",
            "{'loss': 0.6605, 'learning_rate': 1.247863247863248e-05, 'epoch': 11.28}\n",
            "{'loss': 0.6384, 'learning_rate': 1.230769230769231e-05, 'epoch': 11.53}\n",
            "{'loss': 0.6326, 'learning_rate': 1.2136752136752137e-05, 'epoch': 11.79}\n",
            " 40% 468/1170 [2:55:29<3:49:30, 19.62s/it][INFO|trainer.py:2443] 2022-04-28 05:01:45,520 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 05:01:45,521 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 05:01:45,521 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:21,  1.55it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.15it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.02s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.07s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.11s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.15s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.19s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:30,  1.18s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.21s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:29,  1.23s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.22s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:26,  1.21s/it]\u001b[A\n",
            " 40% 14/35 [00:15<00:25,  1.21s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.21s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.19s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.21s/it]\u001b[A\n",
            " 51% 18/35 [00:20<00:20,  1.23s/it]\u001b[A\n",
            " 54% 19/35 [00:21<00:18,  1.17s/it]\u001b[A\n",
            " 57% 20/35 [00:22<00:17,  1.15s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.16s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:14,  1.14s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:13,  1.17s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:12,  1.17s/it]\u001b[A\n",
            " 71% 25/35 [00:28<00:11,  1.16s/it]\u001b[A\n",
            " 74% 26/35 [00:29<00:10,  1.14s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.15s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.17s/it]\u001b[A\n",
            " 83% 29/35 [00:33<00:07,  1.21s/it]\u001b[A\n",
            " 86% 30/35 [00:34<00:05,  1.20s/it]\u001b[A\n",
            " 89% 31/35 [00:35<00:04,  1.17s/it]\u001b[A\n",
            " 91% 32/35 [00:36<00:03,  1.16s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.15s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.19s/it]\u001b[A\n",
            "100% 35/35 [00:40<00:00,  1.20s/it]\u001b[A\n",
            "{'eval_loss': 0.7071954011917114, 'eval_accuracy': 0.9285714285714286, 'eval_runtime': 41.9422, 'eval_samples_per_second': 3.338, 'eval_steps_per_second': 0.834, 'epoch': 11.99}\n",
            "\n",
            " 40% 468/1170 [2:56:18<3:49:30, 19.62s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 05:02:27,464 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-468\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 05:02:27,465 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-468/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 05:02:37,042 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-468/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 05:02:37,043 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-468/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 05:02:57,061 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-390] due to args.save_total_limit\n",
            "{'loss': 0.608, 'learning_rate': 1.1965811965811966e-05, 'epoch': 12.05}\n",
            "{'loss': 0.6238, 'learning_rate': 1.1794871794871796e-05, 'epoch': 12.3}\n",
            "{'loss': 0.563, 'learning_rate': 1.1623931623931625e-05, 'epoch': 12.56}\n",
            "{'loss': 0.5669, 'learning_rate': 1.1452991452991454e-05, 'epoch': 12.81}\n",
            " 43% 507/1170 [3:09:31<3:37:58, 19.73s/it][INFO|trainer.py:2443] 2022-04-28 05:15:47,889 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 05:15:47,889 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 05:15:47,890 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:19,  1.67it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:26,  1.19it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:30,  1.00it/s]\u001b[A\n",
            " 14% 5/35 [00:04<00:31,  1.06s/it]\u001b[A\n",
            " 17% 6/35 [00:05<00:31,  1.10s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.15s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.19s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:30,  1.18s/it]\u001b[A\n",
            " 29% 10/35 [00:10<00:30,  1.21s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:29,  1.23s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.22s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:26,  1.21s/it]\u001b[A\n",
            " 40% 14/35 [00:15<00:25,  1.22s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.22s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.20s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.21s/it]\u001b[A\n",
            " 51% 18/35 [00:20<00:20,  1.23s/it]\u001b[A\n",
            " 54% 19/35 [00:21<00:18,  1.18s/it]\u001b[A\n",
            " 57% 20/35 [00:22<00:17,  1.16s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.16s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.16s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.18s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:12,  1.18s/it]\u001b[A\n",
            " 71% 25/35 [00:28<00:11,  1.17s/it]\u001b[A\n",
            " 74% 26/35 [00:29<00:10,  1.15s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.15s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.19s/it]\u001b[A\n",
            " 83% 29/35 [00:33<00:07,  1.23s/it]\u001b[A\n",
            " 86% 30/35 [00:34<00:06,  1.21s/it]\u001b[A\n",
            " 89% 31/35 [00:35<00:04,  1.18s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.17s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.15s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.19s/it]\u001b[A\n",
            "100% 35/35 [00:40<00:00,  1.20s/it]\u001b[A\n",
            "{'eval_loss': 0.684544026851654, 'eval_accuracy': 0.9285714285714286, 'eval_runtime': 42.0424, 'eval_samples_per_second': 3.33, 'eval_steps_per_second': 0.832, 'epoch': 12.99}\n",
            "\n",
            " 43% 507/1170 [3:10:21<3:37:58, 19.73s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 05:16:29,933 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-507\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 05:16:29,934 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-507/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 05:16:39,512 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-507/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 05:16:39,513 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-507/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 05:16:59,608 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-429] due to args.save_total_limit\n",
            "{'loss': 0.5839, 'learning_rate': 1.1282051282051283e-05, 'epoch': 13.08}\n",
            "{'loss': 0.5367, 'learning_rate': 1.1111111111111113e-05, 'epoch': 13.33}\n",
            "{'loss': 0.5916, 'learning_rate': 1.0940170940170942e-05, 'epoch': 13.58}\n",
            "{'loss': 0.5437, 'learning_rate': 1.076923076923077e-05, 'epoch': 13.84}\n",
            " 47% 546/1170 [3:23:37<3:25:12, 19.73s/it][INFO|trainer.py:2443] 2022-04-28 05:29:54,083 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 05:29:54,083 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 05:29:54,083 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:19,  1.69it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.18it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.01s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:31,  1.06s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:31,  1.10s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.15s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.19s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:30,  1.19s/it]\u001b[A\n",
            " 29% 10/35 [00:10<00:30,  1.21s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:29,  1.24s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.22s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:26,  1.21s/it]\u001b[A\n",
            " 40% 14/35 [00:15<00:25,  1.22s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.22s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.20s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.20s/it]\u001b[A\n",
            " 51% 18/35 [00:20<00:20,  1.23s/it]\u001b[A\n",
            " 54% 19/35 [00:21<00:18,  1.18s/it]\u001b[A\n",
            " 57% 20/35 [00:22<00:17,  1.16s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.17s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:14,  1.15s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.17s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:12,  1.18s/it]\u001b[A\n",
            " 71% 25/35 [00:28<00:11,  1.18s/it]\u001b[A\n",
            " 74% 26/35 [00:29<00:10,  1.16s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.16s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.19s/it]\u001b[A\n",
            " 83% 29/35 [00:33<00:07,  1.22s/it]\u001b[A\n",
            " 86% 30/35 [00:34<00:05,  1.20s/it]\u001b[A\n",
            " 89% 31/35 [00:35<00:04,  1.17s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.17s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.15s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.19s/it]\u001b[A\n",
            "100% 35/35 [00:40<00:00,  1.20s/it]\u001b[A\n",
            "{'eval_loss': 0.6740865707397461, 'eval_accuracy': 0.9, 'eval_runtime': 42.0801, 'eval_samples_per_second': 3.327, 'eval_steps_per_second': 0.832, 'epoch': 13.99}\n",
            "\n",
            " 47% 546/1170 [3:24:27<3:25:12, 19.73s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 05:30:36,164 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-546\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 05:30:36,165 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-546/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 05:30:45,746 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-546/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 05:30:45,747 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-546/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 05:31:05,814 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-468] due to args.save_total_limit\n",
            "{'loss': 0.5407, 'learning_rate': 1.05982905982906e-05, 'epoch': 14.1}\n",
            "{'loss': 0.5087, 'learning_rate': 1.0427350427350429e-05, 'epoch': 14.36}\n",
            "{'loss': 0.5366, 'learning_rate': 1.0256410256410256e-05, 'epoch': 14.61}\n",
            "{'loss': 0.5082, 'learning_rate': 1.0085470085470086e-05, 'epoch': 14.86}\n",
            " 50% 585/1170 [3:38:13<3:18:14, 20.33s/it][INFO|trainer.py:2443] 2022-04-28 05:44:30,054 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 05:44:30,054 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 05:44:30,054 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.62it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.16it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.03s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.08s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.12s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.17s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.21s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.20s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.23s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:30,  1.25s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.24s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:27,  1.23s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:26,  1.24s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.24s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:23,  1.22s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:22,  1.24s/it]\u001b[A\n",
            " 51% 18/35 [00:21<00:21,  1.26s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.21s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.19s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.20s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.19s/it]\u001b[A\n",
            " 66% 23/35 [00:27<00:14,  1.21s/it]\u001b[A\n",
            " 69% 24/35 [00:28<00:13,  1.21s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:12,  1.20s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.18s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.19s/it]\u001b[A\n",
            " 80% 28/35 [00:33<00:08,  1.21s/it]\u001b[A\n",
            " 83% 29/35 [00:34<00:07,  1.25s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.23s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.22s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.21s/it]\u001b[A\n",
            " 94% 33/35 [00:39<00:02,  1.19s/it]\u001b[A\n",
            " 97% 34/35 [00:40<00:01,  1.23s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.24s/it]\u001b[A\n",
            "{'eval_loss': 0.6704251170158386, 'eval_accuracy': 0.9071428571428571, 'eval_runtime': 43.0741, 'eval_samples_per_second': 3.25, 'eval_steps_per_second': 0.813, 'epoch': 14.99}\n",
            "\n",
            " 50% 585/1170 [3:39:04<3:18:14, 20.33s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 05:45:13,129 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-585\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 05:45:13,130 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-585/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 05:45:22,584 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-585/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 05:45:22,585 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-585/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 05:45:42,718 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-507] due to args.save_total_limit\n",
            "{'loss': 0.5818, 'learning_rate': 9.914529914529915e-06, 'epoch': 15.13}\n",
            "{'loss': 0.493, 'learning_rate': 9.743589743589744e-06, 'epoch': 15.38}\n",
            "{'loss': 0.4983, 'learning_rate': 9.572649572649575e-06, 'epoch': 15.63}\n",
            "{'loss': 0.5117, 'learning_rate': 9.401709401709402e-06, 'epoch': 15.89}\n",
            " 53% 624/1170 [3:52:41<3:02:30, 20.06s/it][INFO|trainer.py:2443] 2022-04-28 05:58:57,567 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 05:58:57,567 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 05:58:57,567 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.64it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.17it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.02s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.07s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.11s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.16s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.20s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.19s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.22s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:29,  1.25s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.23s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:26,  1.22s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:25,  1.23s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.23s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.21s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.22s/it]\u001b[A\n",
            " 51% 18/35 [00:20<00:21,  1.24s/it]\u001b[A\n",
            " 54% 19/35 [00:21<00:18,  1.18s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.16s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.17s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:14,  1.15s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.18s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:13,  1.19s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:11,  1.18s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.16s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.17s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.19s/it]\u001b[A\n",
            " 83% 29/35 [00:33<00:07,  1.23s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.21s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.19s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.18s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.17s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.21s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.22s/it]\u001b[A\n",
            "{'eval_loss': 0.6518473029136658, 'eval_accuracy': 0.8857142857142857, 'eval_runtime': 42.4353, 'eval_samples_per_second': 3.299, 'eval_steps_per_second': 0.825, 'epoch': 15.99}\n",
            "\n",
            " 53% 624/1170 [3:53:31<3:02:30, 20.06s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 05:59:40,004 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-624\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 05:59:40,005 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-624/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 05:59:49,557 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-624/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 05:59:49,558 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-624/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 06:00:09,629 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-546] due to args.save_total_limit\n",
            "{'loss': 0.4804, 'learning_rate': 9.230769230769232e-06, 'epoch': 16.15}\n",
            "{'loss': 0.5042, 'learning_rate': 9.059829059829061e-06, 'epoch': 16.41}\n",
            "{'loss': 0.4441, 'learning_rate': 8.888888888888888e-06, 'epoch': 16.66}\n",
            "{'loss': 0.4404, 'learning_rate': 8.717948717948719e-06, 'epoch': 16.91}\n",
            " 57% 663/1170 [4:07:01<2:50:33, 20.19s/it][INFO|trainer.py:2443] 2022-04-28 06:13:18,115 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 06:13:18,115 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 06:13:18,115 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.62it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.16it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.02s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.08s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.12s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.17s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.20s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.20s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.22s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:29,  1.25s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.23s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:27,  1.23s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:25,  1.23s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.24s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:23,  1.21s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:22,  1.23s/it]\u001b[A\n",
            " 51% 18/35 [00:21<00:21,  1.25s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.19s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.17s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.18s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.16s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.18s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:13,  1.19s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:11,  1.18s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.17s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.17s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.20s/it]\u001b[A\n",
            " 83% 29/35 [00:34<00:07,  1.23s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.21s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.19s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.19s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.17s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.20s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.21s/it]\u001b[A\n",
            "{'eval_loss': 0.6117457747459412, 'eval_accuracy': 0.8928571428571429, 'eval_runtime': 42.5486, 'eval_samples_per_second': 3.29, 'eval_steps_per_second': 0.823, 'epoch': 16.99}\n",
            "\n",
            " 57% 663/1170 [4:07:51<2:50:33, 20.19s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 06:14:00,665 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-663\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 06:14:00,666 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-663/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 06:14:10,177 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-663/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 06:14:10,178 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-663/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 06:14:30,221 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-585] due to args.save_total_limit\n",
            "{'loss': 0.4974, 'learning_rate': 8.547008547008548e-06, 'epoch': 17.18}\n",
            "{'loss': 0.5007, 'learning_rate': 8.376068376068377e-06, 'epoch': 17.43}\n",
            "{'loss': 0.4386, 'learning_rate': 8.205128205128205e-06, 'epoch': 17.69}\n",
            "{'loss': 0.4409, 'learning_rate': 8.034188034188036e-06, 'epoch': 17.94}\n",
            " 60% 702/1170 [4:21:23<2:36:22, 20.05s/it][INFO|trainer.py:2443] 2022-04-28 06:27:39,483 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 06:27:39,484 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 06:27:39,484 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.63it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.16it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.03s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.08s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.12s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.17s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.21s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.20s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.23s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:30,  1.25s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.24s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:27,  1.23s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:25,  1.24s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.23s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.21s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.22s/it]\u001b[A\n",
            " 51% 18/35 [00:21<00:21,  1.24s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:18,  1.19s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.17s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.17s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.16s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.19s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:13,  1.20s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:11,  1.19s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.16s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.17s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.19s/it]\u001b[A\n",
            " 83% 29/35 [00:34<00:07,  1.23s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.20s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.18s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.18s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.16s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.20s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.21s/it]\u001b[A\n",
            "{'eval_loss': 0.6299775242805481, 'eval_accuracy': 0.9, 'eval_runtime': 42.4821, 'eval_samples_per_second': 3.296, 'eval_steps_per_second': 0.824, 'epoch': 17.99}\n",
            "\n",
            " 60% 702/1170 [4:22:13<2:36:22, 20.05s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 06:28:21,967 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-702\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 06:28:21,968 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-702/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 06:28:31,555 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-702/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 06:28:31,556 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-702/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 06:28:51,609 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-624] due to args.save_total_limit\n",
            "{'loss': 0.4878, 'learning_rate': 7.863247863247863e-06, 'epoch': 18.2}\n",
            "{'loss': 0.4376, 'learning_rate': 7.692307692307694e-06, 'epoch': 18.46}\n",
            "{'loss': 0.4551, 'learning_rate': 7.521367521367522e-06, 'epoch': 18.71}\n",
            "{'loss': 0.4532, 'learning_rate': 7.350427350427351e-06, 'epoch': 18.97}\n",
            " 63% 741/1170 [4:35:44<2:25:33, 20.36s/it][INFO|trainer.py:2443] 2022-04-28 06:42:00,944 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 06:42:00,944 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 06:42:00,944 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:19,  1.66it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.17it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.02s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.07s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.11s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.16s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.20s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.20s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.23s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:30,  1.25s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.24s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:27,  1.23s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:25,  1.24s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.24s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:23,  1.21s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.22s/it]\u001b[A\n",
            " 51% 18/35 [00:21<00:21,  1.25s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.19s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.17s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.18s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.16s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.18s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:13,  1.18s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:11,  1.18s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.16s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.17s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.20s/it]\u001b[A\n",
            " 83% 29/35 [00:34<00:07,  1.24s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.22s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.20s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.19s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.18s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.21s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.21s/it]\u001b[A\n",
            "{'eval_loss': 0.6060218811035156, 'eval_accuracy': 0.9071428571428571, 'eval_runtime': 42.5299, 'eval_samples_per_second': 3.292, 'eval_steps_per_second': 0.823, 'epoch': 18.99}\n",
            "\n",
            " 63% 741/1170 [4:36:34<2:25:33, 20.36s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 06:42:43,475 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-741\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 06:42:43,476 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-741/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 06:42:53,040 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-741/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 06:42:53,041 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-741/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 06:43:13,104 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-663] due to args.save_total_limit\n",
            "{'loss': 0.4619, 'learning_rate': 7.17948717948718e-06, 'epoch': 19.23}\n",
            "{'loss': 0.4318, 'learning_rate': 7.008547008547009e-06, 'epoch': 19.48}\n",
            "{'loss': 0.4202, 'learning_rate': 6.837606837606839e-06, 'epoch': 19.74}\n",
            "{'loss': 0.4214, 'learning_rate': 6.666666666666667e-06, 'epoch': 19.99}\n",
            " 67% 780/1170 [4:50:07<2:11:29, 20.23s/it][INFO|trainer.py:2443] 2022-04-28 06:56:23,391 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 06:56:23,391 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 06:56:23,391 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.65it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.17it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.02s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.07s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.11s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.16s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.20s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.20s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.22s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:29,  1.24s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.23s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:26,  1.22s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:25,  1.23s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.23s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.20s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.22s/it]\u001b[A\n",
            " 51% 18/35 [00:20<00:21,  1.24s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.19s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.17s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.18s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.16s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.18s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:13,  1.19s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:11,  1.18s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.16s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.17s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.19s/it]\u001b[A\n",
            " 83% 29/35 [00:33<00:07,  1.23s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.21s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.19s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.19s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.17s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.20s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.21s/it]\u001b[A\n",
            "{'eval_loss': 0.5882512927055359, 'eval_accuracy': 0.9071428571428571, 'eval_runtime': 42.4179, 'eval_samples_per_second': 3.3, 'eval_steps_per_second': 0.825, 'epoch': 19.99}\n",
            "\n",
            " 67% 780/1170 [4:50:57<2:11:29, 20.23s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 06:57:05,810 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-780\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 06:57:05,811 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-780/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 06:57:15,329 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-780/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 06:57:15,330 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-780/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 06:57:35,368 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-702] due to args.save_total_limit\n",
            "{'loss': 0.4267, 'learning_rate': 6.495726495726496e-06, 'epoch': 20.25}\n",
            "{'loss': 0.3895, 'learning_rate': 6.324786324786325e-06, 'epoch': 20.51}\n",
            "{'loss': 0.4175, 'learning_rate': 6.153846153846155e-06, 'epoch': 20.76}\n",
            " 70% 819/1170 [5:04:28<1:57:14, 20.04s/it][INFO|trainer.py:2443] 2022-04-28 07:10:44,360 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 07:10:44,360 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 07:10:44,360 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:19,  1.66it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.17it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.02s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.08s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.12s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.16s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.20s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.20s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.22s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:29,  1.24s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.23s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:26,  1.22s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:25,  1.23s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.23s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.20s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.21s/it]\u001b[A\n",
            " 51% 18/35 [00:20<00:20,  1.23s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:18,  1.18s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.17s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.18s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.16s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.18s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:13,  1.19s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:11,  1.18s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.16s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.17s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.19s/it]\u001b[A\n",
            " 83% 29/35 [00:33<00:07,  1.23s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.21s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.19s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.18s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.17s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.21s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.22s/it]\u001b[A\n",
            "{'eval_loss': 0.5681129693984985, 'eval_accuracy': 0.9214285714285714, 'eval_runtime': 42.4638, 'eval_samples_per_second': 3.297, 'eval_steps_per_second': 0.824, 'epoch': 20.99}\n",
            "\n",
            " 70% 819/1170 [5:05:18<1:57:14, 20.04s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 07:11:26,826 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-819\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 07:11:26,826 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-819/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 07:11:36,365 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-819/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 07:11:36,366 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-819/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 07:11:56,503 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-741] due to args.save_total_limit\n",
            "{'loss': 0.4641, 'learning_rate': 5.982905982905983e-06, 'epoch': 21.03}\n",
            "{'loss': 0.4054, 'learning_rate': 5.8119658119658126e-06, 'epoch': 21.28}\n",
            "{'loss': 0.4197, 'learning_rate': 5.641025641025641e-06, 'epoch': 21.53}\n",
            "{'loss': 0.4002, 'learning_rate': 5.470085470085471e-06, 'epoch': 21.79}\n",
            " 73% 858/1170 [5:18:49<1:45:27, 20.28s/it][INFO|trainer.py:2443] 2022-04-28 07:25:05,048 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 07:25:05,049 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 07:25:05,049 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.63it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.16it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.02s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.08s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.11s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.16s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.20s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.20s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.22s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:29,  1.24s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.23s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:26,  1.23s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:25,  1.23s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.23s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.21s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.22s/it]\u001b[A\n",
            " 51% 18/35 [00:20<00:21,  1.24s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.19s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.17s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.17s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:14,  1.15s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.18s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:13,  1.19s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:11,  1.18s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.16s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.17s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.19s/it]\u001b[A\n",
            " 83% 29/35 [00:33<00:07,  1.23s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.21s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.19s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.18s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.17s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.21s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.22s/it]\u001b[A\n",
            "{'eval_loss': 0.5610222816467285, 'eval_accuracy': 0.9214285714285714, 'eval_runtime': 42.4965, 'eval_samples_per_second': 3.294, 'eval_steps_per_second': 0.824, 'epoch': 21.99}\n",
            "\n",
            " 73% 858/1170 [5:19:38<1:45:27, 20.28s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 07:25:47,546 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-858\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 07:25:47,547 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-858/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 07:25:57,110 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-858/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 07:25:57,111 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-858/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 07:26:17,119 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-780] due to args.save_total_limit\n",
            "{'loss': 0.4045, 'learning_rate': 5.2991452991453e-06, 'epoch': 22.05}\n",
            "{'loss': 0.3941, 'learning_rate': 5.128205128205128e-06, 'epoch': 22.3}\n",
            "{'loss': 0.3991, 'learning_rate': 4.957264957264958e-06, 'epoch': 22.56}\n",
            "{'loss': 0.4055, 'learning_rate': 4.786324786324787e-06, 'epoch': 22.81}\n",
            " 77% 897/1170 [5:33:09<1:31:43, 20.16s/it][INFO|trainer.py:2443] 2022-04-28 07:39:26,021 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 07:39:26,021 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 07:39:26,022 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:19,  1.65it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.18it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.02s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.07s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.11s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.16s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.20s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.19s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.22s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:29,  1.24s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.23s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:26,  1.22s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:25,  1.23s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.23s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.21s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.22s/it]\u001b[A\n",
            " 51% 18/35 [00:20<00:21,  1.24s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.19s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.17s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.17s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.16s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.18s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:13,  1.18s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:11,  1.18s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.15s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.16s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.19s/it]\u001b[A\n",
            " 83% 29/35 [00:33<00:07,  1.23s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.20s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.18s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.18s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.17s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.20s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.22s/it]\u001b[A\n",
            "{'eval_loss': 0.5373738408088684, 'eval_accuracy': 0.9214285714285714, 'eval_runtime': 42.4153, 'eval_samples_per_second': 3.301, 'eval_steps_per_second': 0.825, 'epoch': 22.99}\n",
            "\n",
            " 77% 897/1170 [5:33:59<1:31:43, 20.16s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 07:40:08,438 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-897\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 07:40:08,439 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-897/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 07:40:18,004 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-897/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 07:40:18,005 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-897/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 07:40:38,092 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-819] due to args.save_total_limit\n",
            "{'loss': 0.388, 'learning_rate': 4.615384615384616e-06, 'epoch': 23.08}\n",
            "{'loss': 0.397, 'learning_rate': 4.444444444444444e-06, 'epoch': 23.33}\n",
            "{'loss': 0.3651, 'learning_rate': 4.273504273504274e-06, 'epoch': 23.58}\n",
            "{'loss': 0.3643, 'learning_rate': 4.102564102564103e-06, 'epoch': 23.84}\n",
            " 80% 936/1170 [5:47:34<1:19:15, 20.32s/it][INFO|trainer.py:2443] 2022-04-28 07:53:50,811 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 07:53:50,811 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 07:53:50,811 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.63it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.16it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.02s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.08s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.12s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.17s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.21s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.20s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.23s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:30,  1.25s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.24s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:27,  1.23s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:26,  1.24s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.24s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:23,  1.22s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:22,  1.23s/it]\u001b[A\n",
            " 51% 18/35 [00:21<00:21,  1.25s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.21s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.19s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.19s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.17s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.19s/it]\u001b[A\n",
            " 69% 24/35 [00:28<00:13,  1.20s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:11,  1.20s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.18s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.18s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.21s/it]\u001b[A\n",
            " 83% 29/35 [00:34<00:07,  1.24s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.22s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.20s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.20s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.19s/it]\u001b[A\n",
            " 97% 34/35 [00:40<00:01,  1.22s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.23s/it]\u001b[A\n",
            "{'eval_loss': 0.5111494064331055, 'eval_accuracy': 0.9428571428571428, 'eval_runtime': 42.8895, 'eval_samples_per_second': 3.264, 'eval_steps_per_second': 0.816, 'epoch': 23.99}\n",
            "\n",
            " 80% 936/1170 [5:48:25<1:19:15, 20.32s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 07:54:33,703 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-936\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 07:54:33,703 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-936/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 07:54:43,227 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-936/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 07:54:43,228 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-936/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 07:55:03,299 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-858] due to args.save_total_limit\n",
            "{'loss': 0.4174, 'learning_rate': 3.9316239316239315e-06, 'epoch': 24.1}\n",
            "{'loss': 0.3777, 'learning_rate': 3.760683760683761e-06, 'epoch': 24.36}\n",
            "{'loss': 0.3453, 'learning_rate': 3.58974358974359e-06, 'epoch': 24.61}\n",
            "{'loss': 0.4046, 'learning_rate': 3.4188034188034193e-06, 'epoch': 24.86}\n",
            " 83% 975/1170 [6:02:14<1:06:14, 20.38s/it][INFO|trainer.py:2443] 2022-04-28 08:08:30,717 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 08:08:30,717 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 08:08:30,717 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.60it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:28,  1.14it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:32,  1.04s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.09s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.13s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.18s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.22s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.22s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.24s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:30,  1.26s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.26s/it]\u001b[A\n",
            " 37% 13/35 [00:15<00:27,  1.24s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:26,  1.25s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.25s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:23,  1.23s/it]\u001b[A\n",
            " 49% 17/35 [00:20<00:22,  1.24s/it]\u001b[A\n",
            " 51% 18/35 [00:21<00:21,  1.26s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.21s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.19s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.20s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.18s/it]\u001b[A\n",
            " 66% 23/35 [00:27<00:14,  1.20s/it]\u001b[A\n",
            " 69% 24/35 [00:28<00:13,  1.21s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:12,  1.21s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.19s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.19s/it]\u001b[A\n",
            " 80% 28/35 [00:33<00:08,  1.21s/it]\u001b[A\n",
            " 83% 29/35 [00:34<00:07,  1.25s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.23s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.21s/it]\u001b[A\n",
            " 91% 32/35 [00:38<00:03,  1.21s/it]\u001b[A\n",
            " 94% 33/35 [00:39<00:02,  1.19s/it]\u001b[A\n",
            " 97% 34/35 [00:40<00:01,  1.22s/it]\u001b[A\n",
            "100% 35/35 [00:41<00:00,  1.23s/it]\u001b[A\n",
            "{'eval_loss': 0.5076918601989746, 'eval_accuracy': 0.9428571428571428, 'eval_runtime': 43.1606, 'eval_samples_per_second': 3.244, 'eval_steps_per_second': 0.811, 'epoch': 24.99}\n",
            "\n",
            " 83% 975/1170 [6:03:05<1:06:14, 20.38s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 08:09:13,879 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-975\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 08:09:13,880 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-975/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 08:09:23,420 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-975/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 08:09:23,421 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-975/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 08:09:43,513 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-897] due to args.save_total_limit\n",
            "{'loss': 0.3721, 'learning_rate': 3.247863247863248e-06, 'epoch': 25.13}\n",
            "{'loss': 0.3735, 'learning_rate': 3.0769230769230774e-06, 'epoch': 25.38}\n",
            "{'loss': 0.3821, 'learning_rate': 2.9059829059829063e-06, 'epoch': 25.63}\n",
            "{'loss': 0.3497, 'learning_rate': 2.7350427350427355e-06, 'epoch': 25.89}\n",
            " 87% 1014/1170 [6:16:54<53:09, 20.45s/it][INFO|trainer.py:2443] 2022-04-28 08:23:11,149 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 08:23:11,150 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 08:23:11,150 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.62it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:28,  1.13it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:32,  1.04s/it]\u001b[A\n",
            " 14% 5/35 [00:05<00:33,  1.10s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:33,  1.14s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:33,  1.19s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:33,  1.23s/it]\u001b[A\n",
            " 26% 9/35 [00:10<00:31,  1.22s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:31,  1.25s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:30,  1.27s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.26s/it]\u001b[A\n",
            " 37% 13/35 [00:15<00:27,  1.25s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:26,  1.26s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:25,  1.25s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:23,  1.24s/it]\u001b[A\n",
            " 49% 17/35 [00:20<00:22,  1.25s/it]\u001b[A\n",
            " 51% 18/35 [00:21<00:21,  1.28s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:19,  1.23s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:18,  1.21s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.21s/it]\u001b[A\n",
            " 63% 22/35 [00:26<00:15,  1.19s/it]\u001b[A\n",
            " 66% 23/35 [00:27<00:14,  1.21s/it]\u001b[A\n",
            " 69% 24/35 [00:28<00:13,  1.22s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:12,  1.21s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.20s/it]\u001b[A\n",
            " 77% 27/35 [00:32<00:09,  1.20s/it]\u001b[A\n",
            " 80% 28/35 [00:33<00:08,  1.23s/it]\u001b[A\n",
            " 83% 29/35 [00:34<00:07,  1.27s/it]\u001b[A\n",
            " 86% 30/35 [00:36<00:06,  1.25s/it]\u001b[A\n",
            " 89% 31/35 [00:37<00:04,  1.22s/it]\u001b[A\n",
            " 91% 32/35 [00:38<00:03,  1.22s/it]\u001b[A\n",
            " 94% 33/35 [00:39<00:02,  1.20s/it]\u001b[A\n",
            " 97% 34/35 [00:40<00:01,  1.24s/it]\u001b[A\n",
            "100% 35/35 [00:42<00:00,  1.25s/it]\u001b[A\n",
            "{'eval_loss': 0.5354905724525452, 'eval_accuracy': 0.9142857142857143, 'eval_runtime': 43.5637, 'eval_samples_per_second': 3.214, 'eval_steps_per_second': 0.803, 'epoch': 25.99}\n",
            "\n",
            " 87% 1014/1170 [6:17:46<53:09, 20.45s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 08:23:54,715 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1014\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 08:23:54,716 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1014/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 08:24:04,284 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1014/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 08:24:04,285 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1014/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 08:24:24,323 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-936] due to args.save_total_limit\n",
            "{'loss': 0.3646, 'learning_rate': 2.564102564102564e-06, 'epoch': 26.15}\n",
            "{'loss': 0.3362, 'learning_rate': 2.3931623931623937e-06, 'epoch': 26.41}\n",
            "{'loss': 0.3483, 'learning_rate': 2.222222222222222e-06, 'epoch': 26.66}\n",
            "{'loss': 0.352, 'learning_rate': 2.0512820512820513e-06, 'epoch': 26.91}\n",
            " 90% 1053/1170 [6:31:17<38:31, 19.76s/it][INFO|trainer.py:2443] 2022-04-28 08:37:33,381 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 08:37:33,382 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 08:37:33,382 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.64it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.17it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.02s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.07s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.11s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.17s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.20s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:30,  1.19s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.21s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:29,  1.24s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.23s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:26,  1.22s/it]\u001b[A\n",
            " 40% 14/35 [00:15<00:25,  1.22s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.23s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.21s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.22s/it]\u001b[A\n",
            " 51% 18/35 [00:20<00:21,  1.24s/it]\u001b[A\n",
            " 54% 19/35 [00:21<00:18,  1.18s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.17s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.16s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:14,  1.15s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.17s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:12,  1.18s/it]\u001b[A\n",
            " 71% 25/35 [00:28<00:11,  1.17s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.15s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.16s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.18s/it]\u001b[A\n",
            " 83% 29/35 [00:33<00:07,  1.22s/it]\u001b[A\n",
            " 86% 30/35 [00:34<00:05,  1.20s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.18s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.18s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.16s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.19s/it]\u001b[A\n",
            "100% 35/35 [00:40<00:00,  1.20s/it]\u001b[A\n",
            "{'eval_loss': 0.5288921594619751, 'eval_accuracy': 0.9285714285714286, 'eval_runtime': 42.199, 'eval_samples_per_second': 3.318, 'eval_steps_per_second': 0.829, 'epoch': 26.99}\n",
            "\n",
            " 90% 1053/1170 [6:32:06<38:31, 19.76s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 08:38:15,582 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1053\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 08:38:15,583 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1053/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 08:38:25,148 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1053/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 08:38:25,149 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1053/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 08:38:45,196 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-1014] due to args.save_total_limit\n",
            "{'loss': 0.3717, 'learning_rate': 1.8803418803418804e-06, 'epoch': 27.18}\n",
            "{'loss': 0.3946, 'learning_rate': 1.7094017094017097e-06, 'epoch': 27.43}\n",
            "{'loss': 0.3614, 'learning_rate': 1.5384615384615387e-06, 'epoch': 27.69}\n",
            "{'loss': 0.3478, 'learning_rate': 1.3675213675213678e-06, 'epoch': 27.94}\n",
            " 93% 1092/1170 [6:45:25<25:32, 19.64s/it][INFO|trainer.py:2443] 2022-04-28 08:51:41,582 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 08:51:41,582 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 08:51:41,582 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:20,  1.65it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.17it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.01s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.08s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.11s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.15s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.20s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.20s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.23s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:30,  1.25s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.23s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:27,  1.23s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:25,  1.23s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.23s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.21s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.22s/it]\u001b[A\n",
            " 51% 18/35 [00:20<00:21,  1.24s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:18,  1.18s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.17s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.18s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:15,  1.15s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.18s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:12,  1.18s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:11,  1.17s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.16s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.16s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.20s/it]\u001b[A\n",
            " 83% 29/35 [00:33<00:07,  1.23s/it]\u001b[A\n",
            " 86% 30/35 [00:35<00:06,  1.21s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.18s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.17s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.15s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.19s/it]\u001b[A\n",
            "100% 35/35 [00:40<00:00,  1.20s/it]\u001b[A\n",
            "{'eval_loss': 0.5244995951652527, 'eval_accuracy': 0.9142857142857143, 'eval_runtime': 42.3243, 'eval_samples_per_second': 3.308, 'eval_steps_per_second': 0.827, 'epoch': 27.99}\n",
            "\n",
            " 93% 1092/1170 [6:46:15<25:32, 19.64s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 08:52:23,908 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1092\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 08:52:23,909 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1092/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 08:52:33,459 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1092/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 08:52:33,460 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1092/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 08:52:53,496 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-1053] due to args.save_total_limit\n",
            "{'loss': 0.391, 'learning_rate': 1.1965811965811968e-06, 'epoch': 28.2}\n",
            "{'loss': 0.3409, 'learning_rate': 1.0256410256410257e-06, 'epoch': 28.46}\n",
            "{'loss': 0.3593, 'learning_rate': 8.547008547008548e-07, 'epoch': 28.71}\n",
            "{'loss': 0.336, 'learning_rate': 6.837606837606839e-07, 'epoch': 28.97}\n",
            " 97% 1131/1170 [6:59:31<12:47, 19.68s/it][INFO|trainer.py:2443] 2022-04-28 09:05:47,818 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 09:05:47,818 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 09:05:47,818 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:19,  1.67it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.17it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.03s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.09s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:33,  1.14s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.18s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.20s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:31,  1.20s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.22s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:30,  1.25s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.23s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:26,  1.22s/it]\u001b[A\n",
            " 40% 14/35 [00:16<00:25,  1.23s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.23s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.20s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.21s/it]\u001b[A\n",
            " 51% 18/35 [00:20<00:20,  1.23s/it]\u001b[A\n",
            " 54% 19/35 [00:22<00:18,  1.18s/it]\u001b[A\n",
            " 57% 20/35 [00:23<00:17,  1.16s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.17s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:14,  1.15s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.18s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:12,  1.18s/it]\u001b[A\n",
            " 71% 25/35 [00:29<00:11,  1.18s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.15s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.16s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.18s/it]\u001b[A\n",
            " 83% 29/35 [00:33<00:07,  1.22s/it]\u001b[A\n",
            " 86% 30/35 [00:34<00:05,  1.19s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.18s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.17s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.16s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.20s/it]\u001b[A\n",
            "100% 35/35 [00:40<00:00,  1.21s/it]\u001b[A\n",
            "{'eval_loss': 0.5154244303703308, 'eval_accuracy': 0.9214285714285714, 'eval_runtime': 42.3373, 'eval_samples_per_second': 3.307, 'eval_steps_per_second': 0.827, 'epoch': 28.99}\n",
            "\n",
            " 97% 1131/1170 [7:00:21<12:47, 19.68s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 09:06:30,157 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1131\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 09:06:30,158 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1131/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 09:06:39,703 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1131/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 09:06:39,704 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1131/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 09:06:59,774 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-1092] due to args.save_total_limit\n",
            "{'loss': 0.3843, 'learning_rate': 5.128205128205128e-07, 'epoch': 29.23}\n",
            "{'loss': 0.3308, 'learning_rate': 3.4188034188034194e-07, 'epoch': 29.48}\n",
            "{'loss': 0.3419, 'learning_rate': 1.7094017094017097e-07, 'epoch': 29.74}\n",
            "{'loss': 0.3808, 'learning_rate': 0.0, 'epoch': 29.99}\n",
            "100% 1170/1170 [7:13:39<00:00, 19.59s/it][INFO|trainer.py:2443] 2022-04-28 09:19:48,582 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 09:19:48,582 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 09:19:48,582 >>   Batch size = 4\n",
            "\n",
            "  0% 0/35 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/35 [00:01<00:19,  1.65it/s]\u001b[A\n",
            "  9% 3/35 [00:02<00:27,  1.18it/s]\u001b[A\n",
            " 11% 4/35 [00:03<00:31,  1.01s/it]\u001b[A\n",
            " 14% 5/35 [00:04<00:32,  1.07s/it]\u001b[A\n",
            " 17% 6/35 [00:06<00:32,  1.11s/it]\u001b[A\n",
            " 20% 7/35 [00:07<00:32,  1.15s/it]\u001b[A\n",
            " 23% 8/35 [00:08<00:32,  1.19s/it]\u001b[A\n",
            " 26% 9/35 [00:09<00:30,  1.19s/it]\u001b[A\n",
            " 29% 10/35 [00:11<00:30,  1.21s/it]\u001b[A\n",
            " 31% 11/35 [00:12<00:29,  1.24s/it]\u001b[A\n",
            " 34% 12/35 [00:13<00:28,  1.23s/it]\u001b[A\n",
            " 37% 13/35 [00:14<00:26,  1.22s/it]\u001b[A\n",
            " 40% 14/35 [00:15<00:25,  1.23s/it]\u001b[A\n",
            " 43% 15/35 [00:17<00:24,  1.22s/it]\u001b[A\n",
            " 46% 16/35 [00:18<00:22,  1.20s/it]\u001b[A\n",
            " 49% 17/35 [00:19<00:21,  1.21s/it]\u001b[A\n",
            " 51% 18/35 [00:20<00:20,  1.22s/it]\u001b[A\n",
            " 54% 19/35 [00:21<00:18,  1.18s/it]\u001b[A\n",
            " 57% 20/35 [00:22<00:17,  1.16s/it]\u001b[A\n",
            " 60% 21/35 [00:24<00:16,  1.17s/it]\u001b[A\n",
            " 63% 22/35 [00:25<00:14,  1.15s/it]\u001b[A\n",
            " 66% 23/35 [00:26<00:14,  1.18s/it]\u001b[A\n",
            " 69% 24/35 [00:27<00:13,  1.19s/it]\u001b[A\n",
            " 71% 25/35 [00:28<00:11,  1.17s/it]\u001b[A\n",
            " 74% 26/35 [00:30<00:10,  1.16s/it]\u001b[A\n",
            " 77% 27/35 [00:31<00:09,  1.16s/it]\u001b[A\n",
            " 80% 28/35 [00:32<00:08,  1.19s/it]\u001b[A\n",
            " 83% 29/35 [00:33<00:07,  1.23s/it]\u001b[A\n",
            " 86% 30/35 [00:34<00:06,  1.21s/it]\u001b[A\n",
            " 89% 31/35 [00:36<00:04,  1.18s/it]\u001b[A\n",
            " 91% 32/35 [00:37<00:03,  1.17s/it]\u001b[A\n",
            " 94% 33/35 [00:38<00:02,  1.16s/it]\u001b[A\n",
            " 97% 34/35 [00:39<00:01,  1.20s/it]\u001b[A\n",
            "100% 35/35 [00:40<00:00,  1.20s/it]\u001b[A\n",
            "{'eval_loss': 0.5145094990730286, 'eval_accuracy': 0.9285714285714286, 'eval_runtime': 42.2211, 'eval_samples_per_second': 3.316, 'eval_steps_per_second': 0.829, 'epoch': 29.99}\n",
            "\n",
            "100% 1170/1170 [7:14:22<00:00, 19.59s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:2193] 2022-04-28 09:20:30,804 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1170\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 09:20:30,805 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1170/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 09:20:40,707 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1170/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 09:20:40,708 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-1170/preprocessor_config.json\n",
            "[INFO|trainer.py:2271] 2022-04-28 09:21:00,782 >> Deleting older checkpoint [crop14-small_vit-huge-patch14-224-in21k/checkpoint-1131] due to args.save_total_limit\n",
            "[INFO|trainer.py:1557] 2022-04-28 09:21:00,993 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:1566] 2022-04-28 09:21:00,993 >> Loading best model from ./crop14-small_vit-huge-patch14-224-in21k/checkpoint-975 (score: 0.5076918601989746).\n",
            "{'train_runtime': 26103.443, 'train_samples_per_second': 1.448, 'train_steps_per_second': 0.045, 'train_loss': 0.776876440211239, 'epoch': 29.99}\n",
            "100% 1170/1170 [7:15:03<00:00, 22.31s/it]\n",
            "[INFO|trainer.py:2193] 2022-04-28 09:21:12,407 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 09:21:12,408 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 09:21:21,970 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 09:21:21,970 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/preprocessor_config.json\n",
            "[INFO|trainer.py:2193] 2022-04-28 09:21:21,971 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 09:21:21,972 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 09:21:32,151 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 09:21:32,156 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/preprocessor_config.json\n",
            "Several commits (2) will be pushed upstream.\n",
            "04/28/2022 09:22:55 - WARNING - huggingface_hub.repository - Several commits (2) will be pushed upstream.\n",
            "The progress bars may be unreliable.\n",
            "04/28/2022 09:22:55 - WARNING - huggingface_hub.repository - The progress bars may be unreliable.\n",
            "Upload file pytorch_model.bin:   0% 47.3k/2.35G [00:01<15:34:30, 45.0kB/s]\n",
            "Upload file pytorch_model.bin: 100% 2.35G/2.35G [29:57<00:00, 1.41MB/s]To https://huggingface.co/gary109/crop14-small_vit-huge-patch14-224-in21k\n",
            "   9d5f4ed..ed464e2  main -> main\n",
            "\n",
            "04/28/2022 09:53:01 - WARNING - huggingface_hub.repository - To https://huggingface.co/gary109/crop14-small_vit-huge-patch14-224-in21k\n",
            "   9d5f4ed..ed464e2  main -> main\n",
            "\n",
            "Upload file pytorch_model.bin: 100% 2.35G/2.35G [30:00<00:00, 1.40MB/s]\n",
            "\n",
            "Upload file runs/Apr28_02-00-35_4c65f37d1894/events.out.tfevents.1651111568.4c65f37d1894.452.0: 100% 31.7k/31.7k [29:59<00:00, 16.1B/s]\u001b[A\n",
            "Upload file runs/Apr28_02-00-35_4c65f37d1894/events.out.tfevents.1651111568.4c65f37d1894.452.0: 100% 31.7k/31.7k [29:59<00:00, 16.1B/s]\n",
            "To https://huggingface.co/gary109/crop14-small_vit-huge-patch14-224-in21k\n",
            "   ed464e2..b57824e  main -> main\n",
            "\n",
            "04/28/2022 09:53:11 - WARNING - huggingface_hub.repository - To https://huggingface.co/gary109/crop14-small_vit-huge-patch14-224-in21k\n",
            "   ed464e2..b57824e  main -> main\n",
            "\n",
            "***** train metrics *****\n",
            "  epoch                    =      29.99\n",
            "  train_loss               =     0.7769\n",
            "  train_runtime            = 7:15:03.44\n",
            "  train_samples_per_second =      1.448\n",
            "  train_steps_per_second   =      0.045\n",
            "[INFO|trainer.py:2443] 2022-04-28 09:53:14,005 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2445] 2022-04-28 09:53:14,005 >>   Num examples = 140\n",
            "[INFO|trainer.py:2448] 2022-04-28 09:53:14,006 >>   Batch size = 4\n",
            "100% 35/35 [00:39<00:00,  1.12s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =      29.99\n",
            "  eval_accuracy           =     0.9429\n",
            "  eval_loss               =     0.5077\n",
            "  eval_runtime            = 0:00:41.11\n",
            "  eval_samples_per_second =      3.405\n",
            "  eval_steps_per_second   =      0.851\n",
            "[INFO|trainer.py:2193] 2022-04-28 09:53:55,162 >> Saving model checkpoint to ./crop14-small_vit-huge-patch14-224-in21k/\n",
            "[INFO|configuration_utils.py:446] 2022-04-28 09:53:55,162 >> Configuration saved in ./crop14-small_vit-huge-patch14-224-in21k/config.json\n",
            "[INFO|modeling_utils.py:1468] 2022-04-28 09:54:05,082 >> Model weights saved in ./crop14-small_vit-huge-patch14-224-in21k/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-04-28 09:54:05,083 >> Feature extractor saved in ./crop14-small_vit-huge-patch14-224-in21k/preprocessor_config.json\n",
            "Upload file runs/Apr28_02-00-35_4c65f37d1894/events.out.tfevents.1651139635.4c65f37d1894.452.2: 100% 363/363 [00:00<?, ?B/s]To https://huggingface.co/gary109/crop14-small_vit-huge-patch14-224-in21k\n",
            "   b57824e..2143ac1  main -> main\n",
            "\n",
            "04/28/2022 09:54:35 - WARNING - huggingface_hub.repository - To https://huggingface.co/gary109/crop14-small_vit-huge-patch14-224-in21k\n",
            "   b57824e..2143ac1  main -> main\n",
            "\n",
            "Upload file runs/Apr28_02-00-35_4c65f37d1894/events.out.tfevents.1651139635.4c65f37d1894.452.2: 100% 363/363 [00:02<?, ?B/s]\n",
            "To https://huggingface.co/gary109/crop14-small_vit-huge-patch14-224-in21k\n",
            "   2143ac1..0518518  main -> main\n",
            "\n",
            "04/28/2022 09:54:44 - WARNING - huggingface_hub.repository - To https://huggingface.co/gary109/crop14-small_vit-huge-patch14-224-in21k\n",
            "   2143ac1..0518518  main -> main\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gary109/crop14-small_pretrain_vit-base-mim\n",
        "---\n",
        "- crop14_balance_ft_pretrain_vit-base-mim\n"
      ],
      "metadata": {
        "id": "DQpagmjCuH9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch run_image_classification.py \\\n",
        "    --model_name_or_path \"gary109/crop14-small_pretrain_vit-base-mim\" \\\n",
        "    --dataset_name \"gary109/crop14_balance\" \\\n",
        "    --output_dir=\"crop14_balance_ft_pretrain_vit-base-mim/\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id=\"crop14_balance_ft_pretrain_vit-base-mim\" \\\n",
        "    --hub_token=\"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\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 30 \\\n",
        "    --per_device_train_batch_size 64 \\\n",
        "    --per_device_eval_batch_size 64 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 1 \\\n",
        "    --use_auth_token=\"True\" \\\n",
        "    --seed 1337 \n",
        "\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da4a8270-fc05-4b64-d21f-cb2c33340fa6",
        "id": "XGs7yqz9uH-L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/training_args.py:978: FutureWarning: `--push_to_hub_model_id` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case gary109/crop14_balance_ft_pretrain_vit-base-mim).\n",
            "  FutureWarning,\n",
            "05/07/2022 10:23:48 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/07/2022 10:23:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=gary109/crop14_balance_ft_pretrain_vit-base-mim,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=crop14_balance_ft_pretrain_vit-base-mim/runs/May07_10-23-47_1bd9825ee9f8,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=crop14_balance_ft_pretrain_vit-base-mim/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=64,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=True,\n",
            "push_to_hub_model_id=crop14_balance_ft_pretrain_vit-base-mim,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=False,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=crop14_balance_ft_pretrain_vit-base-mim/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=1,\n",
            "seed=1337,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "05/07/2022 10:23:51 - WARNING - datasets.builder - Using custom data configuration gary109--crop14_balance-5eb0f83fb5c0cc1b\n",
            "05/07/2022 10:23:51 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/gary109___parquet/gary109--crop14_balance-5eb0f83fb5c0cc1b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
            "100% 2/2 [00:00<00:00,  2.34it/s]\n",
            "05/07/2022 10:23:52 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/gary109___parquet/gary109--crop14_balance-5eb0f83fb5c0cc1b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-94012183d1655706.arrow\n",
            "05/07/2022 10:23:52 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/gary109___parquet/gary109--crop14_balance-5eb0f83fb5c0cc1b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-4697d8902d64c83e.arrow\n",
            "[INFO|configuration_utils.py:659] 2022-05-07 10:23:54,860 >> loading configuration file https://huggingface.co/gary109/crop14-small_pretrain_vit-base-mim/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fbde83ff93e90086f07745148dc150105d39a6b2bf0c42becf5d2a6599fc84d3.ecaf01345faacc624eea5f1b32b721433903539046f4bcfeb349a532febd036e\n",
            "[INFO|configuration_utils.py:708] 2022-05-07 10:23:54,861 >> Model config ViTConfig {\n",
            "  \"_name_or_path\": \"gary109/crop14-small_pretrain_vit-base-mim\",\n",
            "  \"architectures\": [\n",
            "    \"ViTForMaskedImageModeling\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"finetuning_task\": \"image-classification\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"peanut\",\n",
            "    \"1\": \"corn\",\n",
            "    \"10\": \"carrot\",\n",
            "    \"11\": \"dragonfruit\",\n",
            "    \"12\": \"pumpkin\",\n",
            "    \"13\": \"tomato\",\n",
            "    \"2\": \"bareland\",\n",
            "    \"3\": \"pineapple\",\n",
            "    \"4\": \"rice\",\n",
            "    \"5\": \"garlic\",\n",
            "    \"6\": \"soybean\",\n",
            "    \"7\": \"guava\",\n",
            "    \"8\": \"banana\",\n",
            "    \"9\": \"sugarcane\"\n",
            "  },\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"banana\": \"8\",\n",
            "    \"bareland\": \"2\",\n",
            "    \"carrot\": \"10\",\n",
            "    \"corn\": \"1\",\n",
            "    \"dragonfruit\": \"11\",\n",
            "    \"garlic\": \"5\",\n",
            "    \"guava\": \"7\",\n",
            "    \"peanut\": \"0\",\n",
            "    \"pineapple\": \"3\",\n",
            "    \"pumpkin\": \"12\",\n",
            "    \"rice\": \"4\",\n",
            "    \"soybean\": \"6\",\n",
            "    \"sugarcane\": \"9\",\n",
            "    \"tomato\": \"13\"\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"qkv_bias\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.19.0.dev0\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1953] 2022-05-07 10:23:55,630 >> loading weights file https://huggingface.co/gary109/crop14-small_pretrain_vit-base-mim/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/47800ff70861a7a62364ecc084072b93cb46d11d7fc7f1d28b979dae4373daa6.5086182baab827282581ae9d2a94c93877b62e5c17ae39e9c72d7a78944627fe\n",
            "[WARNING|modeling_utils.py:2255] 2022-05-07 10:23:58,071 >> Some weights of the model checkpoint at gary109/crop14-small_pretrain_vit-base-mim were not used when initializing ViTForImageClassification: ['vit.embeddings.mask_token', 'decoder.0.bias', 'decoder.0.weight']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:2266] 2022-05-07 10:23:58,071 >> Some weights of ViTForImageClassification were not initialized from the model checkpoint at gary109/crop14-small_pretrain_vit-base-mim and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[INFO|feature_extraction_utils.py:465] 2022-05-07 10:23:58,826 >> loading feature extractor configuration file https://huggingface.co/gary109/crop14-small_pretrain_vit-base-mim/resolve/main/preprocessor_config.json from cache at /root/.cache/huggingface/transformers/7df1343436d0bed1f0b889a1ca4f02861608f39a3ab494bc7193506f2e8533c8.08655ed7bb323a517686dca7a2716a9fa479de0fa6b11dcf6906fa61e45c4490\n",
            "[INFO|feature_extraction_utils.py:501] 2022-05-07 10:23:58,828 >> Feature extractor ViTFeatureExtractor {\n",
            "  \"do_normalize\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
            "  \"image_mean\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"image_std\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"resample\": 2,\n",
            "  \"size\": 224\n",
            "}\n",
            "\n",
            "Cloning https://huggingface.co/gary109/crop14_balance_ft_pretrain_vit-base-mim into local empty directory.\n",
            "05/07/2022 10:24:17 - WARNING - huggingface_hub.repository - Cloning https://huggingface.co/gary109/crop14_balance_ft_pretrain_vit-base-mim into local empty directory.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1294] 2022-05-07 10:24:22,736 >> ***** Running training *****\n",
            "[INFO|trainer.py:1295] 2022-05-07 10:24:22,736 >>   Num examples = 18937\n",
            "[INFO|trainer.py:1296] 2022-05-07 10:24:22,736 >>   Num Epochs = 30\n",
            "[INFO|trainer.py:1297] 2022-05-07 10:24:22,736 >>   Instantaneous batch size per device = 64\n",
            "[INFO|trainer.py:1298] 2022-05-07 10:24:22,736 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1299] 2022-05-07 10:24:22,736 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1300] 2022-05-07 10:24:22,736 >>   Total optimization steps = 8880\n",
            "[INFO|integrations.py:577] 2022-05-07 10:24:22,748 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgary109\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220507_102423-z0my6lf6\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcrop14_balance_ft_pretrain_vit-base-mim/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/gary109/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/gary109/huggingface/runs/z0my6lf6\u001b[0m\n",
            "{'loss': 2.6109, 'learning_rate': 1.9977477477477477e-05, 'epoch': 0.03}\n",
            "  0% 20/8880 [04:07<29:56:14, 12.16s/it]{'loss': 2.5364, 'learning_rate': 1.995495495495496e-05, 'epoch': 0.07}\n",
            "{'loss': 2.4193, 'learning_rate': 1.9932432432432434e-05, 'epoch': 0.1}\n",
            "{'loss': 2.3134, 'learning_rate': 1.9909909909909912e-05, 'epoch': 0.14}\n",
            "{'loss': 2.1967, 'learning_rate': 1.9887387387387388e-05, 'epoch': 0.17}\n",
            "  1% 60/8880 [12:17<30:16:55, 12.36s/it]{'loss': 2.1032, 'learning_rate': 1.9864864864864866e-05, 'epoch': 0.2}\n",
            "  1% 70/8880 [14:19<29:52:33, 12.21s/it]{'loss': 1.9431, 'learning_rate': 1.9842342342342345e-05, 'epoch': 0.24}\n",
            "                                        {'loss': 1.8523, 'learning_rate': 1.981981981981982e-05, 'epoch': 0.27}\n",
            "{'loss': 1.7456, 'learning_rate': 1.97972972972973e-05, 'epoch': 0.3}\n",
            "{'loss': 1.6606, 'learning_rate': 1.9774774774774777e-05, 'epoch': 0.34}\n",
            "{'loss': 1.5832, 'learning_rate': 1.9752252252252252e-05, 'epoch': 0.37}\n",
            "  1% 120/8880 [24:29<29:22:35, 12.07s/it]{'loss': 1.4825, 'learning_rate': 1.972972972972973e-05, 'epoch': 0.41}\n",
            "{'loss': 1.39, 'learning_rate': 1.970720720720721e-05, 'epoch': 0.44}\n",
            "{'loss': 1.3155, 'learning_rate': 1.9684684684684688e-05, 'epoch': 0.47}\n",
            "{'loss': 1.277, 'learning_rate': 1.9662162162162163e-05, 'epoch': 0.51}\n",
            "{'loss': 1.1919, 'learning_rate': 1.963963963963964e-05, 'epoch': 0.54}\n",
            "                                         {'loss': 1.1534, 'learning_rate': 1.961711711711712e-05, 'epoch': 0.57}\n",
            "{'loss': 1.1367, 'learning_rate': 1.9594594594594595e-05, 'epoch': 0.61}\n",
            "  2% 190/8880 [38:46<29:28:33, 12.21s/it]{'loss': 1.0622, 'learning_rate': 1.9572072072072074e-05, 'epoch': 0.64}\n",
            "{'loss': 1.0091, 'learning_rate': 1.9549549549549552e-05, 'epoch': 0.68}\n",
            "{'loss': 0.9208, 'learning_rate': 1.9527027027027027e-05, 'epoch': 0.71}\n",
            "{'loss': 0.9486, 'learning_rate': 1.9504504504504506e-05, 'epoch': 0.74}\n",
            "  3% 230/8880 [46:57<29:39:52, 12.35s/it]{'loss': 0.8734, 'learning_rate': 1.9481981981981985e-05, 'epoch': 0.78}\n",
            "{'loss': 0.8615, 'learning_rate': 1.9459459459459463e-05, 'epoch': 0.81}\n",
            "{'loss': 0.7864, 'learning_rate': 1.9436936936936938e-05, 'epoch': 0.84}\n",
            "  3% 260/8880 [53:05<29:42:28, 12.41s/it]{'loss': 0.8112, 'learning_rate': 1.9414414414414417e-05, 'epoch': 0.88}\n",
            "{'loss': 0.7906, 'learning_rate': 1.9391891891891895e-05, 'epoch': 0.91}\n",
            "{'loss': 0.7245, 'learning_rate': 1.936936936936937e-05, 'epoch': 0.95}\n",
            "  3% 290/8880 [59:14<29:41:17, 12.44s/it]{'loss': 0.7093, 'learning_rate': 1.9346846846846846e-05, 'epoch': 0.98}\n",
            "  3% 296/8880 [1:00:28<29:05:24, 12.20s/it][INFO|trainer.py:2463] 2022-05-07 11:24:55,145 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2465] 2022-05-07 11:24:55,145 >>   Num examples = 2105\n",
            "[INFO|trainer.py:2468] 2022-05-07 11:24:55,145 >>   Batch size = 64\n",
            "\n",
            "  0% 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/33 [00:12<03:19,  6.43s/it]\u001b[A\n",
            "  9% 3/33 [00:25<04:28,  8.94s/it]\u001b[A\n",
            " 12% 4/33 [00:37<04:57, 10.25s/it]\u001b[A\n",
            " 15% 5/33 [00:50<05:08, 11.01s/it]\u001b[A\n",
            " 18% 6/33 [01:01<05:03, 11.24s/it]\u001b[A\n",
            " 21% 7/33 [01:14<05:02, 11.63s/it]\u001b[A\n",
            " 24% 8/33 [01:26<04:56, 11.85s/it]\u001b[A\n",
            " 27% 9/33 [01:38<04:47, 11.96s/it]\u001b[A\n",
            " 30% 10/33 [01:51<04:36, 12.01s/it]\u001b[A\n",
            " 33% 11/33 [02:03<04:24, 12.03s/it]\u001b[A\n",
            " 36% 12/33 [02:14<04:10, 11.94s/it]\u001b[A\n",
            " 39% 13/33 [02:26<03:59, 11.97s/it]\u001b[A\n",
            " 42% 14/33 [02:39<03:48, 12.03s/it]\u001b[A\n",
            " 45% 15/33 [02:50<03:34, 11.92s/it]\u001b[A\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ReadTimeout), entering retry loop.\n",
            "\n",
            " 48% 16/33 [03:02<03:19, 11.76s/it]\u001b[A\n",
            " 52% 17/33 [03:13<03:08, 11.76s/it]\u001b[A\n",
            " 55% 18/33 [03:25<02:57, 11.86s/it]\u001b[A\n",
            " 58% 19/33 [03:37<02:46, 11.86s/it]\u001b[A\n",
            " 61% 20/33 [03:50<02:35, 11.97s/it]\u001b[A\n",
            " 64% 21/33 [04:01<02:23, 11.94s/it]\u001b[A\n",
            " 67% 22/33 [04:14<02:13, 12.11s/it]\u001b[A\n",
            " 70% 23/33 [04:26<02:01, 12.11s/it]\u001b[A\n",
            " 73% 24/33 [04:38<01:48, 12.09s/it]\u001b[A\n",
            " 76% 25/33 [04:50<01:36, 12.12s/it]\u001b[A\n",
            " 79% 26/33 [05:03<01:26, 12.42s/it]\u001b[A\n",
            " 82% 27/33 [05:16<01:14, 12.37s/it]\u001b[A\n",
            " 85% 28/33 [05:28<01:02, 12.44s/it]\u001b[A\n",
            " 88% 29/33 [05:41<00:49, 12.49s/it]\u001b[A\n",
            " 91% 30/33 [05:53<00:37, 12.49s/it]\u001b[A\n",
            " 94% 31/33 [06:05<00:24, 12.35s/it]\u001b[A\n",
            " 97% 32/33 [06:18<00:12, 12.33s/it]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.6975904703140259, 'eval_accuracy': 0.9173396674584323, 'eval_runtime': 401.3321, 'eval_samples_per_second': 5.245, 'eval_steps_per_second': 0.082, 'epoch': 1.0}\n",
            "  3% 296/8880 [1:07:09<29:05:24, 12.20s/it]\n",
            "100% 33/33 [06:28<00:00, 11.72s/it]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2213] 2022-05-07 11:31:36,482 >> Saving model checkpoint to crop14_balance_ft_pretrain_vit-base-mim/checkpoint-296\n",
            "[INFO|configuration_utils.py:446] 2022-05-07 11:31:36,484 >> Configuration saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-296/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-07 11:31:37,204 >> Model weights saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-296/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-07 11:31:37,205 >> Feature extractor saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-296/preprocessor_config.json\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-07 11:31:40,572 >> Feature extractor saved in crop14_balance_ft_pretrain_vit-base-mim/preprocessor_config.json\n",
            "{'loss': 0.7051, 'learning_rate': 1.9324324324324328e-05, 'epoch': 1.01}\n",
            "  3% 310/8880 [1:10:30<34:55:54, 14.67s/it]{'loss': 0.6746, 'learning_rate': 1.9301801801801803e-05, 'epoch': 1.05}\n",
            "{'loss': 0.6775, 'learning_rate': 1.927927927927928e-05, 'epoch': 1.08}\n",
            "{'loss': 0.6237, 'learning_rate': 1.925675675675676e-05, 'epoch': 1.11}\n",
            "  4% 340/8880 [1:16:52<29:35:52, 12.48s/it]{'loss': 0.6431, 'learning_rate': 1.9234234234234235e-05, 'epoch': 1.15}\n",
            "{'loss': 0.6247, 'learning_rate': 1.9211711711711714e-05, 'epoch': 1.18}\n",
            "{'loss': 0.6018, 'learning_rate': 1.918918918918919e-05, 'epoch': 1.22}\n",
            "  4% 370/8880 [1:23:04<29:21:01, 12.42s/it]{'loss': 0.5695, 'learning_rate': 1.916666666666667e-05, 'epoch': 1.25}\n",
            "{'loss': 0.564, 'learning_rate': 1.9144144144144146e-05, 'epoch': 1.28}\n",
            "{'loss': 0.5653, 'learning_rate': 1.912162162162162e-05, 'epoch': 1.32}\n",
            "{'loss': 0.5388, 'learning_rate': 1.90990990990991e-05, 'epoch': 1.35}\n",
            "{'loss': 0.5662, 'learning_rate': 1.9076576576576578e-05, 'epoch': 1.39}\n",
            "  5% 420/8880 [1:33:16<28:53:29, 12.29s/it]{'loss': 0.5145, 'learning_rate': 1.9054054054054057e-05, 'epoch': 1.42}\n",
            "  5% 430/8880 [1:35:18<28:15:34, 12.04s/it]{'loss': 0.5403, 'learning_rate': 1.9031531531531532e-05, 'epoch': 1.45}\n",
            "{'loss': 0.5018, 'learning_rate': 1.900900900900901e-05, 'epoch': 1.49}\n",
            "{'loss': 0.5346, 'learning_rate': 1.898648648648649e-05, 'epoch': 1.52}\n",
            "  5% 460/8880 [1:41:26<28:50:58, 12.33s/it]{'loss': 0.5092, 'learning_rate': 1.8963963963963964e-05, 'epoch': 1.55}\n",
            "{'loss': 0.4904, 'learning_rate': 1.8941441441441443e-05, 'epoch': 1.59}\n",
            "{'loss': 0.4678, 'learning_rate': 1.891891891891892e-05, 'epoch': 1.62}\n",
            "  6% 490/8880 [1:47:34<28:38:06, 12.29s/it]{'loss': 0.487, 'learning_rate': 1.8896396396396396e-05, 'epoch': 1.66}\n",
            "{'loss': 0.4951, 'learning_rate': 1.8873873873873875e-05, 'epoch': 1.69}\n",
            "  6% 510/8880 [1:51:44<28:39:40, 12.33s/it]{'loss': 0.4378, 'learning_rate': 1.8851351351351353e-05, 'epoch': 1.72}\n",
            "{'loss': 0.4578, 'learning_rate': 1.8828828828828832e-05, 'epoch': 1.76}\n",
            "  6% 530/8880 [1:55:50<28:41:51, 12.37s/it]{'loss': 0.4465, 'learning_rate': 1.8806306306306307e-05, 'epoch': 1.79}\n",
            "                                           {'loss': 0.4344, 'learning_rate': 1.8783783783783786e-05, 'epoch': 1.82}\n",
            "  6% 540/8880 [1:57:54<28:45:13, 12.41s/it]\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ReadTimeout), entering retry loop.\n",
            "  6% 550/8880 [1:59:55<27:56:17, 12.07s/it]{'loss': 0.4214, 'learning_rate': 1.8761261261261264e-05, 'epoch': 1.86}\n",
            "{'loss': 0.4383, 'learning_rate': 1.873873873873874e-05, 'epoch': 1.89}\n",
            "  6% 570/8880 [2:04:01<28:05:24, 12.17s/it]{'loss': 0.4314, 'learning_rate': 1.8716216216216218e-05, 'epoch': 1.93}\n",
            "{'loss': 0.4435, 'learning_rate': 1.8693693693693696e-05, 'epoch': 1.96}\n",
            "{'loss': 0.4199, 'learning_rate': 1.867117117117117e-05, 'epoch': 1.99}\n",
            "  7% 592/8880 [2:08:29<27:26:26, 11.92s/it][INFO|trainer.py:2463] 2022-05-07 12:32:56,198 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2465] 2022-05-07 12:32:56,198 >>   Num examples = 2105\n",
            "[INFO|trainer.py:2468] 2022-05-07 12:32:56,198 >>   Batch size = 64\n",
            "\n",
            "  0% 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/33 [00:12<03:13,  6.26s/it]\u001b[A\n",
            "  9% 3/33 [00:24<04:18,  8.63s/it]\u001b[A\n",
            " 12% 4/33 [00:36<04:47,  9.92s/it]\u001b[A\n",
            " 15% 5/33 [00:48<05:00, 10.74s/it]\u001b[A\n",
            " 18% 6/33 [01:00<04:58, 11.07s/it]\u001b[A\n",
            " 21% 7/33 [01:12<04:57, 11.43s/it]\u001b[A\n",
            " 24% 8/33 [01:24<04:52, 11.69s/it]\u001b[A\n",
            " 27% 9/33 [01:37<04:44, 11.86s/it]\u001b[A\n",
            " 30% 10/33 [01:49<04:35, 11.96s/it]\u001b[A\n",
            " 33% 11/33 [02:01<04:24, 12.01s/it]\u001b[A\n",
            " 36% 12/33 [02:13<04:11, 11.98s/it]\u001b[A\n",
            " 39% 13/33 [02:25<04:00, 12.02s/it]\u001b[A\n",
            " 42% 14/33 [02:37<03:49, 12.07s/it]\u001b[A\n",
            " 45% 15/33 [02:49<03:35, 11.96s/it]\u001b[A\n",
            " 48% 16/33 [03:00<03:20, 11.79s/it]\u001b[A\n",
            " 52% 17/33 [03:12<03:07, 11.75s/it]\u001b[A\n",
            " 55% 18/33 [03:24<02:57, 11.84s/it]\u001b[A\n",
            " 58% 19/33 [03:36<02:46, 11.88s/it]\u001b[A\n",
            " 61% 20/33 [03:48<02:36, 12.04s/it]\u001b[A\n",
            " 64% 21/33 [04:00<02:23, 11.98s/it]\u001b[A\n",
            " 67% 22/33 [04:13<02:13, 12.11s/it]\u001b[A\n",
            " 70% 23/33 [04:25<02:01, 12.16s/it]\u001b[A\n",
            " 73% 24/33 [04:37<01:49, 12.17s/it]\u001b[A\n",
            " 76% 25/33 [04:49<01:37, 12.13s/it]\u001b[A\n",
            " 79% 26/33 [05:02<01:26, 12.35s/it]\u001b[A\n",
            " 82% 27/33 [05:14<01:13, 12.30s/it]\u001b[A\n",
            " 85% 28/33 [05:27<01:02, 12.43s/it]\u001b[A\n",
            " 88% 29/33 [05:39<00:49, 12.41s/it]\u001b[A\n",
            " 91% 30/33 [05:52<00:37, 12.40s/it]\u001b[A\n",
            " 94% 31/33 [06:04<00:24, 12.32s/it]\u001b[A\n",
            " 97% 32/33 [06:16<00:12, 12.34s/it]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.3964442312717438, 'eval_accuracy': 0.9429928741092637, 'eval_runtime': 399.9861, 'eval_samples_per_second': 5.263, 'eval_steps_per_second': 0.083, 'epoch': 2.0}\n",
            "  7% 592/8880 [2:15:09<27:26:26, 11.92s/it]\n",
            "100% 33/33 [06:27<00:00, 11.74s/it]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2213] 2022-05-07 12:39:36,188 >> Saving model checkpoint to crop14_balance_ft_pretrain_vit-base-mim/checkpoint-592\n",
            "[INFO|configuration_utils.py:446] 2022-05-07 12:39:36,189 >> Configuration saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-592/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-07 12:39:37,171 >> Model weights saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-592/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-07 12:39:37,171 >> Feature extractor saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-592/preprocessor_config.json\n",
            "[INFO|trainer.py:2291] 2022-05-07 12:39:40,148 >> Deleting older checkpoint [crop14_balance_ft_pretrain_vit-base-mim/checkpoint-296] due to args.save_total_limit\n",
            "  7% 600/8880 [2:16:52<50:58:44, 22.16s/it]{'loss': 0.3498, 'learning_rate': 1.864864864864865e-05, 'epoch': 2.03}\n",
            "  7% 610/8880 [2:18:57<29:22:17, 12.79s/it]{'loss': 0.389, 'learning_rate': 1.862612612612613e-05, 'epoch': 2.06}\n",
            "{'loss': 0.4253, 'learning_rate': 1.8603603603603607e-05, 'epoch': 2.09}\n",
            "  7% 630/8880 [2:23:03<28:25:02, 12.40s/it]{'loss': 0.3736, 'learning_rate': 1.8581081081081082e-05, 'epoch': 2.13}\n",
            "{'loss': 0.3976, 'learning_rate': 1.855855855855856e-05, 'epoch': 2.16}\n",
            "{'loss': 0.3724, 'learning_rate': 1.853603603603604e-05, 'epoch': 2.2}\n",
            "  7% 660/8880 [2:29:11<28:02:58, 12.28s/it]{'loss': 0.3817, 'learning_rate': 1.8513513513513515e-05, 'epoch': 2.23}\n",
            "{'loss': 0.3361, 'learning_rate': 1.8490990990990993e-05, 'epoch': 2.26}\n",
            "{'loss': 0.3542, 'learning_rate': 1.8468468468468472e-05, 'epoch': 2.3}\n",
            "{'loss': 0.366, 'learning_rate': 1.8445945945945947e-05, 'epoch': 2.33}\n",
            "  8% 700/8880 [2:37:25<28:22:53, 12.49s/it]{'loss': 0.3548, 'learning_rate': 1.8423423423423425e-05, 'epoch': 2.36}\n",
            "  8% 710/8880 [2:39:28<27:50:47, 12.27s/it]{'loss': 0.3458, 'learning_rate': 1.84009009009009e-05, 'epoch': 2.4}\n",
            "  8% 720/8880 [2:41:30<27:52:50, 12.30s/it]{'loss': 0.3163, 'learning_rate': 1.8378378378378383e-05, 'epoch': 2.43}\n",
            "{'loss': 0.3233, 'learning_rate': 1.8355855855855858e-05, 'epoch': 2.47}\n",
            "{'loss': 0.3498, 'learning_rate': 1.8333333333333333e-05, 'epoch': 2.5}\n",
            "{'loss': 0.3425, 'learning_rate': 1.831081081081081e-05, 'epoch': 2.53}\n",
            "  9% 760/8880 [2:49:41<27:46:37, 12.31s/it]{'loss': 0.3587, 'learning_rate': 1.828828828828829e-05, 'epoch': 2.57}\n",
            "{'loss': 0.3497, 'learning_rate': 1.826576576576577e-05, 'epoch': 2.6}\n",
            "  9% 780/8880 [2:53:50<28:03:51, 12.47s/it]{'loss': 0.3235, 'learning_rate': 1.8243243243243244e-05, 'epoch': 2.64}\n",
            "{'loss': 0.3257, 'learning_rate': 1.8220720720720722e-05, 'epoch': 2.67}\n",
            "{'loss': 0.3273, 'learning_rate': 1.81981981981982e-05, 'epoch': 2.7}\n",
            "  9% 810/8880 [3:00:00<27:56:04, 12.46s/it]{'loss': 0.3278, 'learning_rate': 1.8175675675675676e-05, 'epoch': 2.74}\n",
            "{'loss': 0.2792, 'learning_rate': 1.8153153153153154e-05, 'epoch': 2.77}\n",
            "  9% 830/8880 [3:04:05<27:20:26, 12.23s/it]{'loss': 0.2786, 'learning_rate': 1.8130630630630633e-05, 'epoch': 2.8}\n",
            "{'loss': 0.3198, 'learning_rate': 1.8108108108108108e-05, 'epoch': 2.84}\n",
            " 10% 850/8880 [3:08:10<27:13:18, 12.20s/it]{'loss': 0.3157, 'learning_rate': 1.8085585585585587e-05, 'epoch': 2.87}\n",
            " 10% 860/8880 [3:10:13<27:35:16, 12.38s/it]{'loss': 0.2979, 'learning_rate': 1.8063063063063065e-05, 'epoch': 2.91}\n",
            "{'loss': 0.2968, 'learning_rate': 1.804054054054054e-05, 'epoch': 2.94}\n",
            "{'loss': 0.2917, 'learning_rate': 1.801801801801802e-05, 'epoch': 2.97}\n",
            " 10% 888/8880 [3:15:53<25:59:50, 11.71s/it][INFO|trainer.py:2463] 2022-05-07 13:40:20,393 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2465] 2022-05-07 13:40:20,393 >>   Num examples = 2105\n",
            "[INFO|trainer.py:2468] 2022-05-07 13:40:20,393 >>   Batch size = 64\n",
            "\n",
            "  0% 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/33 [00:12<03:17,  6.37s/it]\u001b[A\n",
            "  9% 3/33 [00:24<04:22,  8.74s/it]\u001b[A\n",
            " 12% 4/33 [00:37<04:52, 10.07s/it]\u001b[A\n",
            " 15% 5/33 [00:49<05:04, 10.89s/it]\u001b[A\n",
            " 18% 6/33 [01:01<05:02, 11.22s/it]\u001b[A\n",
            " 21% 7/33 [01:13<05:00, 11.54s/it]\u001b[A\n",
            " 24% 8/33 [01:25<04:54, 11.79s/it]\u001b[A\n",
            " 27% 9/33 [01:38<04:48, 12.00s/it]\u001b[A\n",
            " 30% 10/33 [01:50<04:36, 12.03s/it]\u001b[A\n",
            " 33% 11/33 [02:02<04:25, 12.08s/it]\u001b[A\n",
            " 36% 12/33 [02:14<04:12, 12.02s/it]\u001b[A\n",
            " 39% 13/33 [02:26<04:00, 12.02s/it]\u001b[A\n",
            " 42% 14/33 [02:38<03:48, 12.04s/it]\u001b[A\n",
            " 45% 15/33 [02:50<03:35, 11.98s/it]\u001b[A\n",
            " 48% 16/33 [03:02<03:21, 11.83s/it]\u001b[A\n",
            " 52% 17/33 [03:13<03:08, 11.78s/it]\u001b[A\n",
            " 55% 18/33 [03:25<02:58, 11.89s/it]\u001b[A\n",
            " 58% 19/33 [03:37<02:47, 11.95s/it]\u001b[A\n",
            " 61% 20/33 [03:50<02:37, 12.12s/it]\u001b[A\n",
            " 64% 21/33 [04:02<02:24, 12.01s/it]\u001b[A\n",
            " 67% 22/33 [04:14<02:13, 12.13s/it]\u001b[A\n",
            " 70% 23/33 [04:26<02:01, 12.17s/it]\u001b[A\n",
            " 73% 24/33 [04:39<01:49, 12.17s/it]\u001b[A\n",
            " 76% 25/33 [04:51<01:37, 12.14s/it]\u001b[A\n",
            " 79% 26/33 [05:03<01:26, 12.31s/it]\u001b[A\n",
            " 82% 27/33 [05:15<01:13, 12.26s/it]\u001b[A\n",
            " 85% 28/33 [05:28<01:02, 12.40s/it]\u001b[A\n",
            " 88% 29/33 [05:41<00:49, 12.41s/it]\u001b[A\n",
            " 91% 30/33 [05:53<00:37, 12.42s/it]\u001b[A\n",
            " 94% 31/33 [06:05<00:24, 12.31s/it]\u001b[A\n",
            " 97% 32/33 [06:17<00:12, 12.32s/it]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_loss': 0.32410821318626404, 'eval_accuracy': 0.9368171021377673, 'eval_runtime': 401.3336, 'eval_samples_per_second': 5.245, 'eval_steps_per_second': 0.082, 'epoch': 3.0}\n",
            " 10% 888/8880 [3:22:35<25:59:50, 11.71s/it]\n",
            "100% 33/33 [06:28<00:00, 11.76s/it]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2213] 2022-05-07 13:47:01,731 >> Saving model checkpoint to crop14_balance_ft_pretrain_vit-base-mim/checkpoint-888\n",
            "[INFO|configuration_utils.py:446] 2022-05-07 13:47:01,732 >> Configuration saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-888/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-07 13:47:02,440 >> Model weights saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-888/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-07 13:47:02,440 >> Feature extractor saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-888/preprocessor_config.json\n",
            "[INFO|trainer.py:2291] 2022-05-07 13:47:04,671 >> Deleting older checkpoint [crop14_balance_ft_pretrain_vit-base-mim/checkpoint-592] due to args.save_total_limit\n",
            "{'loss': 0.2986, 'learning_rate': 1.7995495495495498e-05, 'epoch': 3.01}\n",
            "                                           {'loss': 0.2706, 'learning_rate': 1.7972972972972976e-05, 'epoch': 3.04}\n",
            "{'loss': 0.2808, 'learning_rate': 1.795045045045045e-05, 'epoch': 3.07}\n",
            " 10% 920/8880 [3:29:09<26:57:24, 12.19s/it]{'loss': 0.2987, 'learning_rate': 1.792792792792793e-05, 'epoch': 3.11}\n",
            " 10% 930/8880 [3:31:11<27:03:58, 12.26s/it]{'loss': 0.3314, 'learning_rate': 1.790540540540541e-05, 'epoch': 3.14}\n",
            "                                           {'loss': 0.2492, 'learning_rate': 1.7882882882882883e-05, 'epoch': 3.18}\n",
            "{'loss': 0.2733, 'learning_rate': 1.7860360360360362e-05, 'epoch': 3.21}\n",
            " 11% 960/8880 [3:37:22<27:14:25, 12.38s/it]{'loss': 0.2563, 'learning_rate': 1.783783783783784e-05, 'epoch': 3.24}\n",
            "{'loss': 0.2272, 'learning_rate': 1.7815315315315316e-05, 'epoch': 3.28}\n",
            "{'loss': 0.2848, 'learning_rate': 1.7792792792792794e-05, 'epoch': 3.31}\n",
            " 11% 990/8880 [3:43:30<26:37:22, 12.15s/it]{'loss': 0.2418, 'learning_rate': 1.7770270270270273e-05, 'epoch': 3.34}\n",
            "{'loss': 0.2755, 'learning_rate': 1.774774774774775e-05, 'epoch': 3.38}\n",
            " 11% 1010/8880 [3:47:34<26:11:50, 11.98s/it]{'loss': 0.23, 'learning_rate': 1.7725225225225227e-05, 'epoch': 3.41}\n",
            " 11% 1020/8880 [3:49:36<26:33:06, 12.16s/it]{'loss': 0.2768, 'learning_rate': 1.7702702702702702e-05, 'epoch': 3.45}\n",
            "{'loss': 0.262, 'learning_rate': 1.7680180180180184e-05, 'epoch': 3.48}\n",
            "{'loss': 0.2311, 'learning_rate': 1.765765765765766e-05, 'epoch': 3.51}\n",
            "{'loss': 0.2535, 'learning_rate': 1.7635135135135137e-05, 'epoch': 3.55}\n",
            "{'loss': 0.2623, 'learning_rate': 1.7612612612612613e-05, 'epoch': 3.58}\n",
            "{'loss': 0.2225, 'learning_rate': 1.759009009009009e-05, 'epoch': 3.61}\n",
            " 12% 1080/8880 [4:01:54<26:42:09, 12.32s/it]{'loss': 0.2269, 'learning_rate': 1.756756756756757e-05, 'epoch': 3.65}\n",
            "{'loss': 0.233, 'learning_rate': 1.7545045045045045e-05, 'epoch': 3.68}\n",
            "{'loss': 0.2071, 'learning_rate': 1.7522522522522523e-05, 'epoch': 3.72}\n",
            "{'loss': 0.2414, 'learning_rate': 1.7500000000000002e-05, 'epoch': 3.75}\n",
            " 13% 1120/8880 [4:10:05<26:26:51, 12.27s/it]{'loss': 0.2008, 'learning_rate': 1.7477477477477477e-05, 'epoch': 3.78}\n",
            " 13% 1130/8880 [4:12:07<26:07:30, 12.14s/it]{'loss': 0.248, 'learning_rate': 1.7454954954954956e-05, 'epoch': 3.82}\n",
            "{'loss': 0.1997, 'learning_rate': 1.7432432432432434e-05, 'epoch': 3.85}\n",
            "{'loss': 0.223, 'learning_rate': 1.7409909909909913e-05, 'epoch': 3.89}\n",
            " 13% 1160/8880 [4:18:20<26:12:06, 12.22s/it]{'loss': 0.2313, 'learning_rate': 1.7387387387387388e-05, 'epoch': 3.92}\n",
            "{'loss': 0.2403, 'learning_rate': 1.7364864864864866e-05, 'epoch': 3.95}\n",
            "{'loss': 0.2003, 'learning_rate': 1.7342342342342345e-05, 'epoch': 3.99}\n",
            " 13% 1184/8880 [4:23:13<25:16:46, 11.83s/it][INFO|trainer.py:2463] 2022-05-07 14:47:40,420 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2465] 2022-05-07 14:47:40,421 >>   Num examples = 2105\n",
            "[INFO|trainer.py:2468] 2022-05-07 14:47:40,421 >>   Batch size = 64\n",
            "\n",
            "  0% 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/33 [00:12<03:13,  6.23s/it]\u001b[A\n",
            "  9% 3/33 [00:24<04:19,  8.66s/it]\u001b[A\n",
            " 12% 4/33 [00:36<04:50, 10.02s/it]\u001b[A\n",
            " 15% 5/33 [00:49<05:02, 10.81s/it]\u001b[A\n",
            " 18% 6/33 [01:00<05:00, 11.14s/it]\u001b[A\n",
            " 21% 7/33 [01:12<04:57, 11.43s/it]\u001b[A\n",
            " 24% 8/33 [01:25<04:53, 11.76s/it]\u001b[A\n",
            " 27% 9/33 [01:37<04:46, 11.94s/it]\u001b[A\n",
            " 30% 10/33 [01:49<04:35, 11.97s/it]\u001b[A\n",
            " 33% 11/33 [02:01<04:24, 12.01s/it]\u001b[A\n",
            " 36% 12/33 [02:13<04:11, 11.95s/it]\u001b[A\n",
            " 39% 13/33 [02:25<04:00, 12.01s/it]\u001b[A\n",
            " 42% 14/33 [02:38<03:48, 12.04s/it]\u001b[A\n",
            " 45% 15/33 [02:49<03:36, 12.00s/it]\u001b[A\n",
            " 48% 16/33 [03:01<03:22, 11.89s/it]\u001b[A\n",
            " 52% 17/33 [03:13<03:10, 11.88s/it]\u001b[A\n",
            " 55% 18/33 [03:25<03:00, 12.01s/it]\u001b[A\n",
            " 58% 19/33 [03:37<02:48, 12.06s/it]\u001b[A\n",
            " 61% 20/33 [03:50<02:38, 12.16s/it]\u001b[A\n",
            " 64% 21/33 [04:02<02:24, 12.08s/it]\u001b[A\n",
            " 67% 22/33 [04:14<02:14, 12.23s/it]\u001b[A\n",
            " 70% 23/33 [04:27<02:02, 12.27s/it]\u001b[A\n",
            " 73% 24/33 [04:39<01:50, 12.29s/it]\u001b[A\n",
            " 76% 25/33 [04:51<01:38, 12.30s/it]\u001b[A\n",
            " 79% 26/33 [05:04<01:27, 12.53s/it]\u001b[A\n",
            " 82% 27/33 [05:17<01:14, 12.42s/it]\u001b[A\n",
            " 85% 28/33 [05:29<01:02, 12.49s/it]\u001b[A\n",
            " 88% 29/33 [05:42<00:49, 12.45s/it]\u001b[A\n",
            " 91% 30/33 [05:54<00:37, 12.48s/it]\u001b[A\n",
            " 94% 31/33 [06:06<00:24, 12.40s/it]\u001b[A\n",
            " 97% 32/33 [06:19<00:12, 12.40s/it]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.2410268932580948, 'eval_accuracy': 0.9520190023752969, 'eval_runtime': 402.3942, 'eval_samples_per_second': 5.231, 'eval_steps_per_second': 0.082, 'epoch': 4.0}\n",
            " 13% 1184/8880 [4:29:56<25:16:46, 11.83s/it]\n",
            "100% 33/33 [06:29<00:00, 11.78s/it]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2213] 2022-05-07 14:54:22,819 >> Saving model checkpoint to crop14_balance_ft_pretrain_vit-base-mim/checkpoint-1184\n",
            "[INFO|configuration_utils.py:446] 2022-05-07 14:54:22,820 >> Configuration saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-1184/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-07 14:54:23,548 >> Model weights saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-1184/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-07 14:54:23,548 >> Feature extractor saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-1184/preprocessor_config.json\n",
            "[INFO|trainer.py:2291] 2022-05-07 14:54:25,741 >> Deleting older checkpoint [crop14_balance_ft_pretrain_vit-base-mim/checkpoint-888] due to args.save_total_limit\n",
            " 13% 1190/8880 [4:31:12<69:17:12, 32.44s/it]{'loss': 0.2075, 'learning_rate': 1.731981981981982e-05, 'epoch': 4.02}\n",
            "                                            {'loss': 0.2107, 'learning_rate': 1.72972972972973e-05, 'epoch': 4.05}\n",
            "{'loss': 0.2104, 'learning_rate': 1.7274774774774777e-05, 'epoch': 4.09}\n",
            " 14% 1220/8880 [4:37:21<26:17:09, 12.35s/it]{'loss': 0.1916, 'learning_rate': 1.7252252252252252e-05, 'epoch': 4.12}\n",
            "{'loss': 0.2392, 'learning_rate': 1.722972972972973e-05, 'epoch': 4.16}\n",
            "{'loss': 0.1649, 'learning_rate': 1.720720720720721e-05, 'epoch': 4.19}\n",
            "{'loss': 0.1949, 'learning_rate': 1.7184684684684688e-05, 'epoch': 4.22}\n",
            "{'loss': 0.2002, 'learning_rate': 1.7162162162162163e-05, 'epoch': 4.26}\n",
            "{'loss': 0.2356, 'learning_rate': 1.713963963963964e-05, 'epoch': 4.29}\n",
            " 14% 1280/8880 [4:49:41<26:08:54, 12.39s/it]{'loss': 0.2036, 'learning_rate': 1.711711711711712e-05, 'epoch': 4.32}\n",
            "{'loss': 0.2059, 'learning_rate': 1.7094594594594595e-05, 'epoch': 4.36}\n",
            "{'loss': 0.1946, 'learning_rate': 1.7072072072072074e-05, 'epoch': 4.39}\n",
            " 15% 1310/8880 [4:55:47<25:34:01, 12.16s/it]{'loss': 0.1884, 'learning_rate': 1.7049549549549552e-05, 'epoch': 4.43}\n",
            " 15% 1320/8880 [4:57:48<25:26:17, 12.11s/it]{'loss': 0.1981, 'learning_rate': 1.7027027027027028e-05, 'epoch': 4.46}\n",
            "{'loss': 0.1894, 'learning_rate': 1.7004504504504506e-05, 'epoch': 4.49}\n",
            "{'loss': 0.1961, 'learning_rate': 1.6981981981981985e-05, 'epoch': 4.53}\n",
            "{'loss': 0.2155, 'learning_rate': 1.6959459459459463e-05, 'epoch': 4.56}\n",
            " 15% 1360/8880 [5:06:03<25:55:33, 12.41s/it]{'loss': 0.2008, 'learning_rate': 1.693693693693694e-05, 'epoch': 4.59}\n",
            "{'loss': 0.2036, 'learning_rate': 1.6914414414414414e-05, 'epoch': 4.63}\n",
            " 16% 1380/8880 [5:10:11<25:49:14, 12.39s/it]{'loss': 0.1747, 'learning_rate': 1.6891891891891896e-05, 'epoch': 4.66}\n",
            "{'loss': 0.2012, 'learning_rate': 1.686936936936937e-05, 'epoch': 4.7}\n",
            "                                            {'loss': 0.2003, 'learning_rate': 1.6846846846846846e-05, 'epoch': 4.73}\n",
            " 16% 1410/8880 [5:16:22<25:37:59, 12.35s/it]{'loss': 0.1972, 'learning_rate': 1.6824324324324324e-05, 'epoch': 4.76}\n",
            " 16% 1420/8880 [5:18:27<25:41:39, 12.40s/it]{'loss': 0.1959, 'learning_rate': 1.6801801801801803e-05, 'epoch': 4.8}\n",
            "{'loss': 0.2232, 'learning_rate': 1.677927927927928e-05, 'epoch': 4.83}\n",
            " 16% 1440/8880 [5:22:35<25:41:47, 12.43s/it]{'loss': 0.1747, 'learning_rate': 1.6756756756756757e-05, 'epoch': 4.86}\n",
            "{'loss': 0.2016, 'learning_rate': 1.6734234234234235e-05, 'epoch': 4.9}\n",
            "{'loss': 0.2114, 'learning_rate': 1.6711711711711714e-05, 'epoch': 4.93}\n",
            "{'loss': 0.1624, 'learning_rate': 1.668918918918919e-05, 'epoch': 4.97}\n",
            " 17% 1480/8880 [5:30:46<24:32:18, 11.94s/it]{'loss': 0.1609, 'learning_rate': 1.6666666666666667e-05, 'epoch': 5.0}\n",
            "[INFO|trainer.py:2463] 2022-05-07 15:55:12,760 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2465] 2022-05-07 15:55:12,760 >>   Num examples = 2105\n",
            "[INFO|trainer.py:2468] 2022-05-07 15:55:12,760 >>   Batch size = 64\n",
            "\n",
            "  0% 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/33 [00:12<03:15,  6.31s/it]\u001b[A\n",
            "  9% 3/33 [00:24<04:22,  8.75s/it]\u001b[A\n",
            " 12% 4/33 [00:36<04:49, 10.00s/it]\u001b[A\n",
            " 15% 5/33 [00:49<05:02, 10.80s/it]\u001b[A\n",
            " 18% 6/33 [01:00<04:59, 11.08s/it]\u001b[A\n",
            " 21% 7/33 [01:13<04:58, 11.47s/it]\u001b[A\n",
            " 24% 8/33 [01:25<04:52, 11.70s/it]\u001b[A\n",
            " 27% 9/33 [01:37<04:44, 11.85s/it]\u001b[A\n",
            " 30% 10/33 [01:49<04:34, 11.92s/it]\u001b[A\n",
            " 33% 11/33 [02:01<04:24, 12.01s/it]\u001b[A\n",
            " 36% 12/33 [02:13<04:11, 11.98s/it]\u001b[A\n",
            " 39% 13/33 [02:25<03:59, 11.97s/it]\u001b[A\n",
            " 42% 14/33 [02:37<03:47, 11.98s/it]\u001b[A\n",
            " 45% 15/33 [02:49<03:35, 11.97s/it]\u001b[A\n",
            " 48% 16/33 [03:01<03:21, 11.84s/it]\u001b[A\n",
            " 52% 17/33 [03:12<03:09, 11.82s/it]\u001b[A\n",
            " 55% 18/33 [03:25<02:59, 11.94s/it]\u001b[A\n",
            " 58% 19/33 [03:37<02:47, 11.95s/it]\u001b[A\n",
            " 61% 20/33 [03:49<02:37, 12.11s/it]\u001b[A\n",
            " 64% 21/33 [04:01<02:24, 12.03s/it]\u001b[A\n",
            " 67% 22/33 [04:13<02:13, 12.15s/it]\u001b[A\n",
            " 70% 23/33 [04:25<02:01, 12.11s/it]\u001b[A\n",
            " 73% 24/33 [04:38<01:49, 12.12s/it]\u001b[A\n",
            " 76% 25/33 [04:50<01:36, 12.10s/it]\u001b[A\n",
            " 79% 26/33 [05:03<01:26, 12.38s/it]\u001b[A\n",
            " 82% 27/33 [05:15<01:14, 12.34s/it]\u001b[A\n",
            " 85% 28/33 [05:27<01:02, 12.42s/it]\u001b[A\n",
            " 88% 29/33 [05:40<00:49, 12.48s/it]\u001b[A\n",
            " 91% 30/33 [05:53<00:37, 12.48s/it]\u001b[A\n",
            " 94% 31/33 [06:05<00:24, 12.40s/it]\u001b[A\n",
            " 97% 32/33 [06:17<00:12, 12.39s/it]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.20281201601028442, 'eval_accuracy': 0.9586698337292161, 'eval_runtime': 400.8308, 'eval_samples_per_second': 5.252, 'eval_steps_per_second': 0.082, 'epoch': 5.0}\n",
            " 17% 1480/8880 [5:37:26<24:32:18, 11.94s/it]\n",
            "100% 33/33 [06:28<00:00, 11.79s/it]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2213] 2022-05-07 16:01:53,595 >> Saving model checkpoint to crop14_balance_ft_pretrain_vit-base-mim/checkpoint-1480\n",
            "[INFO|configuration_utils.py:446] 2022-05-07 16:01:53,597 >> Configuration saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-1480/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-07 16:01:54,315 >> Model weights saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-1480/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-07 16:01:54,315 >> Feature extractor saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-1480/preprocessor_config.json\n",
            "[INFO|trainer.py:2291] 2022-05-07 16:01:57,185 >> Deleting older checkpoint [crop14_balance_ft_pretrain_vit-base-mim/checkpoint-1184] due to args.save_total_limit\n",
            " 17% 1490/8880 [5:39:33<34:59:55, 17.05s/it]{'loss': 0.1782, 'learning_rate': 1.6644144144144146e-05, 'epoch': 5.03}\n",
            " 17% 1500/8880 [5:41:34<25:18:11, 12.34s/it]{'loss': 0.1411, 'learning_rate': 1.662162162162162e-05, 'epoch': 5.07}\n",
            " 17% 1510/8880 [5:43:36<24:52:56, 12.15s/it]{'loss': 0.166, 'learning_rate': 1.65990990990991e-05, 'epoch': 5.1}\n",
            "{'loss': 0.1867, 'learning_rate': 1.6576576576576578e-05, 'epoch': 5.14}\n",
            "{'loss': 0.1616, 'learning_rate': 1.6554054054054057e-05, 'epoch': 5.17}\n",
            "{'loss': 0.1626, 'learning_rate': 1.6531531531531532e-05, 'epoch': 5.2}\n",
            "{'loss': 0.1548, 'learning_rate': 1.650900900900901e-05, 'epoch': 5.24}\n",
            "{'loss': 0.1757, 'learning_rate': 1.648648648648649e-05, 'epoch': 5.27}\n",
            "{'loss': 0.1504, 'learning_rate': 1.6463963963963964e-05, 'epoch': 5.3}\n",
            "{'loss': 0.1637, 'learning_rate': 1.6441441441441443e-05, 'epoch': 5.34}\n",
            " 18% 1590/8880 [5:59:58<24:28:15, 12.08s/it]{'loss': 0.1784, 'learning_rate': 1.641891891891892e-05, 'epoch': 5.37}\n",
            "{'loss': 0.1707, 'learning_rate': 1.6396396396396396e-05, 'epoch': 5.41}\n",
            " 18% 1610/8880 [6:04:04<25:00:26, 12.38s/it]{'loss': 0.1509, 'learning_rate': 1.6373873873873875e-05, 'epoch': 5.44}\n",
            "{'loss': 0.1683, 'learning_rate': 1.6351351351351354e-05, 'epoch': 5.47}\n",
            "{'loss': 0.1501, 'learning_rate': 1.6328828828828832e-05, 'epoch': 5.51}\n",
            "{'loss': 0.1887, 'learning_rate': 1.6306306306306307e-05, 'epoch': 5.54}\n",
            "{'loss': 0.1813, 'learning_rate': 1.6283783783783786e-05, 'epoch': 5.57}\n",
            " 19% 1660/8880 [6:14:19<24:47:53, 12.36s/it]{'loss': 0.1809, 'learning_rate': 1.6261261261261264e-05, 'epoch': 5.61}\n",
            "{'loss': 0.154, 'learning_rate': 1.623873873873874e-05, 'epoch': 5.64}\n",
            "{'loss': 0.1968, 'learning_rate': 1.6216216216216218e-05, 'epoch': 5.68}\n",
            "{'loss': 0.1966, 'learning_rate': 1.6193693693693697e-05, 'epoch': 5.71}\n",
            "{'loss': 0.1667, 'learning_rate': 1.6171171171171172e-05, 'epoch': 5.74}\n",
            " 19% 1710/8880 [6:24:40<24:23:06, 12.24s/it]{'loss': 0.1721, 'learning_rate': 1.614864864864865e-05, 'epoch': 5.78}\n",
            "{'loss': 0.1463, 'learning_rate': 1.6126126126126126e-05, 'epoch': 5.81}\n",
            "{'loss': 0.1686, 'learning_rate': 1.6103603603603607e-05, 'epoch': 5.84}\n",
            "{'loss': 0.1539, 'learning_rate': 1.6081081081081083e-05, 'epoch': 5.88}\n",
            "{'loss': 0.1443, 'learning_rate': 1.6058558558558558e-05, 'epoch': 5.91}\n",
            "{'loss': 0.1223, 'learning_rate': 1.6036036036036036e-05, 'epoch': 5.95}\n",
            " 20% 1770/8880 [6:37:02<24:13:11, 12.26s/it]{'loss': 0.1563, 'learning_rate': 1.6013513513513515e-05, 'epoch': 5.98}\n",
            " 20% 1776/8880 [6:38:15<23:35:18, 11.95s/it][INFO|trainer.py:2463] 2022-05-07 17:02:42,266 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2465] 2022-05-07 17:02:42,266 >>   Num examples = 2105\n",
            "[INFO|trainer.py:2468] 2022-05-07 17:02:42,266 >>   Batch size = 64\n",
            "\n",
            "  0% 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/33 [00:12<03:12,  6.22s/it]\u001b[A\n",
            "  9% 3/33 [00:24<04:20,  8.67s/it]\u001b[A\n",
            " 12% 4/33 [00:36<04:48,  9.95s/it]\u001b[A\n",
            " 15% 5/33 [00:48<05:00, 10.73s/it]\u001b[A\n",
            " 18% 6/33 [01:00<04:59, 11.08s/it]\u001b[A\n",
            " 21% 7/33 [01:13<04:59, 11.51s/it]\u001b[A\n",
            " 24% 8/33 [01:25<04:54, 11.78s/it]\u001b[A\n",
            " 27% 9/33 [01:37<04:47, 11.97s/it]\u001b[A\n",
            " 30% 10/33 [01:49<04:35, 11.98s/it]\u001b[A\n",
            " 33% 11/33 [02:01<04:24, 12.00s/it]\u001b[A\n",
            " 36% 12/33 [02:13<04:10, 11.94s/it]\u001b[A\n",
            " 39% 13/33 [02:25<04:00, 12.00s/it]\u001b[A\n",
            " 42% 14/33 [02:37<03:49, 12.06s/it]\u001b[A\n",
            " 45% 15/33 [02:49<03:36, 12.03s/it]\u001b[A\n",
            " 48% 16/33 [03:01<03:22, 11.90s/it]\u001b[A\n",
            " 52% 17/33 [03:13<03:10, 11.91s/it]\u001b[A\n",
            " 55% 18/33 [03:25<02:59, 11.99s/it]\u001b[A\n",
            " 58% 19/33 [03:37<02:48, 12.00s/it]\u001b[A\n",
            " 61% 20/33 [03:50<02:38, 12.17s/it]\u001b[A\n",
            " 64% 21/33 [04:02<02:25, 12.10s/it]\u001b[A\n",
            " 67% 22/33 [04:14<02:15, 12.27s/it]\u001b[A\n",
            " 70% 23/33 [04:27<02:02, 12.28s/it]\u001b[A\n",
            " 73% 24/33 [04:39<01:50, 12.27s/it]\u001b[A\n",
            " 76% 25/33 [04:51<01:37, 12.20s/it]\u001b[A\n",
            " 79% 26/33 [05:04<01:27, 12.51s/it]\u001b[A\n",
            " 82% 27/33 [05:16<01:14, 12.42s/it]\u001b[A\n",
            " 85% 28/33 [05:29<01:02, 12.53s/it]\u001b[A\n",
            " 88% 29/33 [05:42<00:50, 12.52s/it]\u001b[A\n",
            " 91% 30/33 [05:54<00:37, 12.52s/it]\u001b[A\n",
            " 94% 31/33 [06:06<00:24, 12.35s/it]\u001b[A\n",
            " 97% 32/33 [06:19<00:12, 12.36s/it]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.1624968945980072, 'eval_accuracy': 0.9619952494061758, 'eval_runtime': 402.5611, 'eval_samples_per_second': 5.229, 'eval_steps_per_second': 0.082, 'epoch': 6.0}\n",
            " 20% 1776/8880 [6:44:58<23:35:18, 11.95s/it]\n",
            "100% 33/33 [06:29<00:00, 11.83s/it]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2213] 2022-05-07 17:09:24,833 >> Saving model checkpoint to crop14_balance_ft_pretrain_vit-base-mim/checkpoint-1776\n",
            "[INFO|configuration_utils.py:446] 2022-05-07 17:09:24,834 >> Configuration saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-1776/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-07 17:09:25,632 >> Model weights saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-1776/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-07 17:09:25,632 >> Feature extractor saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-1776/preprocessor_config.json\n",
            "[INFO|trainer.py:2291] 2022-05-07 17:09:27,798 >> Deleting older checkpoint [crop14_balance_ft_pretrain_vit-base-mim/checkpoint-1480] due to args.save_total_limit\n",
            "{'loss': 0.1268, 'learning_rate': 1.5990990990990993e-05, 'epoch': 6.01}\n",
            "{'loss': 0.1775, 'learning_rate': 1.596846846846847e-05, 'epoch': 6.05}\n",
            "{'loss': 0.1495, 'learning_rate': 1.5945945945945947e-05, 'epoch': 6.08}\n",
            " 20% 1800/8880 [6:49:56<24:22:59, 12.40s/it]\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ReadTimeout), entering retry loop.\n",
            "{'loss': 0.1303, 'learning_rate': 1.5923423423423426e-05, 'epoch': 6.11}\n",
            "{'loss': 0.192, 'learning_rate': 1.59009009009009e-05, 'epoch': 6.15}\n",
            "{'loss': 0.1633, 'learning_rate': 1.587837837837838e-05, 'epoch': 6.18}\n",
            "{'loss': 0.1251, 'learning_rate': 1.5855855855855858e-05, 'epoch': 6.22}\n",
            " 21% 1850/8880 [7:00:12<24:07:08, 12.35s/it]{'loss': 0.1278, 'learning_rate': 1.5833333333333333e-05, 'epoch': 6.25}\n",
            "{'loss': 0.1372, 'learning_rate': 1.581081081081081e-05, 'epoch': 6.28}\n",
            "{'loss': 0.1571, 'learning_rate': 1.578828828828829e-05, 'epoch': 6.32}\n",
            "{'loss': 0.1449, 'learning_rate': 1.576576576576577e-05, 'epoch': 6.35}\n",
            "{'loss': 0.1559, 'learning_rate': 1.5743243243243244e-05, 'epoch': 6.39}\n",
            "{'loss': 0.1325, 'learning_rate': 1.5720720720720722e-05, 'epoch': 6.42}\n",
            " 22% 1910/8880 [7:12:36<24:15:51, 12.53s/it]{'loss': 0.1255, 'learning_rate': 1.56981981981982e-05, 'epoch': 6.45}\n",
            "{'loss': 0.1618, 'learning_rate': 1.5675675675675676e-05, 'epoch': 6.49}\n",
            " 22% 1930/8880 [7:16:43<23:39:43, 12.26s/it]{'loss': 0.1668, 'learning_rate': 1.5653153153153155e-05, 'epoch': 6.52}\n",
            "{'loss': 0.1212, 'learning_rate': 1.5630630630630633e-05, 'epoch': 6.55}\n",
            "{'loss': 0.1082, 'learning_rate': 1.560810810810811e-05, 'epoch': 6.59}\n",
            "{'loss': 0.1581, 'learning_rate': 1.5585585585585587e-05, 'epoch': 6.62}\n",
            " 22% 1970/8880 [7:24:56<23:48:31, 12.40s/it]{'loss': 0.1503, 'learning_rate': 1.5563063063063065e-05, 'epoch': 6.66}\n",
            "{'loss': 0.1261, 'learning_rate': 1.554054054054054e-05, 'epoch': 6.69}\n",
            "{'loss': 0.1656, 'learning_rate': 1.551801801801802e-05, 'epoch': 6.72}\n",
            "{'loss': 0.1252, 'learning_rate': 1.5495495495495498e-05, 'epoch': 6.76}\n",
            "{'loss': 0.1505, 'learning_rate': 1.5472972972972976e-05, 'epoch': 6.79}\n",
            "{'loss': 0.1505, 'learning_rate': 1.545045045045045e-05, 'epoch': 6.82}\n",
            "                                            {'loss': 0.1131, 'learning_rate': 1.5427927927927927e-05, 'epoch': 6.86}\n",
            "{'loss': 0.1627, 'learning_rate': 1.540540540540541e-05, 'epoch': 6.89}\n",
            "{'loss': 0.1011, 'learning_rate': 1.5382882882882884e-05, 'epoch': 6.93}\n",
            "{'loss': 0.1165, 'learning_rate': 1.5360360360360362e-05, 'epoch': 6.96}\n",
            "{'loss': 0.1179, 'learning_rate': 1.5337837837837837e-05, 'epoch': 6.99}\n",
            " 23% 2072/8880 [7:46:00<22:46:25, 12.04s/it][INFO|trainer.py:2463] 2022-05-07 18:10:26,864 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2465] 2022-05-07 18:10:26,864 >>   Num examples = 2105\n",
            "[INFO|trainer.py:2468] 2022-05-07 18:10:26,864 >>   Batch size = 64\n",
            "\n",
            "  0% 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/33 [00:12<03:14,  6.28s/it]\u001b[A\n",
            "  9% 3/33 [00:24<04:19,  8.66s/it]\u001b[A\n",
            " 12% 4/33 [00:36<04:50, 10.01s/it]\u001b[A\n",
            " 15% 5/33 [00:49<05:02, 10.81s/it]\u001b[A\n",
            " 18% 6/33 [01:00<04:59, 11.09s/it]\u001b[A\n",
            " 21% 7/33 [01:12<04:56, 11.41s/it]\u001b[A\n",
            " 24% 8/33 [01:25<04:51, 11.67s/it]\u001b[A\n",
            " 27% 9/33 [01:37<04:45, 11.91s/it]\u001b[A\n",
            " 30% 10/33 [01:49<04:35, 11.96s/it]\u001b[A\n",
            " 33% 11/33 [02:01<04:23, 11.99s/it]\u001b[A\n",
            " 36% 12/33 [02:13<04:10, 11.93s/it]\u001b[A\n",
            " 39% 13/33 [02:25<03:59, 11.99s/it]\u001b[A\n",
            " 42% 14/33 [02:37<03:49, 12.06s/it]\u001b[A\n",
            " 45% 15/33 [02:49<03:35, 12.00s/it]\u001b[A\n",
            " 48% 16/33 [03:01<03:21, 11.84s/it]\u001b[A\n",
            " 52% 17/33 [03:13<03:09, 11.84s/it]\u001b[A\n",
            " 55% 18/33 [03:25<02:59, 11.97s/it]\u001b[A\n",
            " 58% 19/33 [03:37<02:48, 12.02s/it]\u001b[A\n",
            " 61% 20/33 [03:49<02:38, 12.16s/it]\u001b[A\n",
            " 64% 21/33 [04:01<02:24, 12.06s/it]\u001b[A\n",
            " 67% 22/33 [04:14<02:14, 12.22s/it]\u001b[A\n",
            " 70% 23/33 [04:26<02:02, 12.21s/it]\u001b[A\n",
            " 73% 24/33 [04:38<01:49, 12.21s/it]\u001b[A\n",
            " 76% 25/33 [04:50<01:37, 12.22s/it]\u001b[A\n",
            " 79% 26/33 [05:04<01:27, 12.47s/it]\u001b[A\n",
            " 82% 27/33 [05:16<01:14, 12.37s/it]\u001b[A\n",
            " 85% 28/33 [05:28<01:02, 12.50s/it]\u001b[A\n",
            " 88% 29/33 [05:41<00:49, 12.47s/it]\u001b[A\n",
            " 91% 30/33 [05:53<00:37, 12.47s/it]\u001b[A\n",
            " 94% 31/33 [06:05<00:24, 12.35s/it]\u001b[A\n",
            " 97% 32/33 [06:18<00:12, 12.33s/it]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.1348516345024109, 'eval_accuracy': 0.9714964370546318, 'eval_runtime': 401.7782, 'eval_samples_per_second': 5.239, 'eval_steps_per_second': 0.082, 'epoch': 7.0}\n",
            " 23% 2072/8880 [7:52:42<22:46:25, 12.04s/it]\n",
            "100% 33/33 [06:29<00:00, 11.82s/it]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2213] 2022-05-07 18:17:08,651 >> Saving model checkpoint to crop14_balance_ft_pretrain_vit-base-mim/checkpoint-2072\n",
            "[INFO|configuration_utils.py:446] 2022-05-07 18:17:08,652 >> Configuration saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-2072/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-07 18:17:09,407 >> Model weights saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-2072/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-07 18:17:09,408 >> Feature extractor saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-2072/preprocessor_config.json\n",
            "[INFO|trainer.py:2291] 2022-05-07 18:17:11,652 >> Deleting older checkpoint [crop14_balance_ft_pretrain_vit-base-mim/checkpoint-1776] due to args.save_total_limit\n",
            "{'loss': 0.1146, 'learning_rate': 1.5315315315315316e-05, 'epoch': 7.03}\n",
            " 24% 2090/8880 [7:56:26<23:39:05, 12.54s/it]{'loss': 0.1277, 'learning_rate': 1.5292792792792795e-05, 'epoch': 7.06}\n",
            "{'loss': 0.1441, 'learning_rate': 1.527027027027027e-05, 'epoch': 7.09}\n",
            "{'loss': 0.1081, 'learning_rate': 1.524774774774775e-05, 'epoch': 7.13}\n",
            "{'loss': 0.1336, 'learning_rate': 1.5225225225225227e-05, 'epoch': 7.16}\n",
            "{'loss': 0.1495, 'learning_rate': 1.5202702702702704e-05, 'epoch': 7.2}\n",
            " 24% 2140/8880 [8:06:43<23:16:06, 12.43s/it]{'loss': 0.1351, 'learning_rate': 1.5180180180180182e-05, 'epoch': 7.23}\n",
            " 24% 2150/8880 [8:08:47<22:56:56, 12.28s/it]{'loss': 0.1524, 'learning_rate': 1.5157657657657659e-05, 'epoch': 7.26}\n",
            "{'loss': 0.1165, 'learning_rate': 1.5135135135135138e-05, 'epoch': 7.3}\n",
            "{'loss': 0.1618, 'learning_rate': 1.5112612612612614e-05, 'epoch': 7.33}\n",
            " 25% 2180/8880 [8:14:55<22:50:15, 12.27s/it]{'loss': 0.1335, 'learning_rate': 1.509009009009009e-05, 'epoch': 7.36}\n",
            " 25% 2190/8880 [8:16:56<22:38:00, 12.18s/it]{'loss': 0.1345, 'learning_rate': 1.506756756756757e-05, 'epoch': 7.4}\n",
            " 25% 2200/8880 [8:19:00<22:59:21, 12.39s/it]{'loss': 0.1231, 'learning_rate': 1.5045045045045045e-05, 'epoch': 7.43}\n",
            " 25% 2210/8880 [8:21:03<22:47:54, 12.30s/it]{'loss': 0.1088, 'learning_rate': 1.5022522522522525e-05, 'epoch': 7.47}\n",
            "{'loss': 0.12, 'learning_rate': 1.5000000000000002e-05, 'epoch': 7.5}\n",
            " 25% 2230/8880 [8:25:07<22:39:46, 12.27s/it]{'loss': 0.1053, 'learning_rate': 1.4977477477477477e-05, 'epoch': 7.53}\n",
            "{'loss': 0.1024, 'learning_rate': 1.4954954954954957e-05, 'epoch': 7.57}\n",
            "{'loss': 0.1356, 'learning_rate': 1.4932432432432433e-05, 'epoch': 7.6}\n",
            "{'loss': 0.1145, 'learning_rate': 1.4909909909909913e-05, 'epoch': 7.64}\n",
            "{'loss': 0.1288, 'learning_rate': 1.4887387387387388e-05, 'epoch': 7.67}\n",
            " 26% 2280/8880 [8:35:23<22:30:27, 12.28s/it]{'loss': 0.1217, 'learning_rate': 1.4864864864864865e-05, 'epoch': 7.7}\n",
            "{'loss': 0.0967, 'learning_rate': 1.4842342342342343e-05, 'epoch': 7.74}\n",
            "{'loss': 0.0978, 'learning_rate': 1.481981981981982e-05, 'epoch': 7.77}\n",
            "{'loss': 0.1089, 'learning_rate': 1.4797297297297299e-05, 'epoch': 7.8}\n",
            " 26% 2320/8880 [8:43:34<22:35:06, 12.39s/it]{'loss': 0.1291, 'learning_rate': 1.4774774774774776e-05, 'epoch': 7.84}\n",
            " 26% 2330/8880 [8:45:36<22:20:14, 12.28s/it]{'loss': 0.095, 'learning_rate': 1.4752252252252253e-05, 'epoch': 7.87}\n",
            "{'loss': 0.1161, 'learning_rate': 1.4729729729729731e-05, 'epoch': 7.91}\n",
            "{'loss': 0.1002, 'learning_rate': 1.4707207207207208e-05, 'epoch': 7.94}\n",
            " 27% 2360/8880 [8:51:47<22:19:43, 12.33s/it]{'loss': 0.0923, 'learning_rate': 1.4684684684684686e-05, 'epoch': 7.97}\n",
            " 27% 2368/8880 [8:53:24<21:23:25, 11.83s/it][INFO|trainer.py:2463] 2022-05-07 19:17:50,997 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2465] 2022-05-07 19:17:50,997 >>   Num examples = 2105\n",
            "[INFO|trainer.py:2468] 2022-05-07 19:17:50,997 >>   Batch size = 64\n",
            "\n",
            "  0% 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/33 [00:12<03:13,  6.24s/it]\u001b[A\n",
            "  9% 3/33 [00:24<04:19,  8.64s/it]\u001b[A\n",
            " 12% 4/33 [00:36<04:48,  9.96s/it]\u001b[A\n",
            " 15% 5/33 [00:48<05:01, 10.77s/it]\u001b[A\n",
            " 18% 6/33 [01:00<04:59, 11.07s/it]\u001b[A\n",
            " 21% 7/33 [01:12<04:56, 11.40s/it]\u001b[A\n",
            " 24% 8/33 [01:25<04:52, 11.69s/it]\u001b[A\n",
            " 27% 9/33 [01:37<04:44, 11.85s/it]\u001b[A\n",
            " 30% 10/33 [01:49<04:35, 11.96s/it]\u001b[A\n",
            " 33% 11/33 [02:01<04:24, 12.02s/it]\u001b[A\n",
            " 36% 12/33 [02:13<04:10, 11.94s/it]\u001b[A\n",
            " 39% 13/33 [02:25<03:59, 11.97s/it]\u001b[A\n",
            " 42% 14/33 [02:37<03:48, 12.02s/it]\u001b[A\n",
            " 45% 15/33 [02:49<03:35, 11.97s/it]\u001b[A\n",
            " 48% 16/33 [03:00<03:21, 11.82s/it]\u001b[A\n",
            " 52% 17/33 [03:12<03:08, 11.80s/it]\u001b[A\n",
            " 55% 18/33 [03:24<02:59, 11.95s/it]\u001b[A\n",
            " 58% 19/33 [03:36<02:47, 11.98s/it]\u001b[A\n",
            " 61% 20/33 [03:49<02:37, 12.08s/it]\u001b[A\n",
            " 64% 21/33 [04:01<02:24, 12.01s/it]\u001b[A\n",
            " 67% 22/33 [04:13<02:13, 12.18s/it]\u001b[A\n",
            " 70% 23/33 [04:25<02:01, 12.18s/it]\u001b[A\n",
            " 73% 24/33 [04:38<01:49, 12.20s/it]\u001b[A\n",
            " 76% 25/33 [04:50<01:37, 12.20s/it]\u001b[A\n",
            " 79% 26/33 [05:03<01:27, 12.46s/it]\u001b[A\n",
            " 82% 27/33 [05:15<01:14, 12.37s/it]\u001b[A\n",
            " 85% 28/33 [05:28<01:02, 12.44s/it]\u001b[A\n",
            " 88% 29/33 [05:40<00:49, 12.40s/it]\u001b[A\n",
            " 91% 30/33 [05:53<00:37, 12.44s/it]\u001b[A\n",
            " 94% 31/33 [06:05<00:24, 12.36s/it]\u001b[A\n",
            " 97% 32/33 [06:17<00:12, 12.34s/it]\u001b[A\n",
            "                                            \n",
            "\u001b[A{'eval_loss': 0.1454136073589325, 'eval_accuracy': 0.961520190023753, 'eval_runtime': 400.9103, 'eval_samples_per_second': 5.251, 'eval_steps_per_second': 0.082, 'epoch': 8.0}\n",
            " 27% 2368/8880 [9:00:05<21:23:25, 11.83s/it]\n",
            "100% 33/33 [06:28<00:00, 11.77s/it]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2213] 2022-05-07 19:24:31,911 >> Saving model checkpoint to crop14_balance_ft_pretrain_vit-base-mim/checkpoint-2368\n",
            "[INFO|configuration_utils.py:446] 2022-05-07 19:24:31,912 >> Configuration saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-2368/config.json\n",
            "[INFO|modeling_utils.py:1542] 2022-05-07 19:24:32,698 >> Model weights saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-2368/pytorch_model.bin\n",
            "[INFO|feature_extraction_utils.py:351] 2022-05-07 19:24:32,699 >> Feature extractor saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-2368/preprocessor_config.json\n",
            "{'loss': 0.1106, 'learning_rate': 1.4662162162162163e-05, 'epoch': 8.01}\n",
            " 27% 2380/8880 [9:02:34<26:34:50, 14.72s/it]{'loss': 0.095, 'learning_rate': 1.463963963963964e-05, 'epoch': 8.04}\n",
            "{'loss': 0.1175, 'learning_rate': 1.4617117117117119e-05, 'epoch': 8.07}\n",
            "{'loss': 0.111, 'learning_rate': 1.4594594594594596e-05, 'epoch': 8.11}\n",
            "{'loss': 0.1213, 'learning_rate': 1.4572072072072074e-05, 'epoch': 8.14}\n",
            "{'loss': 0.1559, 'learning_rate': 1.4549549549549551e-05, 'epoch': 8.18}\n",
            "{'loss': 0.1113, 'learning_rate': 1.4527027027027028e-05, 'epoch': 8.21}\n",
            "{'loss': 0.1293, 'learning_rate': 1.4504504504504506e-05, 'epoch': 8.24}\n",
            "{'loss': 0.1048, 'learning_rate': 1.4481981981981983e-05, 'epoch': 8.28}\n",
            "{'loss': 0.0845, 'learning_rate': 1.4459459459459462e-05, 'epoch': 8.31}\n",
            "{'loss': 0.1129, 'learning_rate': 1.4436936936936939e-05, 'epoch': 8.34}\n",
            "{'loss': 0.1036, 'learning_rate': 1.4414414414414416e-05, 'epoch': 8.38}\n",
            " 28% 2490/8880 [9:25:12<21:55:00, 12.35s/it]{'loss': 0.0671, 'learning_rate': 1.4391891891891894e-05, 'epoch': 8.41}\n",
            " 28% 2500/8880 [9:27:15<22:04:20, 12.45s/it]{'loss': 0.1342, 'learning_rate': 1.4369369369369371e-05, 'epoch': 8.45}\n",
            "{'loss': 0.1155, 'learning_rate': 1.4346846846846846e-05, 'epoch': 8.48}\n",
            "{'loss': 0.1177, 'learning_rate': 1.4324324324324326e-05, 'epoch': 8.51}\n",
            " 28% 2530/8880 [9:33:23<21:33:30, 12.22s/it]{'loss': 0.0781, 'learning_rate': 1.4301801801801803e-05, 'epoch': 8.55}\n",
            "{'loss': 0.1101, 'learning_rate': 1.4279279279279282e-05, 'epoch': 8.58}\n",
            " 29% 2550/8880 [9:37:27<21:24:38, 12.18s/it]{'loss': 0.1103, 'learning_rate': 1.4256756756756759e-05, 'epoch': 8.61}\n",
            " 29% 2560/8880 [9:39:31<21:37:26, 12.32s/it]{'loss': 0.117, 'learning_rate': 1.4234234234234234e-05, 'epoch': 8.65}\n",
            "{'loss': 0.1098, 'learning_rate': 1.4211711711711714e-05, 'epoch': 8.68}\n",
            " 29% 2580/8880 [9:43:37<21:33:35, 12.32s/it]{'loss': 0.0978, 'learning_rate': 1.4189189189189189e-05, 'epoch': 8.72}\n",
            "{'loss': 0.1184, 'learning_rate': 1.416666666666667e-05, 'epoch': 8.75}\n",
            " 29% 2600/8880 [9:47:41<21:34:19, 12.37s/it]{'loss': 0.1007, 'learning_rate': 1.4144144144144145e-05, 'epoch': 8.78}\n",
            "{'loss': 0.105, 'learning_rate': 1.4121621621621621e-05, 'epoch': 8.82}\n",
            "{'loss': 0.081, 'learning_rate': 1.40990990990991e-05, 'epoch': 8.85}\n",
            "{'loss': 0.1584, 'learning_rate': 1.4076576576576577e-05, 'epoch': 8.89}\n",
            "{'loss': 0.1287, 'learning_rate': 1.4054054054054055e-05, 'epoch': 8.92}\n",
            " 30% 2650/8880 [9:57:52<20:54:05, 12.08s/it]{'loss': 0.1174, 'learning_rate': 1.4031531531531532e-05, 'epoch': 8.95}\n",
            "{'loss': 0.0647, 'learning_rate': 1.4009009009009009e-05, 'epoch': 8.99}\n",
            " 30% 2664/8880 [10:00:42<20:36:01, 11.93s/it][INFO|trainer.py:2463] 2022-05-07 20:25:09,416 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2465] 2022-05-07 20:25:09,416 >>   Num examples = 2105\n",
            "[INFO|trainer.py:2468] 2022-05-07 20:25:09,416 >>   Batch size = 64\n",
            "\n",
            "  0% 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/33 [00:12<03:12,  6.20s/it]\u001b[A\n",
            "  9% 3/33 [00:24<04:18,  8.62s/it]\u001b[A\n",
            " 12% 4/33 [00:36<04:47,  9.92s/it]\u001b[A\n",
            " 15% 5/33 [00:48<05:00, 10.74s/it]\u001b[A\n",
            " 18% 6/33 [01:00<04:57, 11.03s/it]\u001b[A\n",
            " 21% 7/33 [01:12<04:56, 11.39s/it]\u001b[A\n",
            " 24% 8/33 [01:24<04:51, 11.68s/it]\u001b[A\n",
            " 27% 9/33 [01:37<04:43, 11.83s/it]\u001b[A\n",
            " 30% 10/33 [01:49<04:34, 11.92s/it]\u001b[A\n",
            " 33% 11/33 [02:01<04:22, 11.93s/it]\u001b[A\n",
            " 36% 12/33 [02:13<04:10, 11.93s/it]\u001b[A\n",
            " 39% 13/33 [02:25<04:00, 12.02s/it]\u001b[A\n",
            " 42% 14/33 [02:37<03:48, 12.03s/it]\u001b[A\n",
            " 45% 15/33 [02:49<03:35, 11.95s/it]\u001b[A\n",
            " 48% 16/33 [03:00<03:20, 11.79s/it]\u001b[A\n",
            " 52% 17/33 [03:12<03:08, 11.79s/it]\u001b[A\n",
            " 55% 18/33 [03:24<02:58, 11.93s/it]\u001b[A\n",
            " 58% 19/33 [03:36<02:47, 11.93s/it]\u001b[A\n",
            " 61% 20/33 [03:48<02:36, 12.07s/it]\u001b[A\n",
            " 64% 21/33 [04:00<02:24, 12.04s/it]\u001b[A\n",
            " 67% 22/33 [04:13<02:13, 12.17s/it]\u001b[A\n",
            " 70% 23/33 [04:25<02:01, 12.15s/it]\u001b[A\n",
            " 73% 24/33 [04:37<01:49, 12.13s/it]\u001b[A\n",
            " 76% 25/33 [04:49<01:37, 12.16s/it]\u001b[A\n",
            " 79% 26/33 [05:02<01:26, 12.42s/it]\u001b[A\n",
            " 82% 27/33 [05:14<01:13, 12.31s/it]\u001b[A\n",
            " 85% 28/33 [05:27<01:02, 12.40s/it]\u001b[A\n",
            " 88% 29/33 [05:39<00:49, 12.36s/it]\u001b[A\n",
            " 91% 30/33 [05:52<00:37, 12.38s/it]\u001b[A\n",
            " 94% 31/33 [06:04<00:24, 12.28s/it]\u001b[A\n",
            " 97% 32/33 [06:16<00:12, 12.26s/it]\u001b[A\n",
            "                                             {'eval_loss': 0.13270272314548492, 'eval_accuracy': 0.9657957244655582, 'eval_runtime': 399.5779, 'eval_samples_per_second': 5.268, 'eval_steps_per_second': 0.083, 'epoch': 9.0}\n",
            "\n",
            " 30% 2664/8880 [10:07:22<20:36:01, 11.93s/it]\n",
            "100% 33/33 [06:26<00:00, 11.65s/it]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2213] 2022-05-07 20:31:49,000 >> Saving model checkpoint to crop14_balance_ft_pretrain_vit-base-mim/checkpoint-2664\n",
            "[INFO|configuration_utils.py:446] 2022-05-07 20:31:49,002 >> Configuration saved in crop14_balance_ft_pretrain_vit-base-mim/checkpoint-2664/config.json\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 380, in save\n",
            "    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 604, in _save\n",
            "    zip_file.write_record(name, storage.data_ptr(), num_bytes)\n",
            "OSError: [Errno 28] No space left on device\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"run_image_classification.py\", line 369, in <module>\n",
            "    main()\n",
            "  File \"run_image_classification.py\", line 343, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1519, in train\n",
            "    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1675, in _maybe_log_save_evaluate\n",
            "    self._save_checkpoint(model, trial, metrics=metrics)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1747, in _save_checkpoint\n",
            "    self.save_model(output_dir, _internal_call=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2175, in save_model\n",
            "    self._save(output_dir)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2227, in _save\n",
            "    self.model.save_pretrained(output_dir, state_dict=state_dict)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\", line 1539, in save_pretrained\n",
            "    save_function(shard, os.path.join(save_directory, shard_file))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 381, in save\n",
            "    return\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 260, in __exit__\n",
            "    self.file_like.write_end_of_file()\n",
            "RuntimeError: [enforce fail at inline_container.cc:300] . unexpected pos 12446976 vs 12446864\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1029, in emit\n",
            "    self.flush()\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1009, in flush\n",
            "    self.stream.flush()\n",
            "OSError: [Errno 28] No space left on device\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 890, in _bootstrap\n",
            "    self._bootstrap_inner()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/vendor/watchdog/observers/api.py\", line 199, in run\n",
            "    self.dispatch_events(self.event_queue, self.timeout)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/vendor/watchdog/observers/api.py\", line 368, in dispatch_events\n",
            "    handler.dispatch(event)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/vendor/watchdog/events.py\", line 454, in dispatch\n",
            "    _method_map[event_type](event)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/filesync/dir_watcher.py\", line 257, in _on_file_modified\n",
            "    logger.info(\"file/dir modified: %s\", event.src_path)\n",
            "Message: 'file/dir modified: %s'\n",
            "Arguments: ('/content/wandb/run-20220507_102423-z0my6lf6/files/output.log',)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1029, in emit\n",
            "    self.flush()\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1009, in flush\n",
            "    self.stream.flush()\n",
            "OSError: [Errno 28] No space left on device\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 890, in _bootstrap\n",
            "    self._bootstrap_inner()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/vendor/watchdog/observers/api.py\", line 199, in run\n",
            "    self.dispatch_events(self.event_queue, self.timeout)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/vendor/watchdog/observers/api.py\", line 368, in dispatch_events\n",
            "    handler.dispatch(event)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/vendor/watchdog/events.py\", line 454, in dispatch\n",
            "    _method_map[event_type](event)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/filesync/dir_watcher.py\", line 257, in _on_file_modified\n",
            "    logger.info(\"file/dir modified: %s\", event.src_path)\n",
            "Message: 'file/dir modified: %s'\n",
            "Arguments: ('/content/wandb/run-20220507_102423-z0my6lf6/files/output.log',)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/accelerate/commands/accelerate_cli.py\", line 43, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/accelerate/commands/launch.py\", line 515, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/accelerate/commands/launch.py\", line 205, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', 'run_image_classification.py', '--model_name_or_path', 'gary109/crop14-small_pretrain_vit-base-mim', '--dataset_name', 'gary109/crop14_balance', '--output_dir=crop14_balance_ft_pretrain_vit-base-mim/', '--remove_unused_columns', 'False', '--overwrite_output_dir', '--do_train', '--do_eval', '--push_to_hub', '--push_to_hub_model_id=crop14_balance_ft_pretrain_vit-base-mim', '--hub_token=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', '--learning_rate', '2e-5', '--num_train_epochs', '30', '--per_device_train_batch_size', '64', '--per_device_eval_batch_size', '64', '--logging_strategy', 'steps', '--logging_steps', '10', '--evaluation_strategy', 'epoch', '--save_strategy', 'epoch', '--load_best_model_at_end', 'True', '--save_total_limit', '1', '--use_auth_token=True', '--seed', '1337']' returned non-zero exit status 255.\n",
            "Thread WriterThread:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/internal/internal_util.py\", line 51, in run\n",
            "    self._run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/internal/internal_util.py\", line 102, in _run\n",
            "    self._process(record)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/internal/internal.py\", line 353, in _process\n",
            "    self._wm.write(record)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/internal/writer.py\", line 34, in write\n",
            "    self._ds.write(record)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/internal/datastore.py\", line 274, in write\n",
            "    ret = self._write_data(s)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/internal/datastore.py\", line 234, in _write_data\n",
            "    self._write_record(s)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/internal/datastore.py\", line 211, in _write_record\n",
            "    self._fp.write(s)\n",
            "OSError: [Errno 28] No space left on device\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Internal wandb error: file data was not synced\n",
            "/usr/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown\n",
            "  len(cache))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gary109/crop14-small_pretrain_vit-mae-large\n",
        "---\n",
        "- crop14_balance_ft_pretrain_vit-base-mae\n"
      ],
      "metadata": {
        "id": "6f7vgLcGog9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch run_image_classification_ViT-MAE.py \\\n",
        "    --model_name_or_path \"gary109/crop14-small_pretrain_vit-mae-large\" \\\n",
        "    --dataset_name \"gary109/crop14_balance\" \\\n",
        "    --output_dir=\"crop14_balance_ft_pretrain_vit-mae-large/\" \\\n",
        "    --remove_unused_columns False \\\n",
        "    --overwrite_output_dir \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --push_to_hub \\\n",
        "    --push_to_hub_model_id=\"crop14_balance_ft_pretrain_vit-mae-large\" \\\n",
        "    --hub_token=\"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\" \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 30 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --evaluation_strategy epoch \\\n",
        "    --save_strategy epoch \\\n",
        "    --load_best_model_at_end True \\\n",
        "    --save_total_limit 1 \\\n",
        "    --use_auth_token=\"True\" \\\n",
        "    --seed 1337 \n",
        "\n",
        "\n",
        "    # --gradient_accumulation_steps 8 \\\n",
        "    # --gradient_checkpointing"
      ],
      "metadata": {
        "id": "63bJm2juog9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Model\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nMaBzIBLWWs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  載入測試資料集\n",
        "---"
      ],
      "metadata": {
        "id": "WBaktYo3bRQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "# # dataset = load_dataset(\"/content/ai-cup-2022-crop_classification/datasets/crop14.py\", 'crop14-small')\n",
        "# dataset = load_dataset(\"/content/crop14.py\", 'crop14-balance')\n",
        "# dataset = load_dataset(\"crop14.py\", 'crop14-small',use_auth_token=True,cache_dir='./ggg')\n",
        "# dataset = load_dataset(\"gary109/crop14_balance\", use_auth_token=True)\n",
        "# dataset = load_dataset(\"/content/drive/MyDrive/datasets/crop14_colab.py\", 'crop14-pretrain', cache_dir='/content/drive/MyDrive/datasets/crop14-pretrain')\n",
        "dataset = load_dataset(\"gary109/crop14-small\", use_auth_token=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "61048689ae8e47b5a54983fa42a1a7fc",
            "82f974bf809742dd99f23a292f16da50",
            "779d92344cb7484cb5579a45d28b0fe3",
            "819703063a004a69930dff87196d08d6",
            "b397b770c75a4ea095ebca3a403d970d",
            "d30056927e114099b5f6bfb9dec3fa53",
            "b30b64bc0f634e4893b281fc9cd7658b",
            "f22f7b3ba35940689f330286cf596d0a",
            "8a9675ebcfec425d91a5b500ffc21a0c",
            "326b3a90551a478a837338d45c5677f7",
            "db620b8127614afe801ae22988fb667b"
          ]
        },
        "id": "P2KC4Lp2bCxX",
        "outputId": "0192c0ee-3c66-42c8-881c-18d86f34588e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration gary109--crop14-small-d2283ee3b6a9a5fe\n",
            "Reusing dataset parquet (/root/.cache/huggingface/datasets/gary109___parquet/gary109--crop14-small-d2283ee3b6a9a5fe/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61048689ae8e47b5a54983fa42a1a7fc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 預測結果\n",
        "---\n",
        "- gary109/crop14-small_vit-base-patch16-224-in21k"
      ],
      "metadata": {
        "id": "p3nqwLgWbKU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "BASE_MODEL_URL = 'gary109/crop14-small_vit-base-patch16-224-in21k'\n",
        "\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(BASE_MODEL_URL, use_auth_token=True)\n",
        "model = ViTForImageClassification.from_pretrained(BASE_MODEL_URL, use_auth_token=True)\n",
        "\n",
        "inputs = feature_extractor(images=dataset['train'][2]['image'], return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "# model predicts one of the 1000 ImageNet classes\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTFdgRvxWb5L",
        "outputId": "76708beb-f2e5-48d7-aee3-ad060022a4ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: carrot\n"
          ]
        }
      ]
    }
  ]
}